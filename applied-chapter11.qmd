---
title: "Principal Component Analysis"
---

## Overview

This chapter introduces **Principal Component Analysis (PCA)**—a foundational tool for unsupervised data analysis and dimensionality reduction. You'll learn how PCA works, when to use it, and what to look out for. We also introduce nonlinear data reduction methods like *t*-SNE and multidimensional scaling.

- What is unsupervised analysis?
- Data reduction  
- Applications to PCA  
- Technical details  

## What Is PCA?

### Supervised vs. Unsupervised Learning

- **Supervised learning**: Provide both the response variable and predictors. Models aim to:
  - Explain the relationship
  - Perform hypothesis testing and build confidence intervals
  - Make predictions

- **Unsupervised learning**: Only provide the predictors.
  - No response variable
  - Goals vary; interpretation is often subjective
  - Not trying to make predictions or explain in the traditional sense as with linear and logistic regression  
  - More challenging than supervised 
  - No easy way to validate results on future data
  - Common uses:
    - Exploratory data analysis (EDA) of a supervised analysis
    - Identifying subpopulations
    - Improving computational efficiency
    - Simplifying regression/classification prediction tasks

### Unsupervised Tools
- **Data reduction**:  
  - Principal Component Analysis (PCA)  
  - *t*-SNE  
  - Multidimensional Scaling (MDS)
- **Clustering**:  
  - Hierarchical  
  - *k*-means


## PCA and Data Reduction

### Motivation for Data Reduction
- Too many variables → Hard to analyze, visualize, or model
- Goal: reduce the number of variables while preserving the “information”
  - Creates a new data set
- "Information" depends on algorithm 
  - "Information" can be defined in terms of **variability**
    
### Principal Components
- New variables $Z_1, Z_2, \dots, Z_p$ are called **principal components**.
- Each new variable, $Z_j$ is a **linear combination** of the original $X_i$ variables:
  - Coefficients change for each principal component.

$$
Z_1 = \phi_{11}X_1 + \phi_{12}X_2 + \dots + \phi_{1p}X_p \\
Z_2 = \phi_{21}X_1 + \phi_{22}X_2 + \dots + \phi_{2p}X_p \\
\vdots \\
Z_p = \phi_{p1}X_1 + \phi_{p2}X_2 + \dots + \phi_{pp}X_p
$$
- PCA creates the same number of principal components as variables in the original data set.  
  - Where is the reduction? Should we:  
    - Keep only a few?, How many?, Which ones? What criterion? 

### Properties of Principal Components
- **Uncorrelated** with each other
- **Ordered by variance**: $\text{Var}(Z_1) > \text{Var}(Z_2) > \dots > \text{Var}(Z_p)$
- Total variance is preserved: $\sum_{i=1}^p \text{Var}(X_i) = \sum_{i=1}^p \text{Var}(Z_i)$

### Reducing Dimensions
- Because $\text{Var}(Z_1) > \text{Var}(Z_2) > \dots > \text{Var}(Z_p)$  
  - Some variables will essentially have 0 variance  
  - Drop low-variance components since all the values in the column are about the same, those components aren't useful.
  - Left with $k < p$ variables.  
- Total variability is preserved:
$$
\sum_{i=1}^p \text{Var}(X_i) \approx \sum_{i=1}^k \text{Var}(Z_i), \quad \text{where } k < p
$$


## Performing PCA

### From Data to Components
- PCA requires:
  - Centering (and possibly scaling) the data
    - Data is centered by subtracting the mean from each value.
  - Computing either:
    - Covariance matrix (if unscaled)
    - Correlation matrix (if standardized, this is 'standard operating procedure')
      - Scaling leads to analysis essentially on the z-scores.
- From the matrix, extract:
  - **Eigenvectors**: the weights (loadings) used to compute $Z_i$
  - **Eigenvalues**: the variance of each $Z_i$
  - Each eigenvalue has an associated eigenvector. There are $p$ of each.

### Matrix View
- Eigenvector for $Z_1$ is $(\phi_{11}, \phi_{12}, \dots, \phi_{1p})$
- **Eigenvector matrix** contains all loading coefficients. Eigenvectors are arranged columnwise.
  - Transpose loading matrix and matrix multiply by old variables to get $p$ rows 
- **Eigenvalues** quantify the variance of each principal component. Sometimes they are expressed in matrix form.

### Scree Plots and Component Selection
- Scree plots
  - Plot eigenvalues arranged in order
  - Plot variance proportions ($\frac{\text{eigenvalue}_i}{\text{sum of eigenvalues}}$) arranged in order
  - Plot cumulative proportion of variance
- Strategies for reduction:
  - Keep enough PCs to explain ~80–90% of the variance (may be arbitraily picked)
  - Keep first 3–4 for visualization in EDA
  - Look for an “elbow” in the scree plot

--- 

**PCA Applications**  

- Simplify regression and classification problems  
  - Only apply PCA to the predictors  
  - No multicollinearity, since new variables are uncorrelated    
  - Fewer predictors (data reduction)   
- Source of confusion  
  - PCA is not supervised (but used within)
  - Can be thought of as a processing step (not a predictive tool)  
  - No guaranteee that PCs will correlate to the response better than the original variables  
  
- PCA in Classification EDA (gives you a sense if your predictors will work well at all)  
  - Not used in the model itself  
  - What if project requires use of original predictors (e.g. you need to interpret)  
  - Large number of predictors  
  - Feature selection is the plan  
  - Strategy  
    - Plot first few PCs in a scatter plot  
    - Examine if separation occurs  
    - Sanity check when assessing models later (can help you determine if you expect to see good prediction accuracy or not in the model with your original predictors)  
    - When no separation occurs  
      - Indicates predictors are not relevant OR predictors may still be important (nonlinear boundaries)  
    - Consider nonlinear data reduction (not PCA, which is linear)  
      - Add complexity if separation is seen

- PCA Image Compression  
  - Data are too big to transfer (send over email, etc.)  
  - Want to make the image smaller  
    - Maintain resolution  
  - Data reduction solves this  
    - Need to convert reduced data set back to original scale  
  - Data reconstruction  
  
- Common Pitfalls / Concerns  
  - Regression  
    - Accidentally using the response in the PCA  
    - Results will be bogus  
  - MLR and logistic regression  
    - Interpretation coefficients are difficult  
    - Use eigenvectors to summarize  
    - There are strategies  
  - PCA is effected by outliers  
  - Really shouldn't include categorical variables  
    - Arbitrary process if converting categorical to numeric  
  - Computational time  
    - Can be slow when variables are extremely high  
    - Fast versions that compute first few  
  - "Information" preservation is variance only  
    - No other information is used  
    - Allows to visualize "global" structure of data (linear separation)  
    - Data points closer together in the PC space does not imply that they are close in the original space  
    
    
**Nonlinear Data Reduction**  

- Traditional PCA  
  - Reduce the dimensionality of the points  
  - Projected on the "closest" plane/hyperplane possible  
  - "Linear" since points always live on a linear surface  
- Nonlinear PCA  
  - Project points onto lower dimensional, nonlinear surfaces  
  - Parameter and method choices  
  - Some are strictly EDA, not used as predictors  
  
- Common Nonlinear Reduction  
  - Kernel PCA  
  - Multidimensional scaling  
  -*t*-SNE (nonparametric)  

- Kernel PCA  
  - Transform your variables using complexity  
  - Kernel functions  
    - Polynomial  
    - Radial basis  
    - Other  
  - Perform PCA after tranformation  
  - Can be used in predictive models  
    
- Multidimensional Scaling  
  - Project data to lower dimensions while preserving distances or similarities between points  
  - Typically shown in 2D or 3D  
  - Not directly used in predictive models  
  
- *t*-SNE: *t*-Distributed Stochastic Neighbor Embedding    
  - Models distances between points probabilistically  
  - Seeks lower-dimensional map that preserves the distribution of the distances  
  - Allows to see more local structures  
  - Requires hyperparameters such as perplexity  
    - Visuals depend on hyperparameter (varying results)  
    - Not used in predictive models directly  
    - Sometimes PCA is applied first (computation time)  
    

**PCA Technical Details**  

- Multiple variables follow a multivariate distribution.  

- Principal Components  
  - When performing PCA, we now know that:  
    - The new variables are uncorrelated  
    - The variances are the eigenvalues  
    - The coefficients to create (using the linear combination) the PCs are the eigenvectors
  - Decomposing $\Sigma$ or $R$  
    - Eigenvectors and eigenvalues are the result of a singular value decomposition of $\Sigma$ or $R$ (depending on which you are using): $\Sigma = \phi\Lambda\phi'$  
  - Properties of Eigenvectors  
    - The transpose of the eigenvector is also its inverse: $\phi\phi' = \phi'\phi = I$  
  - Linear Transformation
    
  