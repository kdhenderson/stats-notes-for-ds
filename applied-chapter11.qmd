---
title: "Principal Component Analysis"
---

## Overview

- **Principal Component Analysis (PCA)** is a foundational tool for unsupervised data analysis and dimensionality reduction.

- This chapter covers:
  - What unsupervised analysis is
  - The motivation for data reduction
  - Applications of PCA
  - Technical details of PCA
  - Introduction to nonlinear data reduction methods (*t*-SNE, MDS) 


## What Is PCA?

### Supervised vs. Unsupervised Learning

- **Supervised learning**: Provide both the response variable and predictors. Models aim to:
  - Explain the relationship
  - Perform hypothesis testing and build confidence intervals
  - Make predictions

- **Unsupervised learning**: Only provide the predictors.
  - No response variable
  - Goals vary; interpretation is often subjective
  - Not trying to make predictions or explain in the traditional sense as with linear and logistic regression  
  - More challenging than supervised 
  - No easy way to validate results on future data
  - Common uses:
    - Exploratory data analysis (EDA) of a supervised analysis
    - Identifying subpopulations
    - Improving computational efficiency
    - Simplifying regression/classification prediction tasks

### Unsupervised Tools
- **Data reduction**:  
  - Principal Component Analysis (PCA)  
  - *t*-SNE  
  - Multidimensional Scaling (MDS)
- **Clustering**:  
  - Hierarchical  
  - *k*-means


## PCA and Data Reduction

### Motivation for Data Reduction
- Too many variables → Hard to analyze, visualize, or model
- Goal: create a new data set with fewer variables while preserving "information"
- "Information" depends on algorithm 
  - "Information" can be defined in terms of **variability**
    
### Principal Components
- New variables $Z_1, Z_2, \dots, Z_p$ are called **principal components**.
- Each new variable, $Z_j$ is a **linear combination** of the original $X_i$ variables:
  - Coefficients change for each principal component.

$$
Z_1 = \phi_{11}X_1 + \phi_{12}X_2 + \dots + \phi_{1p}X_p \\
Z_2 = \phi_{21}X_1 + \phi_{22}X_2 + \dots + \phi_{2p}X_p \\
\vdots \\
Z_p = \phi_{p1}X_1 + \phi_{p2}X_2 + \dots + \phi_{pp}X_p
$$
- PCA creates the same number of principal components as variables in the original data set.  
  - Where is the reduction? Should we:  
    - Keep only a few?, How many?, Which ones? What criterion? 

### Properties of Principal Components
- **Uncorrelated** with each other
- **Ordered by variance**: $\text{Var}(Z_1) > \text{Var}(Z_2) > \dots > \text{Var}(Z_p)$
- Total variance is preserved: $\sum_{i=1}^p \text{Var}(X_i) = \sum_{i=1}^p \text{Var}(Z_i)$

### Reducing Dimensions
- Because $\text{Var}(Z_1) > \text{Var}(Z_2) > \dots > \text{Var}(Z_p)$  
  - Some variables will essentially have 0 variance  
  - Drop low-variance components since all the values in the column are about the same, those components aren't useful.
  - Left with $k < p$ variables.  
- Total variability is preserved:
$$
\sum_{i=1}^p \text{Var}(X_i) \approx \sum_{i=1}^k \text{Var}(Z_i), \quad \text{where } k < p
$$


## Performing PCA

### From Data to Components
- PCA requires:
  - Centering (and possibly scaling) the data
    - Center the data by subtracting the mean from each value.
  - Computing either:
    - Covariance matrix (if unscaled)
    - Correlation matrix (if standardized, this is 'standard operating procedure')
      - Scaling leads to analysis essentially on the z-scores.
- From the matrix, extract:
  - **Eigenvectors**: the weights (loadings) used to compute $Z_i$
  - **Eigenvalues**: the variance of each $Z_i$
  - Each eigenvalue has an associated eigenvector. There are $p$ of each.

### Matrix View
- Eigenvector for $Z_1$ is $(\phi_{11}, \phi_{12}, \dots, \phi_{1p})$
- **Eigenvector matrix** contains all loading coefficients. Eigenvectors are arranged columnwise.
  - Transpose loading matrix and matrix multiply by old variables to get $p$ rows 
- **Eigenvalues** quantify the variance of each principal component. Sometimes they are expressed in matrix form.

### Scree Plots and Component Selection
- Scree plots
  - Plot eigenvalues arranged in order
  - Plot variance proportions ($\frac{\text{eigenvalue}_i}{\text{sum of eigenvalues}}$) arranged in order
  - Plot cumulative proportion of variance
- Strategies for reduction:
  - Keep enough PCs to explain ~80–90% of the variance (may be arbitrarily picked)
  - Keep first 3–4 for visualization in EDA
  - Look for an “elbow” in the scree plot

 
## Applications of PCA

### Simplify Regression and Classification
- Use PCA on predictors only
- New predictors are uncorrelated (no multicollinearity)
- Reduces dimensionality (i.e data reduction, fewer predictors)
- Important notes:
  - PCA is unsupervised!
  - PCs may not align with response variable better than the original variables.
  - Can be thought of as a processing step rather than a predictive tool.

### PCA for EDA in Classification

#### Purpose
- To get a sense if the predictors will work well, not for use in the model itself.
- When to use feature selection instead:
  - If project requires use of original predictors (e.g. must be interpretable)  
  - Large number of predictors  

#### Strategy
- Plot PCs to visualize group separation
- If separation exists, your predictors may work well
- If not:
  - Predictors may be irrelevant
  - Or, nonlinear methods may be required
    - Use nonlinear data reduction or add complexity
- Use as a sanity check for assessing models. Should good prediction accuracy be expected from those variables.

### Image Compression
- Reduce the number of pixels (variables)
- PCA compresses the data
- Reconstruct image from fewer components

### Common Pitfalls

- Do **not** use the response variable in PCA
- Coefficient interpretation is difficult
  - Interpretation strategies exist, such as using eigenvectors to summarize. 
- PCA is sensitive to outliers
- Categorical variables can lead to misleading PCA results, especially if arbitrary numeric coding is used
- Only variance is preserved  
  - Allows to visualize "global" structure of data (i.e. linear separation)  
  - When data points are closer together in PC space, it does not imply that they are close in the original space  

    
## Nonlinear Data Reduction

### Beyond Linear PCA
- Linear PCA maps data to a linear plane
  - Reduce the dimensionality of the points  
  - Projected on the "closest" plane/hyperplane possible
- **Nonlinear methods** allow curved manifolds 
  - Project points onto lower dimensional, nonlinear surfaces  
  - Choosing appropriate parameters and methods is important in nonlinear reduction  
  - Some are strictly EDA, not used as predictors  

### Common Nonlinear Reduction Techniques
- **Kernel PCA**:
  - Use kernel functions (polynomial, radial basis (RBF))
  - Perform PCA in transformed feature space
  - Can be used in predictive models  

- **Multidimensional Scaling (MDS)**:
  - Preserves distances or similarities between points
  - Typically used for 2D/3D visualizations
  - Not directly used in predictive models 

- **t-SNE** (*t*-Distributed Stochastic Neighbor Embedding):
  - Non-parametric
  - Captures local structure using probability models, i.e. models distances between points probabilistically
    - Seeks lower-dimensional map that preserves the distribution of the distances  
  - Not used in predictive models
  - Results can vary based on hyperparameters like *perplexity* 
  - Sometimes PCA is applied first (computation time) 
  

## PCA: Technical Details

### Matrix Decomposition
- Assume multiple variables follow a multivariate distribution
- Principal components are:
  - Uncorrelated
  - Have variances = eigenvalues
  - Computed via eigenvectors
    - The eigenvectors provide the coefficients used to create the principal components via linear combinations
- Decompose $\Sigma$ or $R$: $\Sigma = \phi \Lambda \phi'$
  where:
  - $\phi$ = matrix of eigenvectors
  - $\Lambda$ = diagonal matrix of eigenvalues
- Key property: The transpose of the eigenvector matrix is its inverse.
$$
\phi^\top \phi = \phi \phi^\top = I
$$
  