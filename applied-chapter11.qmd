---
title: "Principal Component Analysis"
---

## Overview

This chapter introduces **Principal Component Analysis (PCA)**—a foundational tool for unsupervised data analysis and dimensionality reduction. You'll learn how PCA works, when to use it, and what to look out for. We also introduce nonlinear data reduction methods like *t*-SNE and multidimensional scaling.

- What is unsupervised analysis?
- Data reduction  
- Applications to PCA  
- Technical details  

## What Is PCA?

### Supervised vs. Unsupervised Learning

- **Supervised learning**: Provide both the response variable and predictors. Models aim to:
  - Explain the relationship
  - Perform hypothesis testing and build confidence intervals
  - Make predictions

- **Unsupervised learning**: Only provide the predictors.
  - No response variable
  - Goals vary; interpretation is often subjective
  - Not trying to make predictions or explain in the traditional sense as with linear and logistic regression  
  - More challenging than supervised 
  - No easy way to validate results on future data
  - Common uses:
    - Exploratory data analysis (EDA) of a supervised analysis
    - Identifying subpopulations
    - Improving computational efficiency
    - Simplifying regression/classification prediction tasks

### Unsupervised Tools
- **Data reduction**:  
  - Principal Component Analysis (PCA)  
  - *t*-SNE  
  - Multidimensional Scaling (MDS)
- **Clustering**:  
  - Hierarchical  
  - *k*-means


## PCA and Data Reduction

### Motivation for Data Reduction
- Too many variables → Hard to analyze, visualize, or model
- Goal: reduce the number of variables while preserving the “information”
  - Creates a new data set
- "Information" depends on algorithm 
  - "Information" can be defined in terms of **variability**
    
### Principal Components
- New variables $Z_1, Z_2, \dots, Z_p$ are called **principal components**.
- Each new variable, $Z_j$ is a **linear combination** of the original $X_i$ variables:
  - Coefficients change for each principal component.

$$
Z_1 = \phi_{11}X_1 + \phi_{12}X_2 + \dots + \phi_{1p}X_p \\
Z_2 = \phi_{21}X_1 + \phi_{22}X_2 + \dots + \phi_{2p}X_p \\
\vdots \\
Z_p = \phi_{p1}X_1 + \phi_{p2}X_2 + \dots + \phi_{pp}X_p
$$
- PCA creates the same number of principal components as variables in the original data set.  
  - Where is the reduction? Should we:  
    - Keep only a few?, How many?, Which ones? What criterion? 

### Properties of Principal Components
- **Uncorrelated** with each other
- **Ordered by variance**: $\text{Var}(Z_1) > \text{Var}(Z_2) > \dots > \text{Var}(Z_p)$
- Total variance is preserved: $\sum_{i=1}^p \text{Var}(X_i) = \sum_{i=1}^p \text{Var}(Z_i)$

### Reducing Dimensions
- Because $\text{Var}(Z_1) > \text{Var}(Z_2) > \dots > \text{Var}(Z_p)$  
  - Some variables will essentially have 0 variance  
  - Drop low-variance components since all the values in the column are about the same, those components aren't useful.
  - Left with $k < p$ variables.  
- Total variability is preserved:
$$
\sum_{i=1}^p \text{Var}(X_i) \approx \sum_{i=1}^k \text{Var}(Z_i), \quad \text{where } k < p
$$


## Performing PCA

### From Data to Components
- PCA requires:
  - Centering (and possibly scaling) the data
    - Data is centered by subtracting the mean from each value.
  - Computing either:
    - Covariance matrix (if unscaled)
    - Correlation matrix (if standardized, this is 'standard operating procedure')
      - Scaling leads to analysis essentially on the z-scores.
- From the matrix, extract:
  - **Eigenvectors**: the weights (loadings) used to compute $Z_i$
  - **Eigenvalues**: the variance of each $Z_i$
  - Each eigenvalue has an associated eigenvector. There are $p$ of each.

### Matrix View
- Eigenvector for $Z_1$ is $(\phi_{11}, \phi_{12}, \dots, \phi_{1p})$
- **Eigenvector matrix** contains all loading coefficients. Eigenvectors are arranged columnwise.
  - Transpose loading matrix and matrix multiply by old variables to get $p$ rows 
- **Eigenvalues** quantify the variance of each principal component. Sometimes they are expressed in matrix form.

### Scree Plots and Component Selection
- Scree plots
  - Plot eigenvalues arranged in order
  - Plot variance proportions ($\frac{\text{eigenvalue}_i}{\text{sum of eigenvalues}}$) arranged in order
  - Plot cumulative proportion of variance
- Strategies for reduction:
  - Keep enough PCs to explain ~80–90% of the variance (may be arbitraily picked)
  - Keep first 3–4 for visualization in EDA
  - Look for an “elbow” in the scree plot

 
## Applications of PCA

### Simplify Regression and Classification
- Use PCA on predictors only
- New predictors are uncorrelated (no multicollinearity)
- Reduces dimensionality (i.e data reduction, fewer predictors)
- Sometimes confusing:
  - PCA is unsupervised!
  - PCs may not align with response variable better than the original variables.
  - Can be thought of as a processing step rather than a predictive tool.

### PCA for EDA in Classification

#### Purpose
- To get a sense if the predictors will work well, not for use in the model itself.
- When to use feature selection instead:
  - If project requires use of original predictors (e.g. must be interpretable)  
  - Large number of predictors  

#### Strategy
- Plot PCs to visualize group separation
- If separation exists, your predictors may work well
- If not:
  - Predictors may be irrelevant
  - Or, nonlinear methods may be required
    - Use nonlinear data reduction or add complexity
- Use as a sanity check for assessing models. Should good prediction accuracy be expected from those variables.

### Image Compression
- Reduce the number of pixels (variables)
- PCA compresses the data
- Reconstruct image from fewer components

### Common Pitfalls

- Do **not** use the response variable in PCA
- Coefficient interpretation is difficult
  - Strategies exist. Use eigenvectors to summarize. 
- PCA is sensitive to **outliers**
- Categorical variables are problematic
- Only **variance** is preserved  
  - Allows to visualize "global" structure of data (i.e. linear separation)  
  - When data points are closer together in PC space, it does not imply that they are close in the original space  

---    
    
**Nonlinear Data Reduction**  

- Traditional PCA  
  - Reduce the dimensionality of the points  
  - Projected on the "closest" plane/hyperplane possible  
  - "Linear" since points always live on a linear surface  
- Nonlinear PCA  
  - Project points onto lower dimensional, nonlinear surfaces  
  - Parameter and method choices  
  - Some are strictly EDA, not used as predictors  
  
- Common Nonlinear Reduction  
  - Kernel PCA  
  - Multidimensional scaling  
  -*t*-SNE (nonparametric)  

- Kernel PCA  
  - Transform your variables using complexity  
  - Kernel functions  
    - Polynomial  
    - Radial basis  
    - Other  
  - Perform PCA after tranformation  
  - Can be used in predictive models  
    
- Multidimensional Scaling  
  - Project data to lower dimensions while preserving distances or similarities between points  
  - Typically shown in 2D or 3D  
  - Not directly used in predictive models  
  
- *t*-SNE: *t*-Distributed Stochastic Neighbor Embedding    
  - Models distances between points probabilistically  
  - Seeks lower-dimensional map that preserves the distribution of the distances  
  - Allows to see more local structures  
  - Requires hyperparameters such as perplexity  
    - Visuals depend on hyperparameter (varying results)  
    - Not used in predictive models directly  
    - Sometimes PCA is applied first (computation time)  
    

**PCA Technical Details**  

- Multiple variables follow a multivariate distribution.  

- Principal Components  
  - When performing PCA, we now know that:  
    - The new variables are uncorrelated  
    - The variances are the eigenvalues  
    - The coefficients to create (using the linear combination) the PCs are the eigenvectors
  - Decomposing $\Sigma$ or $R$  
    - Eigenvectors and eigenvalues are the result of a singular value decomposition of $\Sigma$ or $R$ (depending on which you are using): $\Sigma = \phi\Lambda\phi'$  
  - Properties of Eigenvectors  
    - The transpose of the eigenvector is also its inverse: $\phi\phi' = \phi'\phi = I$  
  - Linear Transformation
    
  