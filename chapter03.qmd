---
title: "Chapter 3: Data Screening, Assumptions, and Transformations"
---

# Data Screening, Assumptions, and Transformations


## Experimental Design
- **Randomization**: Reduces potential bias.
- **Placebo**: Controls for confounding variables; ensures only the variable being tested differs.
- **Blinding**: Minimizes bias in experimental outcomes.


## Conditions for Null Hypothesis Significance Testing (NHST)
1. Random sampling
2. Independent observations
3. Representative of the population
4. Quantitative observations
5. Nearly normal distribution
6. Equal standard deviations for two-sample tests (homoscedasticity, meaning the distributions have the same shape).


## Paired t-Test
- Used when the assumption of independence is violated.
- Compares the difference between paired samples using a one-sample t-test.


## Tools for Checking Normality
1. **Boxplot**
   - Visualizes the five-number summary.
   - Highlights symmetry, skewness, and tail behavior.
   - Useful for side-by-side comparisons.
2. **Dotplot**
   - Displays individual data points.
   - Simple to construct and interpret.
3. **Histogram**
   - Shows data symmetry and shape.
4. **Normal Quantile (QQ) Plot**
   - Plots theoretical normal values (X-axis) against observed data (Y-axis).
   - Normal distribution aligns points along a straight line.
   - Sensitive to deviations from normality.


## Robustness
- A procedure is **robust** if its results remain valid despite minor assumption violations.
- Example: 95% confidence intervals (CIs) should capture the true parameter value 95% of the time.

### Moderate Robustness in t-Tools
- **Sample size effects**:
  - Larger samples tolerate greater deviations from normality (unless tails are excessively heavy, meaning there are many large outliers).
- **Two-sample t-tests**:
  - Problems arise with differing shapes/skewness or unequal standard deviations.
  - Worst-case scenario: Smaller sample size paired with a population of larger standard deviation. In this case, the sample may fail to accurately represent the population's variability.


## Independence
- **Definition**: Observations are independent if knowing one value provides no information about another. Independent observations must be built into the experimental design.
- **Cluster effects**: Occur in subgroup data (e.g. littermates) and require analysis with different tools or by treating each cluster as a single observation.
- **Serial effects**: Occur when measurements are taken over time.


## Outliers
- **Definition**: Observations far from the group average.
- **Effects**:
  - Long-tailed distributions often result from outliers.
  - t-statistics are sensitive to outliers.

### Dealing with Outliers
- Do not delete unless they are clear data entry or measurement errors.
- Run analyses with and without outliers and report both results.


## Resistance
- A procedure is **resistant** if it remains stable when small parts of the data change.
  - Example: Median is resistant, while the mean is not.
- t-tools are based on means and are not resistant to outliers or long tails.


## Log Transformation (Natural Log)
- **When to use**:
  - Ratio of largest to smallest value > 10.
  - Skewed data.
  - Group with larger mean also have the larger variance.
- **Effects**:
  - Corrects non-normality and non-constant variance.
  - Requires back-transformation to interpret median and CIs on the original scale.

### Other Transformations
- Square root
- Inverse
- Reciprocal
- **Note**: These transformations are harder to back-transform than log transformations.

  
## Log Transformations: Propositions (Logarithmic Properties)
1. Normal distribution: The mean equals the median in a normal distribution.
2. Monotonicity: The log function is monotonically increasing, meaning log(median x) = median(log(x)).
3. Log differences:
   - $\log(a) - \log(b) = \log\left(\frac{a}{b}\right)$
4. Exponentiation: $e^{\log(x)} = x$


## t-Test Interpretation (Log-Transformed Data)
- $\text{mean}(\log x) - \text{mean}(\log y) = \gamma$
- The distribution is approximately normal, so the mean and median are equal:  
  $\text{median}(\log x) - \text{median}(\log y) = \gamma$
- The logarithmic function is monotonically increasing:  
  $\log(\text{median}(x)) - \log(\text{median}(y)) = \gamma$
- Exponentiation gives:  
  $e^{\log(\text{median}(x) / \text{median}(y))} = e^\gamma$
- Therefore,  
  $\text{median}(x) / \text{median}(y) = e^\gamma$
- **Interpretation**: The median of $x$ is estimated to be $e^\gamma$ times the median of $y$.


## Inequality of Variance
- **Visual evidence** should be used as primary evidence for detecting unequal variances.
- **F-test**: Tests for equal variances but is sensitive to normality violations.  
  - $H_0$: population variances are equal
  - $H_a$: population variances are not equal
- Use hypothesis tests for equality of variance with caution.


## General Rules of Thumb
- Equal sample sizes and large samples: t-tools are robust.
- Different standard deviations:
  - Equal sample sizes: t-tools are valid with large samples.
  - Unequal sample sizes: t-tools are not valid.


## Welchâ€™s t-Test
- Adjusted for unequal standard deviations.
- Adjusts degrees of freedom using the Satterthwaite approximation.
- Still assumes normality.


## Non-Normal Distributions
- Long-tailed distributions:
  - Wider CIs than expected (e.g. $> (1-\alpha) \%$), leading to fewer rejections of the null hypothesis.
  - Fewer rejections can increase the true capture rate of $\mu$ to $> (1-\alpha) \%$, resulting in rejection rates $< \alpha \%$.  

![](images/display_3_4.png){width=5in fig-align="left"}^[Source: Ramsey and Schafer (2012).]  

- When sample sizes and standard deviations both differ, CIs can become either too narrow or too wide.  

![](images/display_3_5.png){width=5in fig-align="left"}^[Source: Ramsey and Schafer (2012).]  
