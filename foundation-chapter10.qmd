---
title: "Regression Diagnostics and Model Refinement"
execute:
  engine: knitr
---

## Objectives

- Calculate and interpret residuals, standardized residuals, and studentized residuals.
- Use residuals to check regression assumptions.
- Apply necessary remedies when assumptions are violated.
- Understand the robustness of assumptions (i.e., what can you get away with?)


## Robustness of Regression Assumptions

### Linearity
- Parameter estimates will be misleading if a straight-line model is inadequate or fits only part of the data.
- Predictions will be biased, and confidence intervals (CIs) will not appropriately reflect uncertainty.
- **Remedy**: Consider adding polynomial terms (quadratic or cubic) to improve model fit.

### Normality
- Transformations that correct for normality often address constant variance as well.
- **Effects of non-normality**:
  - **Coefficient estimates and standard errors**: Robust, except with many outliers and small sample sizes.
  - **Confidence intervals**: Affected primarily by outliers (long tails).
  - **Prediction intervals**: Sensitive to non-normality due to reliance on normal distributions.

### Constant Variance (Homoscedasticity)
- For every value of $x$, the spread of $y$ should be the same.
- Least squares estimates remain unbiased with slight violations.
- Large violations can cause standard errors to underestimate or overestimate uncertainty, leading to misleading confidence intervals and hypothesis tests.
- **Remedy**: Large violations should be corrected using a transformation.

### Independence
- **Parameter estimates**: Not affected by violations.
- **Standard errors**: Affected significantly. Violations can lead to underestimated standard errors, which inflate *t*-statistics and make it easier to incorrectly reject the null hypothesis.
- **Remedy**: Serial and cluster effects require different models.

### How Much Deviation Is Acceptable?
- Small violations of assumptions generally do not invalidate regression results.
- However, large deviations can lead to inaccurate estimates, especially for standard errors, confidence intervals, and p-values.
- Transformations can often correct violations and improve interpretability.
- Only severe departures from linearity or normality (e.g., due to outliers) typically require alternative methods.


## Influential and Outlying Observations

### Key Concepts
- **Influential observations**: These are points that, if added or removed, substantially change the regression line (e.g., the slope or intercept).
- **Leverage**: A measure of how far an observation’s $x$ value is from the mean of all $x$ values ($\bar{x}$).
  - Points farther from $\bar{x}$ have higher leverage.
  - Mathematically, leverage increases with the squared distance from $x_i$ to $\bar{x}$, relative to the total sum of squares in $x$. This is closely related to how many standard deviations $x_i$ is from the mean.
  - Leverage is based on the $x$ values alone;it does **not** depend on $y$.

### Impact of Outliers
- **Low leverage, low influence**: Minimal effect on estimates.
- **High leverage, low influence**: Far from most data but consistent with the trend; minimal effect on the correlation, standard errors, and regression estimates.
- **High leverage, high influence**: Can distort results by pulling the regression line toward the outlier, resulting in different parameter estimates.

### Detecting Influential Observations

#### Leverage Statistic, $h_{ii}$
- Measures how far the $x$ value for an observation is from the mean $\bar{x}$, in relation to the total spread of $x$ values. Larger values of $h_{ii}$ indicate higher leverage.
- An observation is considered to have high leverage if $h_{ii} > \dfrac{2p}{n}$, where $p$ is the number of parameters in the model (including the intercept).
- Formula:
  $$
  h_{ii} = \frac{1}{n} + \frac{(X_i - \bar{X})^2}{\sum_{j=1}^{n} (X_j - \bar{X})^2}
  $$
  or
  $$
  h_{ii} = \frac{1}{n - 1} \left[ \frac{(X_i - \bar{X})}{s_x} \right]^2 + \frac{1}{n}
  $$


## Types of Residuals
- **Standardized:** $e_i / \text{RMSE}$
- **Studentized:** $e_i / \sqrt{\text{MSE} \cdot (1 - h_{ii})}$
- **Studentized-deleted (R-student):**
  - Remove an observation, recalculate the regression, and compute the studentized residual.
  - The standard deviation is calculated without the point in question.
  - Large residual values indicate potentially influential points.
  - Formula:
    $$
    \text{RSTUDENT} = \frac{\text{res}_i}{s_i \sqrt{1 - h_i}}
    $$
- **Why different types?** All three aim to normalize variance to make residuals for comparability, but they differ in how they estimate the error term:
  - **Standardized:** Uses a single overall RMSE.
  - **Studentized:** Adjusts for leverage via $(1 - h_{ii})$.
  - **R-student:** Recalculates without the observation for more accurate influence detection.


## Leave-One-Out Statistics

These measures assess the impact of each observation by considering the model fit when that observation is omitted.

Let $\hat{Y}_{i(i)}$ be the predicted value of $Y_i$ when the $i$-th observation is left out of the regression model.

### PRESS (Predicted Residual Sum of Squares)
$$
\text{PRESS}_p = \sum_{i=1}^n \left(Y_i - \hat{Y}_{i(i)}\right)^2 = \sum e_{i(i)}^2
$$
- Smaller PRESS values indicate better-fitting models.

### Cook’s Distance ($D_i$)
- Combines information on residual size and leverage, and is equivalent to comparing predictions from the full model to those from a leave-one-out model:
$$
D_i = \sum_{i=1}^n \frac{\left(\hat{Y}_{i(i)} - \hat{Y}_i\right)^2}{p \cdot \text{MSE}} = \frac{1}{p} (\text{studres}_i)^2 \left[\frac{h_i}{1 - h_i}\right]
$$
- Here, $p$ is the number of parameters in the model (including the intercept).


## Durbin-Watson Test for Independence

$$
d = \frac{\sum_{i=1}^n (e_i - e_{i-1})^2}{\sum_{i=1}^n e_i^2}
$$
- Values near 0 indicate positive correlation; values near 4 indicate negative correlation. The distribution is symmetric about 2.
- Available in R and SAS.


## Residual Types Summary

| **Name**                  | **Expression**                                           | **Use**                        |
|---------------------------|-----------------------------------------------------------|--------------------------------|
| **Residual**              | $e_i = y_i - \hat{y}_i$                                   | Residual plots                 |
| **Standardized residual** | $r_i = \frac{e_i}{s \sqrt{1 - h_{ii}}}$                   | Identify outliers              |
| **Studentized residual**  | $t_i = \frac{e_i}{s_{(i)} \sqrt{1 - h_{ii}}}$             | Test outlying $Y$ values       |
| **Deleted residual**      | $e_{i(i)} = y_i - \hat{y}_{i(i)} = \frac{e_i}{1 - h_{ii}}$ | Calculate PRESS                |

> At $\alpha = 0.05$, we expect 5% of studentized residuals to be greater than 2 or less than –2.

![](images/fch10_residualPanel.png){fig-title="Residual Diagnostics Panel."}

<div class="figure-caption">
<strong>Residual diagnostics panel.</strong> A collection of plots used to assess regression assumptions and identify potential problems. Several plots use standardized, studentized, or studentized-deleted (R-student) residuals to improve comparability across observations. These include: residuals vs. predicted values (checking for nonlinearity or heteroscedasticity), R-studentized residuals vs. predicted values (highlighting outliers), and R-studentized residuals vs. leverage (identifying high-leverage outliers). Other diagnostics include the normal QQ plot and histogram of residuals (assessing normality), predicted values vs. observed values (evaluating fit), Cook’s distance by observation (identifying influential points by combining leverage and residual size), and QQ plots of fitted means and residuals. Together, these diagnostics help evaluate model adequacy and guide possible remedies.
</div>


## Graphical Assessment of Residuals

| **Pattern**                    | **Potential Issue**        | **Solution**                  |
|--------------------------------|----------------------------|-------------------------------|
| Linear means, constant SD      | Model fits well            | No action needed              |
| Curved means, equal SD         | Nonlinearity               | Transform $X$                 |
| Curved means, increasing SD    | Nonlinearity + heteroscedasticity | Transform $Y$        |
| Skewed residuals               | Non-normality              | Can still model the mean, but CIs/PIs may be unreliable. Consider transformations. |
| Linear means, increasing SD    | Heteroscedasticity         | Use weighted regression       |


## Remedies for Violations

### Nonlinearity
- Add more complexity to the model.
- Apply a transformation to $X$.
- Add another variable, which may also help nonconstant variance.

### Nonconstant Variance
- Transform $Y$.
- Use weighted least squares to down-weight observations with larger variance so they don’t influence the regression model as much as observations closer to the line.

### Correlated Errors
- Often detected via residual plots, where you may see small values follow small values, and large follow large.
- Use time series or spatial models for serial effects within data.

### Outliers
- Use robust regression.
- Check for data entry or other errors. Only delect observations in this situation.

### Non-normality
- Usually fixed via the above methods, so wait to fix until last.
- Consider transforming the data.

:::{layout-ncol=2}

![](images/fch10_residuals_a.png){fig-title="(a) Check distribution of $Y$ at different $X$ values"}

![](images/fch10_residuals_b.png){fig-title="(b) Curvature $\rightarrow$ Consider transforming $X$"}

![](images/fch10_residuals_c.png){fig-title="(c) Quadratic trend $\rightarrow$ Add $X^2$"}

![](images/fch10_residuals_d.png){fig-title="(d) Fan-shaped spread $\rightarrow$ Transform $Y$"}

![](images/fch10_residuals_e.png){fig-title="(e) Skewed residuals $\rightarrow$ Report skewness"}

![](images/fch10_residuals_f.png){fig-title="(f) Spread depends on $X$ $\rightarrow$ Weighted regression"}

:::

<div class="figure-caption">
<strong>Residual patterns to watch for.</strong> Each panel shows a scatter with bin-wise means and standard deviations (dark line and bars). Patterns suggest remedies: (a) compare the distribution of $Y$ across $X$; (b) curvature may call for transforming $X$; (c) symmetric curvature suggests adding a quadratic term; (d) fan-shaped spread indicates transforming $Y$; (e) right-skewed residuals should be reported; (f) variance that changes with $X$ motivates weighted regression.
</div>


## Nonconstant Variance and Transformations

When the spread of residuals changes with fitted values, a transformation of $Y$ can often stabilize variance.

### Transformation Recommendations
1. Log
2. Square root
3. Other

:::{layout-ncol=2}

![](images/fch10_YTransforms_log.png)

![](images/fch10_YTransforms_sqrt.png)

![](images/fch10_YTransforms_reciprocal.png)
:::

<div class="figure-caption">
<strong>Examples of variance-stabilizing transformations.</strong> When the spread of residuals changes with the fitted values, transforming the response variable $Y$ can help stabilize variance. The panels illustrate three common recommendations: (a) a log transformation for decreasing variance, (b) a square root transformation for increasing variance, and (c) a reciprocal transformation for variance proportional to the mean. These transformations change the interpretation of model coefficients, and results may need to be back-transformed for reporting.
</div>

### Interpretation Considerations
- Adjust interpretation to fit the audience and the type of transformation and variables transformed.
- Back-transform results if necessary.


## Log Transformations: Types and Interpretations

Log transformations can be very useful and effective and provide easy interpretation.

### Log-Linear Model (Log on Response)
$$
\log\hat{Y}_i = \beta_0 + \beta_1 X_i
$$
- $\text{Median}(Y|X) = \exp(\beta_0 + \beta_1 X)$
- Mean of the response is log-linearly related to the explanatory variable.
  - $\text{Median}\{Y|X\} = \exp(\beta_0) \exp(\beta_1 X)$
- One unit increase in $X$ results in a multiplicative change of $\exp(\beta_1)$ in the median of $Y|X$.
  - $\text{Median}\{Y|(X + 1)\} / \text{Median}\{Y|X\} = \exp(\beta_1)$

### Linear-Log Model (Log on Explanatory Variable)
$$
\hat{Y}_i = \beta_0 + \beta_1 \log X_i
$$
- A doubling of $X$ leads to a $\beta_1 \log(2)$ unit change in the mean of $Y$.
  - Interpretation depends on log base (e.g., $2$ → doubling, $10$ → ten-fold).

### Log-Log Model (Both Explanatory and Response Variables Logged)
$$
\log\hat{Y}_i = \beta_0 + \beta_1 \log X_i
$$
- More complicated than the other two log transformations.
- A doubling of $X$ is associated with a $2^{\beta_1}$ change in the median of $Y$.
  - Or this may be a tenfold increase of $X$ with a change of $10^{\beta_1}$ in the median of $Y$.

---

## Interpretations of Log Transformation Varieties

- **Log-linear model**:  
  A one unit increase in $X$ is associated with a multiplicative change of $e^{\beta_1}$ in the median of $Y|X$.
- **Linear-log model**:  
  A doubling of $X$ is associated with a $\beta_1 \log(2)$ unit change in the mean of $Y|X$.
- **Log-log model**:  
  A doubling of $X$ is associated with a $2^{\beta_1}$ multiplicative change in the median of $Y|X$.
- **Remember**: Logging $Y$ → median.


::: {layout-ncol=2}
![Log-Linear Transformation](images/notes_10_4_loglinear.png)

![Linear-Log Transformation](images/notes_10_5_linearlog.png)
:::

---

::: {layout-ncol=2}
![Log-Log Transformation](images/notes_10_6_loglog.png)
:::

## Formal Test for Lack of Fit
- **Use**: different values of Y for replicated observations of X
- **F-test for lack of fit** has the usual assumptions:
  - Normality of $Y|X$,
  - Independence of $(X, Y)$ pairs,
  - Constant variance of $Y$ across all values of $X$.

- **Procedure**:
  1. Fit a linear regression and obtain $SS_{\text{res}_{LR}}$ (linear fit).
  2. Fit an ANOVA and use replicated $X$ values as the treatment variable to obtain $SS_{\text{res}_{SM}}$ (separate means model).
  3. **Null hypothesis**: The linear model fits.  
     **Alternative**: Variability cannot be explained by the model.
  4. Compute:
     $F = \frac{(SS_{\text{res}_{LR}} - SS_{\text{res}_{SM}}) / (\text{df}_{LR} - \text{df}_{SM})}{\text{MSE}_{SM}}$
  5. In the example: Reject $\rightarrow$ Cubic fit, $y' = b_0 + b_1x + b_2x^2 + b_3x^3$.
- **Guidance**:
  - Even if the model fits well, it may not be (and probably isn’t) the best model.
  - **Principle of parsimony**: Find the simplest model (i.e. the model with the smallest number of predictors) that explains the most variation.


## Strategy of the Lack of Fit Test
- ANOVA compares the equal means model to a linear regression model.
- Lack of fit test – when it fails to reject, the model is comparable with the best fitting model.


### Example in SAS:
```{sas eval=FALSE}
/* linear regression model */
proc glm data = IronCor;
model Corrosion = IronContent / solution;
run;

/* separate means model (7 groups) */
proc glm data = IronCor;
class IronContent;
model Corrosion = IronContent;
run;

data critval;
criticalValue = finv(0.95, 5, 6);
run;
proc print data = critval;
run;

data pval;
pValue = 1-probf(9.28, 5, 6);
run;
proc print data = pval;
run;

```


::: {layout-ncol=2}
![](images/notes_10_7_sas1.png){width=5in fig-align="left"}

![](images/notes_10_8_sas2.png){width=5in fig-align="left"}
:::

---

::: {layout-ncol=2}
![](images/notes_10_9_extrasumofsquares.png){width=5in fig-align="left"}

![](images/notes_10_10_sas3.png){width=5in fig-align="left"}
:::

---

::: {layout-ncol=2}
![](images/notes_10_11_sas4.png){width=5in fig-align="left"}
:::

- $H_0$: Linear regression model has a good fit. (No lack of fit.)
- $H_A$: The SMM fits better. (LRM is lacking fit.)
- There is strong evidence to suggest the linear regression model has a lack of fit with respect to the separate means model. (It is not comparable with Messi.)


## References
