---
title: "Regression Diagnostics and Model Refinement"
execute:
  engine: knitr
---

## Objectives
- Calculate and interpret residuals, standardized residuals, and studentized residuals.
- Use residuals to check regression assumptions.
- Apply necessary remedies when assumptions are violated.
- Understand the robustness of assumptions (i.e. what can you get away with?).


## Robustness of Regression Assumptions

### Linearity
- Parameter estimates will be misleading if a straight-line model is inadequate or fits only part of the data. Predictions will be biased, and confidence intervals (CI) will not appropriately reflect uncertainty.
- **Remedy:** Consider adding polynomial terms (quadratic or cubic) to improve model fit.

### Normality
- Transformations that correct for normality often address constant variance as well.
- **Effects of non-normality:**
  - **Coefficient estimates & standard errors:** Robust, except with many outliers and small sample sizes.
  - **Confidence intervals:** Affected primarily by outliers (long tails).
  - **Prediction intervals:** Sensitive to non-normality due to their reliance on normal distributions.

### Constant Variance (Homoscedasticity): for every value of x, the spread of y is the same.
- Least squares estimates remain unbiased with slight violations. However, large violations cause standard errors to misrepresent uncertainty.
- **Remedy:** Large violations should be corrected using a transformation.

### Independence
- **Parameter estimates:** Not affected by violations.
- **Standard errors:** Affected significantly. Smaller standard errors make hypothesis testing misleading (make it easier to reject).
- **Remedy:** Serial and cluster effects require different models.

### How much deviation from assumptions still give accurate estimates?
- Small violations are acceptable.
- Transformations can often help.
- Only severe departures from linearity or normality (due to outliers) require alternative methods.


## Influential and Outlying Observations

### Key Concepts
- **Influential observations:** Does adding one point change the slope?
- **Leverage:** Reflects how far a point is from $\bar{x}$. Points that are farther have higher leverage.
  - Function of distance from $x_i$ to $\bar{x}$ in standard deviations.
  - Function of the proportion of total sum of squares contributed by $x_i$.

### Impact of Outliers
- **Low leverage, low influence:** Minimal effect on estimates.
- **High leverage, low influence:** Far from most data but consistent with trend; minimal effect on correlation, standard error, and estimates.
- **High leverage, high influence:** Can distort results by pulling the regression line toward the outlier resulting in different parameter estimates.

### How do we detect them? How far is too far?

#### Leverage Statistic, $h_{ii}$
- Measures the influence of $x$ values on predictions (how far the $x$ values are from the $\bar{x}$).
- **High leverage:** When $h_{ii} > \frac{2p}{n}$ (where $p$ is the number of parameters).
- Calculation:
  $$
  h_{ii} = \frac{1}{n} + \frac{(X_i - \bar{X})^2}{\sum_{j=1}^{n} (X_j - \bar{X})^2}
  $$
  **or**
  $$
  h_{ii} = \frac{1}{n-1} \left[ \frac{(X_i - \bar{X})}{s_x} \right]^2 + \frac{1}{n}
  $$


## Types of Residuals
- **Standardized:** $e_i / \text{RMSE}$
- **Studentized:** $e_i / \sqrt{\text{MSE} \cdot (1 - h_{ii})}$
- **Studentized-deleted (R-student):**
  - Remove an observation, recalculate the regression, and compute the studentized residual.
  - Standard deviation is calculated without the point in question.
  - Large residual values indicate influential points.
  - Formula:
    $$
    \text{RSTUDENT} = \frac{\text{res}_i}{s_i \sqrt{1 - h_i}}
    $$
- **Why different types?** To normalize variance and make residuals comparable.


## Leave-One-Out Statistics  
$\hat{Y}_{i(i)}$ is the predicted value of $Y_i$ when the $i$-th observation is left out of the regression model.

### PRESS (Predicted Residual Sum of Squares)
$$
\text{PRESS}_p = \sum_{i=1}^n \left(Y_i - \hat{Y}_{i(i)}\right)^2 = \sum e_{i(i)}^2
$$
- A smaller PRESS value indicates a better-fitting model.

### Cook’s Distance ($D_i$)
- Blends residuals and leverage:
$$
D_i = \sum_{i=1}^n \frac{\left(\hat{Y}_{i(i)} - \hat{Y}_i\right)^2}{p \cdot \text{MSE}} = \frac{1}{p} (\text{studres}_i)^2 \left[\frac{h_i}{1 - h_i}\right]
$$
- $p =$ number of parameters.


## Durbin-Watson Test for Independence
$$
d = \frac{\sum_{i=1}^n (e_i - e_{i-1})^2}{\sum_{i=1}^n e_i^2}
$$
- Values near 0 indicate positive correlation; values near 4 indicate negative correlation. Distributed symmetrically about 2.
- Available in R and SAS.


## Recap

| **Name**                | **Expression**                                       | **Use**                        |
|-------------------------|-----------------------------------------------------|--------------------------------|
| **Residual**            | $e_i = y_i - \hat{y}_i$                              | Residual plots                |
| **Standardized residual** | $r_i = \frac{e_i}{s \sqrt{1 - h_{ii}}}$            | Identify outliers             |
| **Studentized residual** | $t_i = \frac{e_i}{s_{(i)} \sqrt{1 - h_{ii}}}$       | Test outlying $Y$’s           |
| **Deleted residual**     | $e_{i(i)} = y_i - \hat{y}_{i(i)} = \frac{e_i}{1 - h_{ii}}$ | Calculate PRESS               |

> At $\alpha = 0.05$, we expect 5% of studentized residuals to be greater than 2 or less than -2.

![Residual Panel](images/notes_10_1_residual_panel.png)


## Graphical Assessment of Residuals

| **Pattern**             | **Potential Issue** | **Solution** |
|-------------------------|--------------------|-------------|
| Linear means, constant SD | Model fits well | No action needed |
| Curved means, equal SD | Nonlinearity | Transform $X$ |
| Curved means, increasing SD | Nonlinearity + heteroscedasticity | Transform $Y$ |
| Skewed residuals | Non-normality | Can model mean, but CIs/PIs unreliable. Consider transformations. |
| Linear means, increasing SD | Heteroscedasticity | Use weighted regression |

![Regression Patterns](images/notes_10_2_regression_patterns.png){width=5in fig-align="left"} 


## Transformation Recommendations
1. Log
2. Square root
3. Other

### Interpretation Considerations
- Adjust wording for the audience and transformed variables.
- Back-transform results if needed.


## Remedies for Violations

### Nonlinearity: 
- More complicated models
- Transformation on X
- Add another variable (might help nonconstant variance too)

### Nonconstant variance:
- Transformation on Y
- Weighted least squares – down weight observations with larger variance, so they don’t influence the regression model as much as observations closer to the line

### Correlated errors: In residual plots, you may see small values follow small values, and large follow large.
- Serial effects within data -> time series or spatial models

### Outliers
- Use robust regression procedures
- Check for data entry or other errors (only delete in this situation)

### Non-normality
- Fix last, usually fixed with the above
- Transform the data

![Residual Patterns](images/notes_10_3_regression_patterns.png){width=6in fig-align="left"} 
Source: @ramsey2012statistical.


## Log Transformation: They Work & Are Easy to Interpret

- **Log on response (log-linear):**  
  $\log\hat{Y}_i = \beta_0 + \beta_1 X_i$
  - Mean of the response is log-linearly related to the explanatory variable.
  - $\text{Median}\{Y|X\} = \exp(\beta_0) \exp(\beta_1 X)$
  - One unit increase in $X$ → multiplicative change in $\exp(\beta_1)$ in the median of $Y|X$.
  - $\text{Median}\{Y|(X + 1)\} / \text{Median}\{Y|X\} = \exp(\beta_1)$
- **Log on explanatory variable (linear-log):**  
  $\hat{Y}_i = \beta_0 + \beta_1 \log X_i$
  - Depends on log base (e.g. $2$ → doubling, $10$ → ten-fold).
- **Both logged (log-log):**  
  $\log\hat{Y}_i = \beta_0 + \beta_1 \log X_i$
  - **Complicated** :/
  - Doubling (or tenfold increase) of $X$ is associated with a change of $2^{\beta_1}$ (or $10^{\beta_1}$) in the median of $Y$.


## Interpretations of Log Transformation Varieties

- **Log-linear model:**  
  A one unit increase in $X$ is associated with a multiplicative change of $e^{\beta_1}$ in the median of $Y|X$.
- **Linear-log model:**  
  A doubling of $X$ is associated with a $\beta_1 \log(2)$ unit change in the mean of $Y|X$.
- **Log-log model:**  
  A doubling of $X$ is associated with a $2^{\beta_1}$ multiplicative change in the median of $Y|X$.
- **Remember:** Logging $Y$ → median.


::: {layout-ncol=2}
![Log-Linear Transformation](images/notes_10_4_loglinear.png)

![Linear-Log Transformation](images/notes_10_5_linearlog.png)
:::

---

::: {layout-ncol=2}
![Log-Log Transformation](images/notes_10_6_loglog.png)
:::

## Formal Test for Lack of Fit
- **Use:** different values of Y for replicated observations of X
- **F-test for lack of fit** has the usual assumptions:
  - Normality of $Y|X$,
  - Independence of $(X, Y)$ pairs,
  - Constant variance of $Y$ across all values of $X$.

- **Procedure**:
  1. Fit a linear regression and obtain $SS_{\text{res}_{LR}}$ (linear fit).
  2. Fit an ANOVA and use replicated $X$ values as the treatment variable to obtain $SS_{\text{res}_{SM}}$ (separate means model).
  3. **Null hypothesis**: The linear model fits.  
     **Alternative**: Variability cannot be explained by the model.
  4. Compute:
     $F = \frac{(SS_{\text{res}_{LR}} - SS_{\text{res}_{SM}}) / (\text{df}_{LR} - \text{df}_{SM})}{\text{MSE}_{SM}}$
  5. In the example: Reject $\rightarrow$ Cubic fit, $y' = b_0 + b_1x + b_2x^2 + b_3x^3$.
- **Guidance**:
  - Even if the model fits well, it may not be (and probably isn’t) the best model.
  - **Principle of parsimony**: Find the simplest model (i.e. the model with the smallest number of predictors) that explains the most variation.


## Strategy of the Lack of Fit Test
- ANOVA compares the equal means model to a linear regression model.
- Lack of fit test – when it fails to reject, the model is comparable with the best fitting model.


### Example in SAS:
```{sas eval=FALSE}
/* linear regression model */
proc glm data = IronCor;
model Corrosion = IronContent / solution;
run;

/* separate means model (7 groups) */
proc glm data = IronCor;
class IronContent;
model Corrosion = IronContent;
run;

data critval;
criticalValue = finv(0.95, 5, 6);
run;
proc print data = critval;
run;

data pval;
pValue = 1-probf(9.28, 5, 6);
run;
proc print data = pval;
run;

```


::: {layout-ncol=2}
![](images/notes_10_7_sas1.png){width=5in fig-align="left"}

![](images/notes_10_8_sas2.png){width=5in fig-align="left"}
:::

---

::: {layout-ncol=2}
![](images/notes_10_9_extrasumofsquares.png){width=5in fig-align="left"}

![](images/notes_10_10_sas3.png){width=5in fig-align="left"}
:::

---

::: {layout-ncol=2}
![](images/notes_10_11_sas4.png){width=5in fig-align="left"}
:::

- $H_0$: Linear regression model has a good fit. (No lack of fit.)
- $H_A$: The SMM fits better. (LRM is lacking fit.)
- There is strong evidence to suggest the linear regression model has a lack of fit with respect to the separate means model. (It is not comparable with Messi.)


## References
