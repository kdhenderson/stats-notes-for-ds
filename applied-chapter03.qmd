---
title: "The Bootstrap"
---

## Objectives

This chapter covers the bootstrap procedure, its benefits, and its role in statistical inference and predictive modeling.

## Bootstrap Procedure

### Overview
The bootstrap is a resampling method that allows us to:
- Estimate the variability (uncertainty) of statistical estimators.
- Perform statistical inference, such as hypothesis testing and confidence intervals.
- Enhance predictive modeling by stabilizing estimates in high-variance models.

### Key Benefits
Difficulties in Multiple Linear Regression (MLR) that bootstrap addresses:
- No transformation adequately satisfies model assumptions.
- The transformation that works hinders interpretability, which is needed for the situation.
- A complex model is required but introduces high variance (overfitting).

The bootstrap is heavily used in **nonparametric predictive modeling**, such as **random forests**, where high variance is common, and it can be used in many statistical procedures.

### Conceptual Understanding
- **Standard Errors and Uncertainty**:
  - When assumptions are not met, standard errors (SE) are often incorrect.
  - SE is critical for quantifying the variability of estimators like the mean.
  - Example: The standard error of the mean is:
    $$
    SE(\bar{X}) = \frac{\sigma}{\sqrt{n}}
    $$
  - However, for statistics like the **median** or **trimmed mean**, , there is no simple theoretical SE formula. This makes bootstrap-based estimation particularly valuable.

### Impact of Outliers on SE
- The mean is highly sensitive to outliers, while the median is more robust.
- The standard error of the mean has a known formula:
  $$
  SE(\bar{X}) = \frac{\sigma}{\sqrt{n}}
  $$
  whereas the SE for the median is unknown.
- The bootstrap provides a way to estimate SE for any statistic, even when no closed-form solution exists.

### Key Analogy
- The **population** is to the **sample** as the **sample** is to the **bootstrap samples**.

### Main Idea
- The bootstrap mimics the process of generating a sampling distribution.
- The resulting bootstrap distribution allows:
  - Estimation of SE for any statistic.
  - Computation of confidence intervals (CIs).
- The standard deviation of the bootstrap distribution directly serves as an estimate of the standard error of the statistic because each bootstrap sample has the same size as the original dataset.

### Pseudo-Code for Bootstrap Sampling
1. Determine the sample size $n$.
2. Randomly sample $n$ observations **with replacement** from the dataset to obtain the a bootstrap sample.
3. Compute the statistic of interest (e.g. mean, median).
4. Repeat steps 2 and 3 **$B$** times (typically $B = 1000$ or more).
5. Use the resulting bootstrap distribution for inference.


## Statistical Inference Using the Bootstrap

### Bootstrap Confidence Intervals
The bootstrap is commonly used to construct confidence intervals (CIs). Five major approaches:
1. Percentile Bootstrap (Basic)
2. Empirical Bootstrap
3. Bootstrap $t$-Intervals
4. Bias-Corrected and Accelerated (BCa) Bootstrap
5. ABC Method

#### Percentile Bootstrap Intervals
- Uses the empirical distribution of the bootstrap samples, which is the most intuitive approach and easies to implement.
- For a **95% CI**, take the **2.5th and 97.5th percentiles** from the bootstrap distribution.
- No parametric assumptions of the distribution of data are required.

##### Issues:
- May not achieve the nominal coverage probability (e.g. a 95% CI may only contain the true parameter 90% of the time).
- Causes:
  - Bias in the center of the bootstrap distribution.
  - Skewness in the bootstrap distribution.
- Problems arise when dealing with **nuisance parameters** (e.g. variance in regression models).

#### Alternative Approaches
Several adjustments improve upon the percentile bootstrap:
- Empirical Bootstrap (addresses bias only).
- Bootstrap $t$-intervals.
- BCa Bootstrap (corrects for both bias and skewness).
- ABC Method (can be computationally demanding).
  

## Bootstrapping in Multiple Linear Regression

There are two main bootstrap approaches for MLR:

1. **Bootstrapping Pairs** handles violations of normality and constant variance.
2. **Bootstrapping Residuals**
   - Assumes normality of residuals.
   - Addresses high leverage points by resampling residuals rather than entire observations.
   - Not recommended when there are violations of constant variance. A special type, the wild bootstrap is good for handling heteroskedasticity.
  
### Bootstrapping Pairs
 **Process**:
  - Sample entire rows of data (predictor-response pairs) with replacement.
  - Each observed response value remains paired with its corresponding predictor values (i.e. values in the whole row stay together).
  - Fit the regression model on each resampled dataset (bootstrap sample).
  - Obtain a sampling distribution of regression coefficients.
- **Key Benefit**:
  - Preserves the original structure of the data, including any existing nonconstant variance.

### Bootstrapping Residuals
- **Process**:
  1. Fit the model $Y = f(X) + \epsilon$ to obtain residuals.
  2. Resample the residuals with replacement.
  3. Create new responses using: $Y^* = \hat{Y} + \text{bootstrap residual}$
     - The new bootstrapped response is created adding the bootstrapped errors to the predicted values from the original fit.
  4. Fit the model to the new dataset and store coefficients to build the sampling distribution.
- **Limitations**:
  - Assumes independence.
  - Not recommended when variance is nonconstant, as the nonconstant variance property is lost.
  - May drop low-frequency categorical levels:
    - Consider collapsing levels before bootstrapping.
    - Otherwise, a level might be missing, causing the corresponding coefficient to be omittedâ€”this is especially problematic when coding your own bootstrap procedure.
- **Implementation**:
  - The `lmboot` R package provides tools for bootstrapping residuals.


Bootstrap Inferencing Recap
- bootstrapping allows us to estimate SEs for statistical estimates in a nonparametric way
- we don't have to attach assumptions other than independence
- provides a way to produce SEs of any statistic
- there are numerous ways to produce a CI under the bootstrap strategy
  - percentile is easiest to implement
  - consider alternatives that perform better
  - $BC_A$ and bootstrap t-interval tend to have better approximation properties that percentile
  - always exceptions (e.g. small sample sizes hard to estimate skewness, so $BC_A$ has probs)
- paired bootstrap is preferred in regression when dealing with nonconstant variance
- if you are concerned your model is too simple and doesn't capture true trend, any bootstrap regression approach can obtain better SE estimates than MLR
- a bootstrap can be used to provide inference on penalized regression coefficient estimates
- there are also parametric bootstrap procedures
    - assumptions are still in place
    - resamples come from a theoretical distribution (simulated dataset rather than from your actual dataset)
- bootstrap is only as good as the information your sample contains (representative samples are important)
  - nonparametric method can get lumped as small sample size methods, but sample size in bootstrap needed a representative sample
  - small data sets cannot guarantedd that we have a good sense of the true distribution


## Ensembling and Bagging

Bootstrap's role in predictive modeling
- Leo Breiman (one of the first to use in a non-classical CI way), developed bagging

Ensembling
- taking a training data set
- fit multiple models
- make a new prediction for the same observation with each model
- average the predictions for all those models

How does ensembing help?
- the average of a set of values has a smaller variance than the individual errors
- Each model's predictions has its own variance (MSE)
- Ensembles average predictions, so the variance (MSE) for the ensemble tends to get smaller as more models are used in the ensemble
- The drop in variance is not as strong as for X-bar because the models were built from the same training data.
- An ensemble prediction tends to smooth out the overfitting. It dampens the variance.

Bias-variance trade-off in ensembling
1. fit models that are highly complex on purpose that have high variance
2. the averaging dampens the variance.
3. the final model could have low bias and lower variance (it has to start with low bias)

Bootstrap Aggregation
- Bootstrap aggregation (i.e. bagging) is a modificaion of the ensembling strategy to boost prediction performace of a single modeling tool.
- Ensembling uses a bunch of different tools.
- You can "bag" a MLR model or "bag" a tree model, etc.
- Since the modeling tool doesn't change, and there is one training set, you'd get the same model every time, so there is no benefit to ensembling in that case.
- Solution: get different model fits by using different data sets each time with bootstrap resampling.

---

## Recap

### Bootstrap Inference
- The bootstrap enables nonparametric estimation of standard errors (SEs) without strict distributional assumptions.
  - The independence assumption still applies.
  - Bootstrap regression SE estimates tend to be better than MLR SE estimates if the model is too simple and fails to capture the true trend or complexity.
- Several approaches exist for constructing confidence intervals, with BCa and bootstrap $t$-intervals being more robust than percentile bootstrap.
  - Exception: Avoid BCa for small sample sizes, as estimating skewness is unreliable.
- Paired bootstrap is preferred for regression models with nonconstant variance.
- The bootstrap can provide inference on penalized regression coefficient estimates.
- Parametric bootstrap methods exist, but they retain model assumptions.
  - Resamples come from a theoretical distribution rather than from the actual dataset (i.e. a simulated dataset).
- Representative samples are crucial for accurate inference.
  - Although bootstrapping is often used in nonparametric small-sample methods, it still requires a representative sample to capture the true distribution.
  - Small sample sizes may fail to provide an accurate representation of the population, limiting the effectiveness of bootstrapping.

--- 

Bagging recap
- invaluable tool that utilizes ensembling for a single prediction tool
    - helpful for tools that suffer from high variance
    - lose interpretation for the gain in prediction performance

- not as helpful for things like MLR (they tend to suffer from bias because they aren't complex enough)
  - argued to help in situations such as feature selection where models can be much more unstable

Bootstrap inferencing - recommend 1000 - 5000 bootstrap samples

Bagging - recommend 100-500 bootstrap samples

Bagging also provides a way to comput a validation MSE for free
  - commonly referred to as the "out of bag error"
    - each bootstrap sample has observations that were not included
    - mimics the holdout of k-fold

Bootstrap procedure and its applications is one of the biggest contributions to statistics and data science over the past 40 years.

For data scientists, its use in predictive models is critical to understand, as it can help both current methods and future ones to come.
