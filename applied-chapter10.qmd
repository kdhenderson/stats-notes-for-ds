---
title: "Multiple Logistic Regression"
---

## Overview

This chapter introduces multiple logistic regression as both a classification tool and a method for statistical inference. We will discuss modeling strategies, communication strategies, and the general workflow for using multiple logistic regression in applied settings.


## Motivations for Multiple Logistic Regression

### Simple vs. Multiple Logistic Regression
- **Simple logistic regression**:
  - Assumes no confounders
  - Estimates the effect of a single predictor, ignoring all others
- **Multiple logistic regression**:
  - Adjusts for other variables while estimating the effect of one predictor
  - Allows for both numerical and categorical predictors  

### Simpson’s Paradox
- An example of a confounding variable distorting the association between predictors and response
- **Crane/Eagle Math/Physics Example**:
  - You are more likely to pass if you are a math student versus a physics student.
  - Almost all Eagles students were math students.
  - Crane had an even distribution of math and physics students.
  - A single 2x2 table ignores this confounding info.
  - Logistic regression allows for comparison of schools while holding department fixed.
  - Interaction effects between predictors (e.g. school × department) can also be assessed.

### Prediction vs. Explanation
- Determining important health factors is a statistical inference problem (explanation).
  - Not only identifying importance but quantifying it
- Clinicians are also interested in predicting disease probability.

#### Case Study Examples
- **CAD Study**: Predicting coronary artery disease using sex, age, and ECG.
  - Questions: Is sex a risk factor after accounting for age and ECG? Are there interactions?
- **Titanic**: Survival prediction using ticket class, age, and sex.
  - Questions: What contributed most to survival? Do the effects depend on each other?


## The Multiple Logistic Regression Model

For multiple predictors $X = (X_1, X_2, \ldots, X_p)$, the general logistic regression model is:
$$
\log\left( \frac{p(X)}{1 - p(X)} \right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p
$$
  
### Interpretation

- **Additive models**:
  - Each predictor appears only once in the model.
  - $e^{\hat{\beta}}$ can be interpreted as an odds ratio
  - Interpretation assumes "holding all other variables fixed"

- **Complex models**:
  - Include interaction and/or polynomial terms
  - Odds ratios are still interpretable but require careful consideration of the regression formula
  - **Effects plots** are often used for graphical interpretation 

### Classification Boundaries
- **Additive**: linear boundaries (only numeric predictors)
  - Similar to LDA but does not require predictors to be normally distributed
- **Complex**: nonlinear boundaries arise from:
  - Categorical × continuous interactions (unique boundary per level)
  - Continuous × continuous or polynomial terms (curved/multiple boundaries)
  - Categorical × categorical: N/A


## Feature Selection

### Penalized Logistic Regression
- Use `glmnet` with `family = "binomial"` for logistic regression
- Useful for regularization and feature selection
    
### Stepwise Selection
- Forward, backward, and stepwise approaches
- Best subset selection
- Compare models using:
  - AIC
  - LogLoss or Brier Score on validation set or K-fold cross-validation
 
### Separability Problem
- When training data is perfectly separated:
  - A good thing! Indicates strong, important predictors
  - However, maximum likelihood estimates, i.e. the regression coefficient that minimizes the logLoss is $\hat{\beta} = \infty$
  - Software may fail or issue warnings
  - Resulting predicted probabilities are exactly 0 or 1 → no interpretation can be made, and no estimates or confidence intervals are possible  
- **Solutions**:
  - For explanation: penalization, Firth’s logistic regression
  - For prediction: Firth’s logistic, penalized logistic (GLM-NET), or alternative models (e.g. LDA)


## Complex Logistic Models

### Modeling Strategy
- When including high-order interaction terms:
  - Also include lower-order interactions
    - e.g. if modeling a 3-way interaction, also include all 2-way interactions
  - Use ANOVA-style tests to assess overall significance at each level of complexity
  - Remove non-significant higher-order interactions and re-assess
  - Compare models using:
    - Hosmer-Lemeshow test
    - Validation or cross-validation metrics
  
### Coefficient Interpretation
- Follow similar strategy as in MLR:
  - Determine the appropriate contrast to combine regression coefficients to estimate specific comparison or trend based on the significant interactions
  - Exponentiate to obtain odds ratio interpretation

- For highly complex models:
  - Interpretation becomes difficult
  - Use effects plots for visualization
  
### Effects Plots
- Show predicted probabilities across 1–3 variables
- Other variables are held fixed:
  - Categorical: reference level (usually, depends on software)
  - Continuous: mean or median (depends on software)    

> **Note**: Effects plots are not part of EDA; they visualize predictions from your fitted model. They can look simple or complex, depending on model structure—not the underlying raw data.
  
 
## General Workflow

The general workflow for multiple logistic regression is similar to that of multiple linear regression.

### Modeling Steps

1. Define primary question(s) and predictor(s)
2. Identify potential confounders and covariates, i.e. additional variables for which you want to account
3. Perform EDA and data cleaning:
   - Summary statistics and plots
   - Explore possible interactions
4. Fit candidate models:
   - Based on insights from EDA
   - Use ANOVA tests or feature selection to decide on complexity
5. Assess model fit:
   - Hosmer-Lemeshow test
   - Evaluate metrics on validation or K-fold CV
6. Interpret final model:
   - Odds ratios (additive models)
     - Complex models with interactions can also be interpreted, taking care with contrasts 
   - Effects plots (complex models)
   
### Classification Considerations

- Try multiple classification tools (e.g. non-parametric approaches) comparing error metrics
- Choose threshold based on context (e.g. cost of misclassification types)
- Validate both model and threshold using:
  - Independent test set
  - Entirely new dataset
- Set up a monitoring strategy:
  - Regular checks on prediction accuracy
  - Adapt to changes in data collection or algorithm dynamics
