---
title: "Multiple Logistic Regression"
---

## Overview

This chapter introduces multiple logistic regression as both a classification tool and a method for statistical inference. We will discuss modeling strategies, communication strategies, and the general workflow for using multiple logistic regression in applied settings.

## Overview

- Introduce multiple logistic regression as a tool for classification and statistical inference.  
- Explore the benefits of using multiple predictors, including adjusting for confounders.  
- Understand how to interpret coefficients and interactions.  
- Learn how to assess model fit and compare alternative models.  
- Review strategies for feature selection and model complexity.  
- Apply a general modeling workflow in practical settings.


## Motivations for Multiple Logistic Regression

### Simple vs. Multiple Logistic Regression
- **Simple Logistic Regression**:
  - Assumes there are no confounding variables.  
  - Estimates the effect of a single predictor **without accounting for other factors**.  
- **Multiple Logistic Regression**:
  - Adjusts for additional variables while estimating the effect of one predictor.  
  - Allows for both numerical and categorical predictors.  

### Simpson’s Paradox
- Anexample where a confounding variable distorts the observed relationship between predictors and the response.  
- **Crane/Eagle Math/Physics Example**:
  - Math students are more likely to pass than physics students.  
  - Most Eagle students were in math, while Crane had an even mix.  
  - A single 2×2 table masks this confounding structure.  
  - Logistic regression enables comparisons between schools **while holding department fixed**.  
  - Can also test for interaction effects (e.g., school × department).  

### Prediction vs. Explanation
- Identifying important health risk factors is an explanatory modeling goal—--a statistical inference problem.  
  - The goal is not just to detect which variables matter, but to quantify their effects while accounting for other factors.  
- In contrast, clinicians may also be interested in predicting the probability of disease for individual patients.  
  - The emphasis is on accurate predictions, not necessarily understanding the individual role of each variable.

#### Case Study Examples
- **CAD Study**: Predicting coronary artery disease using sex, age, and ECG results.  
  - Questions: Is sex still a risk factor after adjusting for age and ECG? Are there interaction effects?  
- **Titanic**: Predicting survival based on ticket class, age, and sex.  
  - Questions: Which factors had the largest impact on survival? Do the effects depend on each other?  


## The Multiple Logistic Regression Model

For multiple predictors $X = (X_1, X_2, \ldots, X_p)$, the general logistic regression model is:
$$
\log\left( \frac{p(X)}{1 - p(X)} \right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p
$$

This model expresses the **log odds** of the outcome as a linear combination of predictors.

### Interpretation
- **Additive models**:
  - Each predictor appears once in the model.
  - $e^{\hat{\beta}}$ is interpreted as an odds ratio.
  - Interpretation assumes we are “holding all other variables fixed.”
- **Complex models**:
  - Include interaction terms and/or polynomial terms.
  - Odds ratios are still interpretable but require careful consideration of the regression formula.
  - **Effects plots** are commonly used for visual interpretation in these cases.

#### Odds Ratios in Complex Models

When a model includes interaction terms, odds ratios must be interpreted in the context of the full model. Here is an example:

Model:  
$$
\log\left( \frac{p}{1 - p} \right) = \beta_0 + \beta_1 \cdot \text{Age} + \beta_2 \cdot \text{Smoking} + \beta_3 \cdot (\text{Age} \times \text{Smoking})
$$

- **Age**: continuous  
- **Smoking**: binary (1 = smoker, 0 = non-smoker)

Interpretation:

- For **non-smokers** (`Smoking = 0`):  
  Odds ratio for a 1-year increase in age is $\exp(\beta_1)$
- For **smokers** (`Smoking = 1`):  
  Odds ratio for a 1-year increase in age is $\exp(\beta_1 + \beta_3)$

> **Key idea**: Odds ratios remain interpretable but must account for interaction terms and the full model formula.

### Classification Boundaries
- **Additive models**:
  - Produce linear decision boundaries (when predictors are numeric).
  - Similar to LDA but without assuming normality of predictors.
- **Complex models**:
  - Can produce nonlinear boundaries, depending on the terms included:
    - Categorical × continuous: separate linear boundary per category
    - Continuous × continuous or polynomial: curved or non-parallel boundaries
    - Categorical × categorical: nonlinear effects, but not meaningful to describe in terms of a boundary shape

---

## Feature Selection

### Penalized Logistic Regression
- Use `glmnet` with `family = "binomial"` for logistic regression
- Useful for regularization and feature selection
    
### Stepwise Selection
- Forward, backward, and stepwise approaches
- Best subset selection
- Compare models using:
  - AIC
  - LogLoss or Brier Score on validation set or K-fold cross-validation
 
### Separability Problem
- When training data is perfectly separated:
  - A good thing! Indicates strong, important predictors
  - However, maximum likelihood estimates, i.e. the regression coefficient that minimizes the logLoss is $\hat{\beta} = \infty$
  - Software may fail or issue warnings
  - Resulting predicted probabilities are exactly 0 or 1 → no interpretation can be made, and no estimates or confidence intervals are possible  
- **Solutions**:
  - For explanation: penalization, Firth’s logistic regression
  - For prediction: Firth’s logistic, penalized logistic (GLM-NET), or alternative models (e.g. LDA)


## Complex Logistic Models

### Modeling Strategy
- When including high-order interaction terms:
  - Also include lower-order interactions
    - e.g. if modeling a 3-way interaction, also include all 2-way interactions
  - Use ANOVA-style tests to assess overall significance at each level of complexity
  - Remove non-significant higher-order interactions and re-assess
  - Compare models using:
    - Hosmer-Lemeshow test
    - Validation or cross-validation metrics
  
### Coefficient Interpretation
- Follow similar strategy as in MLR:
  - Determine the appropriate contrast to combine regression coefficients to estimate specific comparison or trend based on the significant interactions
  - Exponentiate to obtain odds ratio interpretation

- For highly complex models:
  - Interpretation becomes difficult
  - Use effects plots for visualization
  
### Effects Plots
- Show predicted probabilities across 1–3 variables
- Other variables are held fixed:
  - Categorical: reference level (usually, depends on software)
  - Continuous: mean or median (depends on software)    

> **Note**: Effects plots are not part of EDA; they visualize predictions from your fitted model. They can look simple or complex, depending on model structure—not the underlying raw data.
  
 
## General Workflow

The general workflow for multiple logistic regression is similar to that of multiple linear regression.

### Modeling Steps

1. Define primary question(s) and predictor(s)
2. Identify potential confounders and covariates, i.e. additional variables for which you want to account
3. Perform EDA and data cleaning:
   - Summary statistics and plots
   - Explore possible interactions
4. Fit candidate models:
   - Based on insights from EDA
   - Use ANOVA tests or feature selection to decide on complexity
5. Assess model fit:
   - Hosmer-Lemeshow test
   - Evaluate metrics on validation or K-fold CV
6. Interpret final model:
   - Odds ratios (additive models)
     - Complex models with interactions can also be interpreted, taking care with contrasts 
   - Effects plots (complex models)
   
### Classification Considerations

- Try multiple classification tools (e.g. non-parametric approaches) comparing error metrics
- Choose threshold based on context (e.g. cost of misclassification types)
- Validate both model and threshold using:
  - Independent test set
  - Entirely new dataset
- Set up a monitoring strategy:
  - Regular checks on prediction accuracy
  - Adapt to changes in data collection or algorithm dynamics
