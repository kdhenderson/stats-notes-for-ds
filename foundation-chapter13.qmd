---
title: "Model Selection and Validation"
---

## Objectives

- Describe the proper order for a regression analysis.
- Use residuals to check regression assumptions.
- Apply the necessary remedies when the assumptions are violated.
- Apply variable selection algorithms and interpret the results.
- Understand statistical and conceptual issues for automatic variable selection in multiple linear regression.


## Parameter estimates are highly context dependent.

- The magnitude of a correlation coefficient can vary depending on factors such as:
  - The range of values observed for the variables.
  - The precision and reliability of the measurement instruments or techniques.
  - The characteristics of the units being studied.
- A specific slope estimate in a regression model can be affected by:
  - The range of $x$ and $y$ values in the data.
  - The characteristics of the units or cases in the sample.
  - The context provided by the specific set of predictor variables included in the model.
- Example: A tentative model for alcohol metabolism in men and women should:
  - Contain variables whose values best answer the research questions in a straightforward manner.
  - Include potentially confounding variables.
  - Include features that capture important relationships found in the initial graphical analysis (e.g., variables that are highly correlated with the response in a scatterplot matrix).


## Inferences on Partial Slopes

- Examine individual tests for each slope in the model to determine statistical significance. Variables can be excluded if the slope is not statistically significant.
- **Variance inflation factor (VIF):** An $R^2$ value that represents the proportion of variance in a particular explanatory variable explained by all other predictor variables.  
  - A large $R^2$ means the value is well predicted by other variables and may not be needed.  
  - A small $R^2$ means the variable is not well predicted by others and is likely needed.

## Collinearity

- Some explanatory variables can be well explained by other explanatory variables in the mode and including them would be redundant.


## First-Pass Model

- Look for variables that are not statistically significant. These could be three-way interactions, two-way interactions, or single variables.
- Consider excluding these variables. Check whether the variable is collinear with others in the model by examining the VIF.
  - This value is relative; if VIF $> 10$, the variable may be redundant.
  - Investigate further with a partial residual plot.
- **Partial residual plot**:
  - Fit the model with $x_1$ and $x_2$ in the model.
  - Calculate partial residuals for $x_2$: $\text{p}_{\text{res}} = y - (\hat{\beta}_0 + \hat{\beta}_1 x_1)$.
  - Create a new dataset with $\text{p}_{\text{res}}$ as the response and $x_2$ as the predictor.
  - Fit a simple linear regression (SLR) of $\text{p}_{\text{res}}$ on $x_2$ to assess the relationship.  
    If the partial residual plot looks linear, keep $x_2$ in the model; if not, consider removing it.

::: {.callout-tip}

### Computing Partial Residuals for an SLR
To create the dataset for the simple linear regression of $\text{p}_{\text{res}}$ on $x_2$:

1. **Fit the full multiple linear regression model** with $x_1$ and $x_2$:  
   $$
   y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \varepsilon
   $$
2. **Compute partial residuals** for $x_2$:  
   $$
   \text{p}_{\text{res}} = y - \hat{\beta}_0 - \hat{\beta}_1 x_1
   $$
3. **Form a new dataset**:
   - **Response**: $\text{p}_{\text{res}}$
   - **Predictor**: $x_2$
4. **Fit a simple linear regression** of $\text{p}_{\text{res}}$ on $x_2$:  
   $$
   \text{p}_{\text{res}} = \beta_0 + \beta_1 x_2 + \varepsilon
   $$  
   The slope from this regression should match the $\hat{\beta}_2$ from the full model.

:::


## Partial Residuals – Mammal Brain Example

We want to understand what is happening after accounting for another variable.

**Suppose we fit the multiple linear regression model**:

$$
\mu(\text{logBrain} \mid \text{logBody}, \text{logGest}) = \beta_0 + \beta_1 \text{logBody} + \beta_2 \text{logGest}.
$$
**To examine the partial effect of `logGest`, we remove the variation explained by `logBody`**:

$$
\mu(\text{logBrain} \mid \text{logBody}, \text{logGest}) = \beta_0 + \beta_1 \text{logBody} + f(\text{logGest}),
$$

which can be rewritten as:

$$
f(\text{logGest}) = \mu(\text{logBrain} \mid \text{logBody}, \text{logGest}) - (\beta_0 + \beta_1 \text{logBody}).
$$

**We estimate this by looking at the residuals**:

$$
\text{p}_{\text{res}} = \text{logBrain} - \beta_0 - \beta_1 \text{logBody}.
$$

Here, $\text{p}_{\text{res}}$ represents what is left after we take out the effect with respect to body size.

**Parameter estimates for the full model**:

| Variable   | DF | Parameter Estimate | Standard Error | t Value | Pr > \|t\| |
|------------|----|--------------------|----------------|---------|------------|
| Intercept  | 1  | -0.4573             | 0.4585         | -1.00   | 0.3212     |
| logGest    | 1  | 0.6678              | 0.1088         | 6.14    | <0.0001    |
| logBody    | 1  | 0.5512              | 0.0324         | 17.03   | <0.0001    |

The partial residuals are computed as:

$$
\text{p}_{\text{res}} = \text{logBrain} - (-0.4573) - 0.5512(\text{logBody}).
$$

**Forming the SLR dataset**:

- **Response variable**: $\text{p}_{\text{res}}$ (one value per observation, from the above formula)
- **Predictor variable**: `logGest` (same values as in the original dataset)

**We then fit a SLR of $\text{p}_{\text{res}}$ on `logGest`**:

$$
\text{p}_{\text{res}} = \beta_0 + \beta_1(\text{logGest}).
$$

**Parameter estimates for the SLR**:

| Variable   | DF | Parameter Estimate | Standard Error | t Value | Pr > \|t\| |
|------------|----|--------------------|----------------|---------|------------|
| Intercept  | 1  | 0.00005             | 0.2767         | 0.00    | 0.9999     |
| logGest    | 1  | 0.6678              | 0.0578         | 11.56   | <0.0001    |

This shows that the information in the partial residuals can be explained by `logGest`.

![**Scatterplot of partial residuals vs. `logGest`.** This plot shows the relationship between `logGest` and the residual variation in `logBrain` after accounting for `logBody`.](images/fch13_mammal_pres.png){width=5in}  

---

## Dealing with outliers

- Review: least squares is not resistant to outliers.
- A robust regression procedure is useful when the response distribution is prone to outliers even with remedies.
- If we use least squares, examine outliers/influence.
  - Are suspect observations influential and why?
  - Do they provide interesting information about the process under study?


## Guidelines

- If the observation differs from the rest of the data in a sparsely represented region (not many values of y measured for a range of x values), restrict the range.
  - Those few points will influence the regression.
  - Document that the range was restricted and why.
- If the observation is not unusual and no other explanation can be found, do not omit. Report results with and without the observation.
- If there is strong reason to believe that the case belongs to a different population, omit it.
- If there is no reason to believe it belongs to a different population, can anything be learned from it? A rare value can reveal information that would’ve been otherwise overlooked.


## Identifying influential cases

- Outlier influence can be tested with measurements including these that use the ‘leave one out’ strategy (big change after leaving one out, that one is influential):
  - DFFITS, difference  in the fits: Measures the difference in fitted values for the ith case when all n cases are used vs. with the ith case excluded.
    - $\text{DFFITS}_i = \frac{\hat{Y}_i - \hat{Y}_{i(i)}}{\sqrt{\text{MSE} \cdot h_{ii}}}$, where hii is leverage statistic. There is an alternate calculation that relates it to studentized residual.
  - DFBetas, difference in the betas: Measures the difference in slopes n cases are used vs. when the ith case is excluded.
  - Cook’s distance: Measures the distance of the outlier and the leverage at the same time (joint influence).
    - Plot Di to identify large values.
    - Removing an influential point may have little effect because there’s another influential point nearby. Could worsen the fit because there’s another influential point opposite to it.
    - High VIF may be due to influential points. Check before removing variables.
- Covariance ratio: if the covariance ration > 1 + (3k/n), deleting the observation adversely affects the accuracy of at least one of the parameter estimates.


## Testing slopes for statistical significance

- Key assumption: the explanatory variables (terms in the model) sufficiently describe the process in question. It is misleading to leave out important variables or to include nonessential terms.
- Find the smallest model that explains the relationship sufficiently and includes all the important explanatory variables.

## Strategies

- Examine data for outliers and influence observations and decide to include or not.
- Use partial residual plot to determine influence. Remove terms that don’t have a linear relationship. Keep in mind that removing non-statistically significant terms can change significance of other terms.
- Remove nuisance variables, terms that don’t help answer the study ?s.
- Check model after every change.


## Strategies for numerous outliers or nonconstant variance

- Weighted least squares weights values with large variants less than values with small variants, so less precise measurements will affect the values less.
- Measurement error models are appropriate when X is measured with error as well as Y. Not an issue if the purpose is prediction.


## Proving causality

- Only possible in randomized experiments
- ood regression model / high r-square show association not causation. We may have left out an important variable.

## Example of putting it together (alcohol metabolism)

1. Plot the data in a scatterplot
2. Develop tentative models (use QOIs, account for confounders and relationships)
3. Fit the model
4. Evaluate the residual plots (for constant SD, normality and zero mean, identification of influential points)


## Example: Alcohol Metabolism

::: {layout-ncol=2}
![](images/notes_13_2.png)

![](images/notes_13_3.png) 
:::

---

::: {layout-ncol=2}
![](images/notes_13_4.png)
:::


## Variable selection

- “Garbage predictors” can enlarge R2 creating a false appearance of prediction.
- How can we determine which of many predictors should be included?
  - Removing and inserting various variables can change the results of the others and can be confusing.


## Automatic variable selection techniques

- Forward selection
- Backward selection
- Stepwise regression
- Sometimes these result in the same model, and sometimes not. Which model makes sense, the smallest?, look at R2, and check regression diagnostics for influential points/variables.


## Forward selection

- Begins with no predictors in the model.
- Fits SLR model for all potential predictors one at a time.
- If first predictor is statistically significant, it is left in the model. Then the next predictor with the best statistical significance is included.
- Predictors are added until R2 doesn’t increase significantly.
- Pros: 
  - Easy to compute and understand
  - Can always be performed, even if n < k (sample size < predictors).
- Cons: 
  - Possible to get a bad initial fit. Maybe it was significant only with no other variables in the model or with the previous variable.
  - Doesn’t guarantee the best subset selections.
  - No check for collinearity, so collinear variables may enter “randomly”.


## Backward elimination

- Begins with all predictors in the model.
- For each predictor, we compute an F-statistic, which tells us how much the overall F decreases when we drop a variable from the model.
- Keep dropping variables from the model until reaching a significant increase in the F-statistic.
  - Drops variables that are neither statistically significant nor cause a large change in R2.
- When we get a large change in R2, we stop dropping variables.
- Pros: 
  - Begin with a good fit (has all the variables).
  - Good initial estimate of s2 (if n is large).
  - Only deletes noninfluential predictors.
- Cons: 
  - Cannot be used if n < k.
  - May have few error degrees of freedom initially, making it harder to select values to omit.
  - Doesn’t guarantee best subset selections. Collinear variables may be deleted randomly.
  - No model (residual) checks at any stage.


## Stepwise selection

- Starts with no predictors.
- First, adds an explanatory variable. Uses simple linear regression for each possible variable and adds the one with the most significant results based on the p-value and t-statistic from a t-test.
- At each stage, performs a backward elimination.
- If you enter a 2nd variable and the 1st is no longer significant, then drop it. (This may be due to collinearity between the variables.) This continues until all terms are statistically significant.
- Thresholds are set for entering / removing variables. Specify a p-value-to-enter (p-valueE = 0.15 or whatever you choose) and a p-value-to-remove (p-valueR = 0.15 or whatever you choose).
P-value statistic is only one option for making these selections.
- Cautions: 
  - All the important predictor variables for predicting Y may not have been identified (type II error).
  - Also, not all the unimportant predictors may have eliminated (type I error).
  - You can make the p-value threshold higher (or different) to subset the variables or use different selection techniques to get candidate models for further analysis.
  

## Select a selection procedure

- No one is universally better.
- We are doing multiple comparisons. The more variables the more tests, and the increase in probability of a type I error.
- Use your knowledge of the data ( :o) human in the loop) Look at the included variables, regression diagnostics, and influence statistics. Ensure assumptions are met and the included variables make sense.


## Variable selection criteria

- P-values
- AIC (penalized for # parameters, smaller the better)
- BIC (or SBC, more penalized for # parameters, smaller better)
- PRESS – leave one out statistic, where you fit on all but one and get a residual for the left out one, do that n times and square the residuals
- Mallow Cp statistic - smaller the better, closest to p
- R2 - larger the better (not overfit)


## Mallow’s Cp – best subset selection

- $C_p = p + (n - p) \times \frac{\hat{\sigma}^2 - \hat{\sigma}^2_{\text{full}}}{\hat{\sigma}^2_{\text{full}}}$
- Model with the Cp closest to p is preferred, where p is the number of parameters including the intercept. The expected Cp for a model with no overfit is p.
- For linear regression, the Cp statistic is equivalent to the AIC.
