---
title: "Type I, Type II Error, and Power"
---

## Objectives

- Define and distinguish between **Type I error**, **Type II error**, and **statistical power**.
- Explain how effect size, sample size, and significance level influence power.
- Calculate power or required sample size using R or SAS.
- Create and interpret power curves to communicate trade-offs in study design.


## Helpful Resources

- [Interactive Power Applet](https://statsthroughsimulation.shinyapps.io/PowerApplet/)
- [SAS Power and Sample Size Documentation](https://support.sas.com/documentation/onlinedoc/stat/141/power.pdf)


## Hypothesis Test Errors and Outcomes

### Type I Error
- If the null hypothesis ($H_0$) is true, the test is correct when it **fails to reject** $H_0$.
- Rejecting $H_0$ when it is true is a **Type I error**, with a probability equal to $\alpha$.
- The critical value determines the rejection region. If $\bar{x}$ falls on the $H_0$ side, the test fails to reject.
- There is an $\alpha$% chance that $H_0$ is true and $\bar{x}$ is as extreme or more extreme than the critical value, leading to a Type I error.


### Type II Error
- If $H_0$ is false, the test is correct when it **rejects** $H_0$.
- Failing to reject $H_0$ when it is false is a **Type II error**, with a probability equal to $\beta$.

### Hypothesis Test Outcome Matrix
The following matrix summarizes the four possible outcomes of a hypothesis test, highlighting the probabilities of Type I error ($\alpha$), Type II error ($\beta$), and power ($1 - \beta$).  
  
<div style="max-width: 400px; margin: auto; text-align: center;">
$$
\begin{array}{|c|c|c|}
\hline
& H_0\ \textbf{True} & H_0\ \textbf{False} \\ \hline
\textbf{Reject } H_0 & \text{Type I error } (\alpha) & \text{✅ Power } (1 - \beta) \\ \hline
\textbf{FTR } H_0 & \text{✅ Correct} & \text{Type II error } (\beta) \\ \hline
\end{array}
$$

<div style="font-size: 0.9em; line-height: 1.2em; text-align: center;">
<em>*Matrix of hypothesis test outcomes based on the truth of $H_0$ and the decision to reject or not reject.*</em>
</div>
</div>


## Power
- Power is the probability of correctly rejecting $H_0$ when it is false ($1 - \beta$).
- It measures the test's ability to detect true differences in population distributions.
- Higher power means a greater ability to detect a real effect.


## Effect Size
- Effect size is the difference between the null mean and the true mean.
- **Example:** If the null mean is 50 and the true mean is 60, the raw effect size is 10.
- Cohen's D is one measure, $\frac{\text{raw effect size}}{\text{standard deviation}}$. It is the normalized difference between the means.
- Increasing the effect size increases power and decreases $\beta$, but it does not affect the critical value or $\alpha$.
- Power and the probability of a Type II error will change depending on the actual value of $\mu$.

![Sampling Distributions, Power, and Effect Size](images/sketch_4_2.png){width=6in fig-align="left"}   
*This diagram illustrates the relationship between effect size, critical values, power, and Type I and Type II errors. Narrower distributions from larger sample sizes improve the ability to detect true differences.*


## How to Increase Power
- **Increase effect size:** Larger differences are easier to detect.
- **Increase $\alpha$:** A less stringent threshold makes rejection more likely. Increasing $\alpha$ makes the critical value less extreme, making it easier for the test statistic to to exceed it.
- **Increase sample size:** Larger samples reduce variability in $\bar{x}$, making it more likely to be close to $\mu$ and increasing the test's ability to detect true differences.
- **Note:** If power is close to $\beta$, the chances of detect the difference or make a type II error are similar. This can happen when the effect size is too small. Increasing sample size can help.


## Balancing Type I and Type II Errors
- Reducing $\alpha$ decreases the chance of a Type I error but increases the chance of a Type II error.


## Example Code For Finding Power or Other Missing Parameters

R code:
```{r eval=FALSE}
library(tidyverse)

## Missing parameter is indicated with `NULL`

# Calculate power for one-sample t-test with specified parameters
power.t.test(n = 41, delta = .07, 
sd = .2, sig.level = 0.05,
power = NULL, 
type = "one.sample", alternative = "one.sided")

# Example for two-sample t-test
power.t.test(n = 250, delta = 5,
sd = 25, sig.level = 0.05, 
power = NULL, 
type = "two.sample", alternative = "one.sided")

```

SAS code:
```{sas eval=FALSE}
/* Missing parameter is indicated with a period */

/* Example for one-sample t-test */
proc power;
	onesamplemeans
		sides=1
		alpha=.1
		nullmean=20000
		mean=20450
		stddev=500
		ntotal=8
		power=.;
run;

/* Example for two-sample t-test */
proc power; 
*can specify mean for each (nullmean) and ntotal;
	twosamplemeans
		sides=1
		alpha=.05
		meandiff=4
		stddev=3
		npergroup=50
		power=.;
run;

```


## Example Code For Creating a Power Curve

R code:
```{r eval=FALSE}
# Loop to calculate power for sample sizes from 60 to 80
powerholder = c()
samplesizes = seq(60, 80, by = 1)

for(i in 1:length(samplesizes))
{
  powerholder[i] = power.t.test(n = samplesizes[i], delta = 0.07, sd = 0.20, sig.level = .05,
                                type = "one.sample", alternative = "one.sided")$power
}

# Create a data frame
powerdf = data.frame(samplesizes, powerholder)

# Create the power curve
powerdf %>% ggplot(aes(x = samplesizes, y = powerholder)) +
  geom_line(color = "blue3", linewidth = 1.5) +
  ggtitle("Power Curve") +
  ylab("Power") +
  xlab("Sample Sizes") +
  ylim(0.75, 1.0) +
  theme_bw()

```

SAS code:
```{sas eval = FALSE}
/* Power curve for different sample sizes */
ods graphics on;

proc power;
	onesamplemeans
		sides=1
		alpha=.05
		nullmean=0
		ntotal=60 80
		mean=.07
		stddev=.2
		power = .;
		plot x=n min=60 max=80;
run;

ods graphics off;


/* Power curve for different effect sizes */
ods graphics on;

proc power;
	twosamplemeans
		sides=1
		alpha=.05
		nulldiff=0
		meandiff=3 to 5 by 0.1 
		ntotal=47
		stddev=4.5
		power = .;
		plot x=effect min=3 max=5;
run;

ods graphics off;

```


## Summary of Start-to-Finish Analysis for Client Presentation

### Initial Meetings:
- Define key parameters, such as power ($\geq 80\%$).
- Perform a power analysis to determine required sample sizes for different power thresholds.
- Provide sample size options based on budget and desired power.
- Present a power curve to illustrate how power changes with sample size.

### Final Presentation:
- Restate the question of interest and test parameters.
- Visualize the data using histograms or boxplots and summary statistics.
- Present results, including $p$-values, confidence intervals, and scope.
- Provide an appendix with assumptions and a six-step hypothesis test.
- Include contact information.
