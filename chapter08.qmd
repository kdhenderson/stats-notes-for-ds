---
title: "Linear Correlation and Simple Linear Regression"
---

## Objectives
- Understand how to calculate and interpret Pearson’s $r$.
- Recognize the relationship between Pearson’s $r$ and scatterplots.
- Identify how patterns in data affect the magnitude and sign of Pearson’s $r$.
- Discuss when correlation equals causation and when it does not.
- Explore the basics of simple linear regression.

### Resources
- [Interactive Correlation Visualization](https://rpsychologist.com/d3/correlation)
- [Regression Shuffle Applet](https://www.rossmanchance.com/applets/2021/regshuffle/regshuffle.htm)


## Scatterplots
Scatterplots visually represent the relationship between two quantitative variables, illustrating how they vary together. (Variations can show 3+ variables). Key points:
- **Explanatory variable (X)** is plotted on the X-axis, while the **response variable (Y)** is on the Y-axis.
- Each dot represents an observation.
- Patterns to look for include:
  - Positive or negative linear relationships
  - Curved patterns (e.g., parabolic)
  - Logarithmic or sinusoidal relationships
  - Random scatter (no clear relationship)


## Interpreting Correlations
### Square of Correlation Coefficient ($r^2$)
- Represents shared variance. Example: $r = 0.8$ implies $r^2 = 0.64$, meaning 64% of variability in $Y$ is explained by its linear relationship with $X$.

### Significance
- $r = 1$: Perfect positive linear relationship.
- $r = -1$: Perfect negative linear relationship.
- $r = 0$: No linear relationship.

### Key Notes
- The sign of $r$ indicates the direction of the slope (positive or negative):
  - A negative correlation reflects an inverse relationship.
  - Positive and negative correlations of the same magnitude have the same shared variance.
- Changing sample size does not affect $r$.
- Correlation does not imply causation.


## Correlation Analysis
### Key Formula
$$
r = \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{(n-1) S_x S_y} = \frac{\sum_{i=1}^n Z_x Z_y}{n-1}
$$
Where $n$ = number of $(x, y)$ pairs.

### Why Plot the Data First?
- Use scatterplots to visually check for linear relationships, outliers, or curved patterns, e.g. Anscombe dataset. Then find the correlation for the strength of the relationship.
- Outliers can significantly distort $r$, regression slopes, and intercepts, as these measures are not resistant to extreme values.


## Factors Affecting the Magnitude of $r$
- **Bivariate Outliers**: Can inflate or deflate $r$.
- **Distribution Shape**: Differences in variability between $X$ and $Y$ can distort $r$.
- **Range Restrictions**: Restricted ranges of data can misrepresent $r$.
- **Combining Groups**: Mixed groups may lead to misleading correlations. Averages are less variable, so the correlation will be stronger.


## Facts About $r$
- $r$ estimates the population correlation, $\rho$.
- Measures the strength of a linear relationship between two variables.
- Always between $-1$ and $1$.
- Unitless; unaffected by changes in measurement scales.
- Assumes both variables are quantitative.
- Assumes bivariate normal distribution (for any value of $y$, $x$ has a normal distribution, and vice versa).
- Not resistant to outliers.


## Interpreting $r^2$, the Coefficient of Determination

### Definition
- $r^2$ represents the proportion of variance in $Y$ explained by its linear relationship with $X$:
  - $r^2$ represents the proportion of variance in $Y$ that can be explained by changes in $X$ (Dr. McGee).
  - $r^2$ is the estimated (if calculated from a sample) proportion of the variation in $Y$ explained by the linear relationship between $X$ and $Y$ (Dr. Sadler).  
  - For example, an $r^2$ of 0.75 means that 75% of the variability in $Y$ can be attributed to its linear relationship with $X$, while the remaining 25% is unexplained by the model.

### Formula
$$
R^2 = 1 - \frac{SS_{res}}{SS_{tot}}
$$
Where:
- $SS_{res}$ = sum of squared residuals.
- $SS_{tot}$ = total sum of squares.

This measures how much of the total variance from the equal means model is explained by the regression model.


## Correlation Doesn’t Equal Causation
- Association (strong correlation) is not causation.
- The only way to make cause-and-effect statements is with a randomized experiment.


## Keys to Remember
- The correlation coefficient assumes **equal standard deviation**:
  - Don’t confuse fewer data points with smaller standard deviation.
- The range of the data is a proxy for the spread. Knowing the particular $X$ value reduces the variation.


## Data Exploration
- Always plot the data using scatterplots for two quantitative variables.
- Look for overall patterns: positive/negative, strong/weak, curved/linear.
- If the relationship is linear, calculate a numerical summary.
- Plot a regression line for interpretation.


## Least Squares Regression

### Origin of Regression
- The term **regression** originates from Francis Galton’s phrase "Regression toward mediocrity," referring to regression to the mean.
- A regression line is a straight line that describes how a **response variable ($Y$)** changes as an **explanatory variable ($X$)** changes. It is used to predict the value of $Y$ for a given $X$.

### Key Points
1. **One Best-Fit Line**: 
   - There is only one best-fit line per dataset.
   - This line minimizes the distance between observed points and the line (residuals).
   - **Residuals**: The difference between the observed value ($y_i$) and the predicted value ($\hat{y}_i$), calculated as $\text{Residual} = y_i - \hat{y}_i$. Residuals measure the error in prediction.
   - The best-fit line (slope and y-intercept) minimizes the **sum of squared residuals (SSR)**.
   - All the residuals would be zero for a perfect fitting line.

2. **Relation to Correlation**:
   - $\hat{b_1}$ (slope) and $r$ always have the same sign.
   - The least squares regression line always passes through $(\bar{X}, \bar{Y})$.

3. **Choice of Variables**:
   - Regression equation depends on the designation of explanatory and response variables.
   - **With correlation**, you can flip $X$ and $Y$, but not with regression.

4. **Subpopulations**:
   - In simple regression with a single explanatory variable, each value of the explanatory variable corresponds to a subpopulation of responses.
   - Regression describes the relationship between the means of these subpopulations and the explanatory variable.

### Estimating Regression Coefficients
1. **Slope ($\hat{\beta}_1$)**:
   $$
   \hat{\beta}_1 = \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^n (X_i - \bar{X})^2} = r \frac{S_y}{S_x}
   $$
2. **Intercept ($\hat{\beta}_0$)**:
   $$
   \hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{X}
   $$

### Definition
A regression line predicts $Y$ from $X$:
$$
Y = \beta_0 + \beta_1 X
$$

### Interpretation
1. **Slope ($\hat{\beta}_1$)**, coefficient of x:
   - Represents the predicted change in $Y$ for a one-unit increase in $X$.
   - Rise over run, or $\frac{\Delta Y}{\Delta X}$
   - A 1 unit increase in $X$ results in a $\beta_1$ unit increase in the predicted $Y$. It is estimated that a 1 unit increase in $X$ results in a $\beta_1$ unit increase in $Y$.
   - Example: A slope of $6.708$ means a 1-hour increase in study time predicts an average score increase of $6.708$ points.
2. **Intercept ($\hat{\beta}_0$)**, a constant:
   - Represents the predicted $Y$ when $X = 0$.
   - The predicted response that corresponds to $X = 0$. It is estimated that when $X = 0$, $Y$ will be $\beta_0$.
   - Example: An intercept of $40.993$ means a predicted score of $40.993$ when no study time is recorded.

### Extrapolation
- Predictions are only valid for $X$ values within the range used to build the model.
- **Extrapolation**: Predicting values of $Y$ for $X$ outside the observed data range. Use cautiously, as extrapolated predictions may not reliably represent real-world behavior and clearly communicate that the results are extrapolated.

### Assumptions for Linear Regression
1. **Normality**, conditional on $X$:
   - The response variable is normally distributed for each value of $X$.
2. **Linearity**:
   - The relationship between the means of $Y$ for each value of $X$ is linear. The mean of the normal distributions follow a linear pattern.
3. **Equal Standard Deviation**:
   - Variance of residuals is constant for all $X$.
   - Estimate: $\hat{\sigma} = \sqrt{\frac{\sum_{i=1}^n \text{residuals}_i^2}{n - 2}} = \text{RMSE}$.
4. **Independence**:
   - Observations are independent of each other.

### Notation
- $m_{Y|X} = b_0 + b_1X$: Mean of $Y$ as a function of $X$ or for a specific value of $X$.
- $s_{Y|X} = s$: Standard deviation of $Y$ as a function of $X$, assuming equal standard deviation across $X$ values.

::: {#R-hand-calculated layout-ncol=2}
![Pearson's R pg1](images/notes_8_1_pearsonsRbyhand_1.png)  
![Pearson's R pg2](images/notes_8_2_pearsonsRbyhand_2.png)
:::
*Figure 1: Hand-calculated Pearson's $r$.*  

## Pearson’s $r$ and 6-Step Hypothesis Test

### 1. Hypotheses (Testing $\rho$, the Population Correlation Coefficient)
- $H_0$: $\rho = 0$ (No linear correlation exists in the population)
- $H_A$: $\rho \neq 0$ (A linear correlation exists in the population)

### 2. Critical Value (From a $t$-Distribution)
- $\pm t_{0.975, 13-2} = \pm 2.201$

#### Computing the Critical Value:
- In SAS: `quantile("t", 0.975, 13-2)`
- In R: `qt(0.975, 11)`

### 3. Test Statistic
The sample linear correlation coefficient $r$ is transformed into a $t$-distribution using the formula:
$$
t = \frac{r\sqrt{n - 2}}{\sqrt{1 - r^2}} \sim t_{n-2}
$$
#### Substituting Values:
$$
t = \frac{0.8217 \cdot \sqrt{13 - 2}}{\sqrt{1 - (0.8217)^2}} = 4.71818
$$

### Compute $r$ in Statistical Software

#### In SAS:
```{sas eval=FALSE}
proc corr data = Studytime;
run;
To get r:
in SAS:
proc corr data = Studytime;
run;
```

#### In R:
```{r eval=FALSE}
cor.test(StudyTime$StudyHours,StudyTime$ExamScore)
```

::: {#R-from-code layout-ncol=2}
![Pearson's R Code Output SAS](images/notes_8_3_pearsonsR_SAS.png) 
![Pearson's R Code Output R](images/notes_8_4_pearsonsR_R.png)
:::
*Figure 2: Outputs in SAS and R.*  

### 4. Find the $p$-value
- $p-\text{value} = 0.0006$ 

### 5. Decision
- At $\alpha = 0.05$, reject $H_0$ because $p = 0.0006 < 0.05$.

### 6. Conclusion
- **Interpretation**: At the $\alpha = 0.05$ significance level, the test provides strong evidence that exam scores are linearly correlated with study hours (p-value = 0.0006).
- (Since it’s significant, it makes sense to interpret $R^2$).
- It is estimated that $R^2 = 67.52\%$ of the variation in the exam score is explained by its relationship with study hours
- **Scope**:Because the students weren’t randomly assigned study hours, we cannot establish causation, only association. Additionally, because we do not know how the data were selected, we cannot generalize the result beyond the subjects in this study.


![Least Squares Regression Line 1](images/notes_8_5_handCalculatedParameterEstimates.png)  

```{sas eval=False}

proc reg data = StudyTime;
* response = explanatory;
model ExamScore = StudyTime;
run;
* for confidence intervals;
* model ExamScore = StudyTime / clb;
*OR;
proc glm data = StudyTime;
* solution -> regression: parameter estimate table;
model ExamScore = StudyTime / solution;
run;

```

![Least Squares Regression Line 2](images/notes_8_6_parameterEstimateTable_SAS.png) 

```{r}

#lm - linear model(response ~ explanatory, data)
fit = lm(ExamScore~StudyHours, data= StudyTime)
summary(fit)

```

![Least Squares Regression Line 2](images/notes_8_7_parameterEstimateTable_R.png) 

*Figure 4: Least Squares Regression Line*  

Hypothesis testing for the intercept and slope
![](images/display_8_1.png)
![](images/display_8_2.png)
![](images/display_8_3.png)
![](images/display_8_4.png)
Two T-Tests!!!
$H_0: \beta_0 = 0$
$H_a: \beta_0 \neq 0$

$H_0: \beta_1 = 0$
$H_a: \beta_1 \neq 0$

![Study Hours Example](images/notes_8_8_studyHourExample.png)

### Putting It All Together
1. Plot the data to visualize the relationship.
2. Test the linear correlation coefficient.
3. Reject $H_0$ if $\rho = 0$ is not supported (significant correlation). The slope ($\beta_1$) is also significantly different from zero.
4. Calculate and interpret regression coefficients.
5. Use the regression model for predictions within the valid range.

