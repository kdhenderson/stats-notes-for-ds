---
title: "Linear Correlation and Simple Linear Regression"
---

## Objectives
- Understand how to calculate and interpret Pearsonâ€™s $r$.
- Recognize the relationship between Pearsonâ€™s $r$ and scatterplots.
- Identify how patterns in data affect the magnitude and sign of Pearsonâ€™s $r$.
- Discuss when correlation equals causation and when it does not.
- Explore the basics of simple linear regression.

### Resources
- [Interactive Correlation Visualization](https://rpsychologist.com/d3/correlation)
- [Regression Shuffle Applet](https://www.rossmanchance.com/applets/2021/regshuffle/regshuffle.htm)


## Scatterplots
Scatterplots represent the relationship between two quantitative variables, i.e. how they move together (variations can show 3+ variables). Key points:
- **Explanatory variable (X)** is plotted on the X-axis, while the **response variable (Y)** is on the Y-axis.
- Each dot represents an observation.
- Patterns to look for include:
  - Positive or negative linear relationships
  - Curved patterns (e.g., parabolic)
  - Logarithmic or sinusoidal relationships
  - Random scatter (no clear relationship)


## Interpreting Correlations
### Square of Correlation Coefficient ($r^2$)
- Represents shared variance. Example: $r = 0.8$ implies $r^2 = 0.64$, meaning 64% of variability in $Y$ is explained by its linear relationship with $X$.

### Significance
- $r = 1$: Perfect positive linear relationship.
- $r = -1$: Perfect negative linear relationship.
- $r = 0$: No linear relationship.

### Key Notes
- The sign of $r$ indicates the direction of the slope (positive or negative):
  - A negative correlation reflects an inverse relationship.
  - Positive and negative correlations of the same magnitude have the same shared variance.
- Changing sample size does not affect $r$.
- Correlation does not imply causation.


## Correlation Analysis
### Key Formula
$$
r = \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{(n-1) S_x S_y} = \frac{\sum_{i=1}^n Z_x Z_y}{n-1}
$$
Where $n$ = number of $(x, y)$ pairs.

### Why Plot the Data First?
- Use scatterplots to visually check for linear relationships, outliers, or curved patterns, e.g. Anscombe dataset. Then find the correlation for the strength of the relationship.
- Outliers can heavily influence $r$, regression slopes, and intercepts, as they are not resistant statistics.


## Factors Affecting the Magnitude of $r$
- **Bivariate Outliers**: Can inflate or deflate $r$.
- **Distribution Shape**: Differences in variability between $X$ and $Y$ can distort $r$.
- **Range Restrictions**: Restricted ranges of data can misrepresent $r$.
- **Combining Groups**: Mixed groups may lead to misleading correlations. Averages are less variable, so the correlation will be stronger.


## Facts About $r$
- $r$ estimates the population correlation, $\rho$.
- Measures the strength of a linear relationship between two variables.
- Always between $-1$ and $1$.
- Unitless; unaffected by changes in measurement scales.
- Assumes both variables are quantitative.
- Assumes bivariate normal distribution (for any value of $y$, $x$ has a normal distribution, and vice versa).
- Not resistant to outliers.


## Interpreting $r^2$, the Coefficient of Determination

### Definition
- $r^2$ represents the proportion of variance in $Y$ explained by its linear relationship with $X$:
  - $r^2$ represents the proportion of variance in $Y$ that can be explained by changes in $X$ (Dr. McGee).
  - $r^2$ is the estimated (if calculated from a sample) proportion of the variation in $Y$ explained by the linear relationship between $X$ and $Y$ (Dr. Sadler).

### Formula
$$
R^2 = 1 - \frac{SS_{res}}{SS_{tot}}
$$
Where:
- $SS_{res}$ = sum of squared residuals.
- $SS_{tot}$ = total sum of squares.

This measures how much of the total variance from the equal means model is explained by the regression model.


## Correlation Doesnâ€™t Equal Causation
- Association (strong correlation) is not causation.
- The only way to make cause-and-effect statements is with a randomized experiment.


## Keys to Remember
- The correlation coefficient assumes **equal standard deviation**:
  - Donâ€™t confuse fewer data points with smaller standard deviation.
- The range of the data is a proxy for the spread. Knowing the particular $X$ value reduces the variation.


## Data Exploration
- Always plot the data using scatterplots for two quantitative variables.
- Look for overall patterns: positive/negative, strong/weak, curved/linear.
- If the relationship is linear, calculate a numerical summary.
- Plot a regression line for interpretation.


## Least Squares Regression

### Origin of Regression
- The term **regression** originates from Francis Galtonâ€™s phrase "Regression toward mediocrity," referring to regression to the mean.
- A regression line is a straight line that describes how a **response variable ($Y$)** changes as an **explanatory variable ($X$)** changes. It is used to predict the value of $Y$ for a given $X$.

### Key Points
1. **One Best-Fit Line**: 
   - There is only one best-fit line per dataset.
   - This line minimizes the distance between observed points and the line (residuals).
   - Residuals: $\text{Residual} = \text{Observed} - \text{Predicted}$.
   - The best-fit line (slope and y-intercept) minimizes the **sum of squared residuals (SSR)**.
   - All the residuals would be zero for a perfect fitting line.

2. **Relation to Correlation**:
   - $\hat{b_1}$ (slope) and $r$ always have the same sign.
   - The least squares regression line always passes through $(\bar{X}, \bar{Y})$.

3. **Choice of Variables**:
   - Regression equation depends on the designation of explanatory and response variables.
   - **With correlation**, you can flip $X$ and $Y$, but not with regression.

4. **Subpopulations**:
   - In simple regression with a single explanatory variable, each value of the explanatory variable corresponds to a subpopulation of responses.
   - Regression describes the relationship between the means of these subpopulations and the explanatory variable.

### Estimating Regression Coefficients
1. **Slope ($\hat{\beta}_1$)**:
   $$
   \hat{\beta}_1 = \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^n (X_i - \bar{X})^2} = r \frac{S_y}{S_x}
   $$
2. **Intercept ($\hat{\beta}_0$)**:
   $$
   \hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{X}
   $$

### Definition
A regression line predicts $Y$ from $X$:
$$
Y = \beta_0 + \beta_1 X
$$

### Interpretation
1. **Slope ($\hat{\beta}_1$)**, coefficient of x:
   - Represents the predicted change in $Y$ for a one-unit increase in $X$.
   - Example: A slope of $6.708$ means a 1-hour increase in study time predicts an average score increase of $6.708$ points.
2. **Intercept ($\hat{\beta}_0$)**, a constant:
   - Represents the predicted $Y$ when $X = 0$.
   - Example: An intercept of $40.993$ means a predicted score of $40.993$ when no study time is recorded.


Main uses of regression
Prediction 
Interpretation:
b1  (coefficient of x) = slope = rise / run = Dy / Dx
b0 (constant) = y-intercept
Slope interpretation: a 1 unit increase in x results in a b1 unit increase in the predicted y, or it is estimated that a 1 unit increase in x results in a b1 unit increase in y.
y-intercept interpretation: the predicted response that corresponds to x = 0, or it is estimated that when x = 0, y will be b0.

Extrapolation
Predictions are only valid for values of x inside the domain of x that were used to build the model.
Use extrapolation with care and communicate that the data are the result of extrapolation.


Assumptions for linear regression
Normality â€“ conditional on x
Linearity â€“ mean of the normal distributions follow a linear pattern (have a linear relationship with x)
Equal standard deviation - $\hat{\sigma} = \sqrt{\frac{\sum_{i=1}^n residuals_i^2}{n - 2}} = \text{RMSE}$
Independence

Some notation for the regression of Y on X
m{Y|X} = b0 + b1X -> the mean of Y as a function of X or for a specific value of x, the mean of Y when X = Xi
s{Y|X} = s, for all X (equal standard deviation assumption -> the standard deviation of Y as a function of X

::: {#R-hand-calculated layout-ncol=2}
![Pearson's R pg1](images/notes_8_1_pearsonsRbyhand_1.png)  
![Pearson's R pg2](images/notes_8_2_pearsonsRbyhand_2.png)
:::
*Figure 1: Pearson's R (hand-calculated).*  

Pearsonâ€™s R and 6-step hypothesis test 
Hypotheses (testing rho, the population correlation coefficient):
H0: ðœŒ = 0
HA: ðœŒ â‰  0
Critical value (from a t-distribution):
Â±t0.975, 13-2 = Â±2.201
As before:
SAS â€“ quantile("t", 0.975, 13-2); R â€“ qt(0.975, 11)
Test statistic - transform the sample linear correlation coefficient to something with a t-distribution by this:
(ð‘Ÿâˆš(ð‘› âˆ’2))/âˆš(1 âˆ’ð‘Ÿ^2 )~ð‘¡_(ð‘›âˆ’2)	t =(.8217âˆš(13 âˆ’2))/âˆš(1 âˆ’ã€–.8217ã€—^2 ) = 4.71818
To get r:
in SAS:
proc corr data = Studytime;
run;
in R:
cor.test(StudyTime$StudyHours,StudyTime$ExamScore)
p-value = 0.0006

::: {#R-from-code layout-ncol=2}
![Pearson's R Code Output SAS](images/notes_8_3_pearsonsR_SAS.png) 
![Pearson's R Code Output R](images/notes_8_4_pearsonsR_R.png)
:::
*Figure 2: Pearson's R output in SAS and R.*  

Decision: Reject H0 at a = 0.05
Conclusion: There is strong evidence at the a = 0.05 level of significance to suggest that exam scores are linearly related to study hours (p-value = 0.0006 from a test of linear correlation?).
(Since itâ€™s significant, it makes sense to interpret R2). It is estimated that R2 = 67.52% of the variation in the exam score is explained by its relationship with study hours
Because the students werenâ€™t randomly assigned study hours, we cannot establish causation, only association. Additionally, because we do not know how the data were selected, we cannot generalize the result beyond the subjects in this study.


![Least Squares Regression Line 1](images/notes_8_5_handCalculatedParameterEstimates.png)  

```{sas eval=False}

proc reg data = StudyTime;
* response = explanatory;
model ExamScore = StudyTime;
run;
* for confidence intervals;
* model ExamScore = StudyTime / clb;
*OR;
proc glm data = StudyTime;
* solution -> regression: parameter estimate table;
model ExamScore = StudyTime / solution;
run;

```

![Least Squares Regression Line 2](images/notes_8_6_parameterEstimateTable_SAS.png) 

```{r}

#lm - linear model(response ~ explanatory, data)
fit = lm(ExamScore~StudyHours, data= StudyTime)
summary(fit)

```

![Least Squares Regression Line 2](images/notes_8_7_parameterEstimateTable_R.png) 

*Figure 4: Least Squares Regression Line*  

Hypothesis testing for the intercept and slope
![](images/display_8_1.png)
![](images/display_8_2.png)
![](images/display_8_3.png)
![](images/display_8_4.png)
Two T-Tests!!!
$H_0: \beta_0 = 0$
$H_a: \beta_0 \neq 0$

$H_0: \beta_1 = 0$
$H_a: \beta_1 \neq 0$

![Study Hours Example](images/notes_8_8_studyHourExample.png)

### Putting It All Together
1. Plot the data to visualize the relationship.
2. Test the linear correlation coefficient.
3. Reject $H_0$ if $\rho = 0$ is not supported (significant correlation). The slope ($\beta_1$) is also significantly different from zero.
4. Calculate and interpret regression coefficients.
5. Use the regression model for predictions within the valid range.

