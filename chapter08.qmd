---
title: "Linear Correlation and Simple Linear Regression"
---

## Objectives
The concept of effect size in two-sample and multi-sample studies. (I don’t think we talked about this…)
How to calculate and interpret Pearson’s r.
The relationship between Pearson’s r and scatterplots.
How patterns in the data affect the magnitude and sign of Pearson’s r.
When does correlation equal causation and when does it not.
Resources
rpsychologist.com/d3/correlation
https://www.rossmanchance.com/applets/2021/regshuffle/regshuffle.htm
Scatterplots
Represent relationship between 2 quantitative variables, i.e. how they move together (variations can show 3+).
Explanatory variable is plotted on X axis and response variable is plotted on the Y axis.
Each dot is an observation.
Look for patterns of association, e.g. positive/negative linear, curved up/down, logarithmic, sinusoidal, or random cloud.

Interpreting Correlations
Square of correlation coefficient equals shared variance.
If correlation is 0.8, then the shared variance is 64%, which means 64% of the variability in response is attributed to / explained by the linear relationship of the explanatory variable with the response variable. 
Correlation is for linear relationships.
A correlation of 1 means that one variable perfectly predicts the other. Correlations close to zero indicate very little shared variance.
The sign of the correlation indicates the direction of the slope, so a negative correlation reflects an inverse relationship. Both positive and negative correlations of the same magnitude have the same shared variance.
Changing the sample size, doesn’t change the correlation.
Correlation Analysis
Always look at a scatterplot of the data first for a visual of the relationship.
Find correlation for the strength of the relationship.
$r = \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{(n-1) S_x S_y} = \frac{\sum_{i=1}^n Z_x Z_y}{(n-1) S_x S_y}$
where	n = number of (x,y) pairs

Importance of plotting the data on a scatterplot (Anscombe dataset)
Is linear regression the correct type of fit? Do you need to account for outliers or curved relationships?
Outliers can influence the relationship between x and y. R2, regression slope and intercept are very susceptible to outliers (not resistant statistics).
If the points are scattered around the x-axis on the residual plot, the linear model is a good fit.

Factors that affect the magnitude of Pearson’s R
Pattern of data points - Only meant to detect differences from a straight line.
Bivariate outliers can make Pearson’s R larger or smaller than it should be.
Different distribution shapes for x and y variables, one has larger variability than the other. Pearson’s R isn’t meant to deal with different variability.
If a restricted range of the data is presented, it can be misleading. Pearson’s R is meant to look at the full range of the data.
Combining groups with different behavior can be misleading.
Averages are less variable, so the correlation will be stronger.

Facts about the correlation coefficient, Pearson’s r
r measures strength of a linear relationship between two variables.
Requires that both variables be quantitative
Always between −1 and +1
If r near 0 → weak relationship
If r near +1 or −1 → strong relationship
r is unitless. It doesn’t change with the unit of measurement.
Not resistant to outliers (assumes bivariate normal distribution: for any value of y, x has normal distribution and vice versa)
r estimates the population correlation, p.
Interpreting the Coefficient of Determination, r2
This is the percentage of variance in y than can be explained by the changes in x (Dr. McGee).
Key: The value of r2 is the estimated (if calculated from a sample) proportion of the variation in y that is explained by the linear relationship between x and y (Dr. Sadler).
R2 = 1 – SSres / SStot = how much of the total amount of variance that could be explained from the equal means model has been reduced / explained by the model.

Correlation doesn’t equal causation.
Association (strong correlation) is not causation.
The (only) way to make cause and effect statements is with a randomized experiment.

Things to remember
One of the assumptions of the correlation coefficient is equal standard deviation. Don’t confuse fewer data points with smaller standard deviation.
The range of the data is a proxy for the spread. Knowing the particular x value reduces the variation.

Data Exploration
Always plot the data
With scatterplots for 2 quantitative variables
Look for overall pattern: positive/negative, strong/weak, curved/linear
Calculate a numerical summary – if the relationship is linear
Plot a regression line

Least Squares Regression
Origin of the term regression came from Francis Galton – ‘Regression toward mediocrity’ -> regression to the mean
A regression line is a straight line that describes how a response variable changes as an explanatory variable changes and is used to predict the value of y for a given value of x.
With correlation you can flip x and y, but the regression equation depends on the choice of explanatory and response variables. 
Only one best-fit line per dataset.
Minimize the distance of the observed points to the line, the residuals. i.e. minimize the sum of the squared distances of each point to the line.
Residual = observed - predicted
All the residuals would be zero for a perfect fitting line.
Best fitting line (slope and y-intercept) minimizes the sum of squared residuals (SSR).
("b1" ) ̂ and r always have the same sign.
Least squares regression line always passes through xbar, ybar.
In simple regression, there is a single explanatory variable. 
For each value of the explanatory variable there is a subpopulation of responses.
The regression of the response variable on the explanatory variable is a mathematical relationship between the means of these subpopulations and the explanatory variable.

Main uses of regression
Prediction 
Interpretation:
y = mx + b -> y = b0 + b1 x 
b1  (coefficient of x) = slope = rise / run = Dy / Dx
b0 (constant) = y-intercept
Slope interpretation: a 1 unit increase in x results in a b1 unit increase in the predicted y, or it is estimated that a 1 unit increase in x results in a b1 unit increase in y.
y-intercept interpretation: the predicted response that corresponds to x = 0, or it is estimated that when x = 0, y will be b0.

Extrapolation
Predictions are only valid for values of x inside the domain of x that were used to build the model.
Use extrapolation with care and communicate that the data are the result of extrapolation.

Estimating Regression Coefficients:
- $\hat{\beta}_1 = \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^n (X_i - \bar{X})^2} = r \frac{S_y}{S_x}$
- $\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{X}$
")"

Assumptions for linear regression
Normality – conditional on x
Linearity – mean of the normal distributions follow a linear pattern (have a linear relationship with x)
Equal standard deviation - $\hat{\sigma} = \sqrt{\frac{\sum_{i=1}^n residuals_i^2}{n - 2}} = \text{RMSE}$
Independence

Some notation for the regression of Y on X
m{Y|X} = b0 + b1X -> the mean of Y as a function of X or for a specific value of x, the mean of Y when X = Xi
s{Y|X} = s, for all X (equal standard deviation assumption -> the standard deviation of Y as a function of X

::: {#R-hand-calculated layout-ncol=2}
![Pearson's R pg1](images/notes_8_1_pearsonsRbyhand_1.png)  
![Pearson's R pg2](images/notes_8_2_pearsonsRbyhand_2.png)
:::
*Figure 1: Pearson's R (hand-calculated).*  

Pearson’s R and 6-step hypothesis test 
Hypotheses (testing rho, the population correlation coefficient):
H0: 𝜌 = 0
HA: 𝜌 ≠ 0
Critical value (from a t-distribution):
±t0.975, 13-2 = ±2.201
As before:
SAS – quantile("t", 0.975, 13-2); R – qt(0.975, 11)
Test statistic - transform the sample linear correlation coefficient to something with a t-distribution by this:
(𝑟√(𝑛 −2))/√(1 −𝑟^2 )~𝑡_(𝑛−2)	t =(.8217√(13 −2))/√(1 −〖.8217〗^2 ) = 4.71818
To get r:
in SAS:
proc corr data = Studytime;
run;
in R:
cor.test(StudyTime$StudyHours,StudyTime$ExamScore)
p-value = 0.0006

::: {#R-from-code layout-ncol=2}
![Pearson's R Code Output SAS](images/notes_8_3_pearsonsR_SAS.png) 
![Pearson's R Code Output R](images/notes_8_4_pearsonsR_R.png)
:::
*Figure 2: Pearson's R output in SAS and R.*  

Decision: Reject H0 at a = 0.05
Conclusion: There is strong evidence at the a = 0.05 level of significance to suggest that exam scores are linearly related to study hours (p-value = 0.0006 from a test of linear correlation?).
(Since it’s significant, it makes sense to interpret R2). It is estimated that R2 = 67.52% of the variation in the exam score is explained by its relationship with study hours
Because the students weren’t randomly assigned study hours, we cannot establish causation, only association. Additionally, because we do not know how the data were selected, we cannot generalize the result beyond the subjects in this study.


![Least Squares Regression Line 1](images/notes_8_5_handCalculatedParameterEstimates.png)  

```{sas eval=False}

proc reg data = StudyTime;
* response = explanatory;
model ExamScore = StudyTime;
run;
* for confidence intervals;
* model ExamScore = StudyTime / clb;
*OR;
proc glm data = StudyTime;
* solution -> regression: parameter estimate table;
model ExamScore = StudyTime / solution;
run;

```

![Least Squares Regression Line 2](images/notes_8_6_parameterEstimateTable_SAS.png) 

```{r}

#lm - linear model(response ~ explanatory, data)
fit = lm(ExamScore~StudyHours, data= StudyTime)
summary(fit)

```

![Least Squares Regression Line 2](images/notes_8_6_parameterEstimateTable_R.png) 

*Figure 4: Least Squares Regression Line*  

Hypothesis testing for the intercept and slope
![](images/display_8_1.png)
![](images/display_8_2.png)
![](images/display_8_3.png)
![](images/display_8_4.png)
Two T-Tests!!!
$H_0: \beta_0 = 0$
$H_a: \beta_0 \neq 0$

$H_0: \beta_1 = 0$
$H_a: \beta_1 \neq 0$

![Study Hours Example](notes_8_8_studyHourExample.png)

Putting it all together
Read in the data.
Plot the data to check for visual evidence of a linear relationship.
Test the linear correlation coefficient.
If we reject that rho equals zero, there is significant linear correlation, then the slope is also significantly different than zero.
Then… we can predict and interpret.

