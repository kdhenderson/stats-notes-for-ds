{
  "hash": "85114b912b86cd84877bfe0fcd09daef",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"2 x 2 Contingency Tables\"\n---\n\n\n\n\n## Objectives\n\nThis chapter introduces contingency tables as a bridge between group comparisons and logistic regression models.\n\n- Understand the role of 2×2 tables in modeling binary outcomes.  \n- Compare group proportions using appropriate statistical metrics.  \n- Learn when and how to apply Fisher’s exact test.  \n- Use chi-squared tests for large-sample inference.\n\n## Classification Tools\n\n- **Linear and Quadratic Discriminant Analysis (LDA/QDA)**  \n- ***k*-Nearest Neighbors (KNN)**  \n- **Logistic Regression**\n  - A parametric method with strong interpretability\n  - Analogous to multiple linear regression, but for binary categorical outcomes\n\n\n## Building Up to Multiple Linear Regression (MLR)\n\n| Explanatory Variable           | Method                        |\n|--------------------------------|-------------------------------|\n| One categorical variable       | *t*-tests (2 groups)            |\n|                                | ANOVA (3+ groups)             |\n| One numeric                    | Simple linear regression      |\n| Mix of categorical and numeric | Multiple linear regression    |\n\n\n## Building Up to Logistic Regression\n\n| Explanatory Variable           | Method                             |\n|--------------------------------|------------------------------------|\n| One categorical variable       | 2x2 contingency tables (2 groups)  |\n|                                | Logistic regression (3+ groups)    |\n| One numeric                    | Simple logistic regression         |\n| Mix of categorical and numeric | Multiple logistic regression       |\n\n\n## Understanding 2x2 Contingency Tables\n\nThe main purpose is to compare the probability of a response outcome between two groups.  \n\n- Are smokers more likely to get cancer than non-smokers?\n  - Explanatory variable: smoker/non-smoker  \n  - Response variable: cancer/no cancer  \n\nThere are three typical data collection designs:\n\n1. **Prospective**  \n2. **Retrospective**  \n3. **Completely observational**  \n\n### Prospective Studies\n- Populations for each level of the explanatory variable are determined in advance.\n  - This may happen naturally (e.g., obese vs. not obese) or by random assignment (e.g., placebo vs. treatment).\n- Simple random samples are collected from each group.\n- **Row totals are fixed**---the sample size for each group is determined before data collection.\n- The explanatory variable is fixed, and the response is observed after a follow-up period.\n- Example: Vitamin C and Colds study\n- Randomized experiments are a special type of prospective study:\n  - Subjects are randomly assigned to predictor groups.\n  - Helps mitigate confounding.\n  - Allows for causal conclusions.\n  - Equal row totals often suggest a prospective study and randomized design.  \n  \n### Retrospective Studies\n- Reverse of prospective: the **response variable is fixed** in advance.\n- Samples are selected based on response status; **column totals are fixed**.\n- The explanatory variable is determined *after* sample selection.\n- Example: Cancer and smoking status study  \n- This approach is often used when:\n  - Ethical concerns prevent random assignment (e.g., assigning people to smoke).\n  - Long follow-up periods are impractical.\n\n### Observational Studies\n- Only the grand total may be fixed---or no totals are fixed at all.\n- The researcher has little or no control over group membership.\n- There is greater potential for confounding.\n- There may be no clearly defined response or predictor, making the study more about **association** than group comparison.\n\n\n## Parameters in 2x2 Tables\n\nThe goal is to compare two groups using **either a difference in means** or **a difference in proportions**.\n\n### Difference in Means (*t*-tests)\n- Parameters:  \n  - $\\mu_1$: mean of the response for group 1  \n  - $\\mu_2$: mean of the response for group 2  \n- Hypotheses:  \n  - $H_0: \\mu_1 = \\mu_2$\n  - $H_a: \\mu_1 \\neq \\mu_2$\n- 95% Confidence Interval:  \n  - $\\mu_1 - \\mu_2$  \n  - If 0 is not in the interval, the result supports $H_a$. \n\n\n### Difference in Proportions\n- Parameters:  \n  - $\\pi_1$: probability of event in group 1  \n  - $\\pi_2$: probability of event in group 2  \n- Hypotheses:  \n  - $H_0: \\pi_1 = \\pi_2$  \n  - $H_a: \\pi_1 \\neq \\pi_2$\n- 95% Confidence Interval:  \n  - $\\pi_1 - \\pi_2$  \n  - If 0 is not in the interval, the result supports $H_a$. \n\n  \n## Problems with Absolute Proportion Differences\n\nEven small absolute differences can be practically important:\n\n$$\\pi_1 - \\pi_2 = 0.05 - 0.01 = 0.04$$\n- The confidence interval may suggest a small estimated difference.\n- This is an absolute difference.\n- But it ignores relative scale:\n  - A 5% event rate is 5 times higher than a 1% event rate!   \n\nThis motivates the use of **relative metrics** like the **odds ratio** and **relative risk**, especially when working with rare events.\n\n    \n## Two Additional Metrics for Group Comparison\n\n1. **Odds Ratio**\n   - Great for retrospective studies\n   - Captures relative difference\n   - Can be harder to understand intuitively and interpret\n2. **Relative Risk**\n   - Also a relative difference metric   \n   - More intuitive to interpret\n   - Often confused with odds ratios\n\n### Odds Example\n- Suppose the odds of getting cancer are 1 to 2: $\\omega = \\frac{1}{2} = 0.5$\n  - Odds are not a probability because the denominator represents the number of non-events, not the total. \n- The corresponding probability of cancer is: $\\frac{1}{3} \\approx 0.33$\n- Another example using the **complement rule**: \n  - Probability of getting cancer: $3/8 = 0.375$  \n  - Probability of not getting cancer: $5/8 = 1 - 3/8 = 0.625$  \n\n### Odds/Probability Relationship\n\nLet:\n\n- $\\omega_c$ = odds of cancer\n- $\\pi_c$ = probability of cancer\n- $1 - \\pi_c$: probability of not getting cancer\n\nThen:\n\n$$\\omega_c = \\frac{\\pi_c}{1 - \\pi_c}$$\nThis formula shows how to convert between a probability and its corresponding odds.\n\n#### Odds Interpretation\n- $0 < \\omega < 1$: odds are against the event (i.e., not in the event’s favor)\n- $\\omega = 1$: 50/50 chance \n- $\\omega > 1$: odds favor the event \n- Odds are always positive (they cannot be negative)\n\n\n### Summary Table: Parameters vs. Statistics\n\n| Metric                   | Parameter (population) | Statistic (sample) |\n|--------------------------|------------------------|--------------------|\n| Proportion / Probability | $\\pi$                  | $\\hat{\\pi}$        |\n| Odds                     | $\\omega$               | $\\hat{\\omega}$     |\n    \n\n## Equivalent Hypotheses\n\nIf two populations have the same probability of an event, they also have the same odds.\n\nThe following hypotheses are equivalent formulations of the null:\n\n- $H_0$: $\\pi_1 = \\pi_2$  \n- $H_0$: $\\pi_1 - \\pi_2 = 0$  \n- $H_0$: $\\omega_1 = \\omega_2$  \n- $H_0$: $\\omega_1 - \\omega_2 = 0$\n\n### Relative Hypotheses\n\nThe following are equivalent relative null hypotheses---these express the idea that the two groups have equal risk or odds:\n\n- $H_0$: $\\pi_1 = \\pi_2$  \n- $H_0$: $\\frac{\\pi_1}{\\pi_2} = 1$  \n- $H_0$: $\\omega_1 = \\omega_2$  \n- $H_0$: $\\frac{\\omega_1}{\\omega_2} = 1$\n\nIn this context:\n\n- $\\dfrac{\\omega_1}{\\omega_2}$ is the **odds ratio**.\n- The odds ratio is always greater than 0.\n  - If OR > 1, the odds are higher in group 1.  \n  - If OR < 1, the odds are lower in group 1.\n\n> Note: Odds are not the same as probabilities!  \n> Avoid words like \"chance\" or phrases like \"x times more likely\" when interpreting an odds ratio.  \n> Instead, say: *“The odds are x times higher.”*\n\n\n## Relative Risk\n\n- Null hypothesis: $H_0: \\dfrac{\\pi_1}{\\pi_2} = 1$\n- Relative risk is often easier to interpret than the odds ratio.\n- Since it is a ratio of two probabilities (i.e., compares two probabilities directly), you *can* use words like “chance” or “more likely” in interpretation.\n\nTips for interpretation:\n\n- Place the larger proportion in the numerator.  \n  - This ensures the relative risk is greater than 1, making interpretation more intuitive.\n\nWhen using software:\n\n- Check how the “event” is defined — many functions choose this automatically.\n- Always verify the output with a quick mental or hand calculation.\n- Most software allows you to change the reference group or event to match your intended interpretation.\n\n\n## Why So Many Metrics?\n\nAll three metrics aim to detect a difference in proportions between groups: \n\n1. Difference in proportions (absolute)  \n2. Odds ratio (relative)  \n3. Relative risk (relative)  \n\n> Use difference in proportions when events are common and you need a simple story.  \n> Use relative risk for rare events.  \n> Use odds ratio when working with retrospective studies.\n\n\n## Study Design Matters\n\n**Retrospective studies**:\n\n- Only the **odds ratio** is valid.\n- Proportion and relative risk estimates are biased and are not valid metrics.\n\n\n## Statistical Inference for 2×2 Tables\n\nThere are several approaches to hypothesis testing and confidence interval construction, depending on:\n\n- Sample size  \n- Study design  \n- How well the test and interval method align\n\n### Analysis Workflow for Inference (Hypothesis Testing)\n\n1. **Identify the study design**:  \n   - Prospective  \n   - Retrospective*  \n   - Observational  \n\n2. **Choose a metric**:  \n   - Difference in proportions → best when the event is not rare; easy to interpret \n   - Relative risk → best for rare events and clear interpretation  \n   - Odds ratio → always valid; required for retrospective studies*\n\n3. **State your hypotheses**:  \n   - $H_0$: $\\pi_1 = \\pi_2$ (difference in proportions)  \n   - $H_0$: $\\frac{\\pi_1}{\\pi_2} = 1$ (relative risk)  \n   - $H_0$: $\\frac{\\omega_1}{\\omega_2} = 1$ (odds ratio)  \n   - For purely observational studies, state without specifying parameters: $H_0$: no association\n\n4. **Run the test and construct a confidence interval**:\n\n   - For large sample sizes:\n     - Use a chi-squared test for the hypothesis test\n     - Use a Wald interval for the confidence interval  \n     - Consider adding a continuity correction to improve the approximation\n   - For small or moderate sample sizes:\n     - Use Fisher’s exact test for the hypothesis test\n     - Use a Fisher’s, bootstrap, or small-sample corrected Wald interval (e.g., Agresti–Coull) for the confidence interval\n   \n5. **Interpret the results**:\n   - Report the test used and the associated *p*-value  \n   - Report the confidence interval and explain what it means  \n   - Always state the method used for inference\n\n> \\* The odds ratio is the only unbiased metric for retrospective studies.\n  \n### Workflow Example\n\n**Polio Vaccine Study (1954)**\n\n- Randomized: vaccine vs. placebo  \n- 400,000 children  \n- Primary concern: paralysis as a side effect  \n\n| Group         | Paralysis | No Paralysis |\n|---------------|-----------|--------------|\n| Placebo       | 142       | 199,858      |\n| Salk Vaccine  | 56        | 199,944      |\n\n\n#### Analysis Decisions\n\n1. **Design**: Prospective (randomized)  \n2. **Metric**: Difference in proportions for easy interpretation (all metrics are valid here)  \n3. **Hypotheses**: $H_0$: $\\pi_{\\text{Placebo}} = \\pi_{\\text{Vaccine}}$  \n4. **Sample size**: Large $n$ → use chi-squared test and Wald interval  \n5. **Conclusion**:  \n   > Using a chi-squared test, there is significant evidence that the chances of a child suffering paralysis differ between placebo and vaccine groups (*p*-value = ...).  \n   > Using a Wald interval, we are 95% confident that the chance of paralysis is 0.029% to 0.0567% higher in the placebo group.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Wald test for difference in proportions (no continuity correction)\nprop.test(c(#eventsRow1,#eventsRow2),c(row1Total, row2Total), correct = FALSE) # events, total sample size, without continuity correction\nprop.test(c(142, 56), c(200000, 200000), correct = FALSE)\n\n# # Relative risk using epitab()\nepitab(polio, method = \"riskratio\", riskratio = \"wald\", rev = \"b\", pvalue = \"chi2\", verbose = TRUE)\n```\n:::\n\n\n\n\n#### Alternative Framing (Relative Risk)\n\n- Because events are rare, relative risk may better communicate the result (very small changes in probability between the two groups)..\n  - The *p*-value remains the same.\n  - The interpretation of the confidence interval changes.\n  - The testing conclusion is the same.\n\n    > We are 95% confident that children in the placebo group are 1.86 to 3.45 times more likely to experience paralysis than children in the vaccine group.\n\n- Be mindful that the choice of metric (difference in proportions vs. relative risk) can affect how results are interpreted in a practical context.\n\n#### Assumptions  \n- Counts in 2x2 tables should be independent.  \n- No repeated measures or time-based observations \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}