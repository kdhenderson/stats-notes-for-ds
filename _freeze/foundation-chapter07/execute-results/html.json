{
  "hash": "2621713b4c10a2467b8a5af7ee15cebe",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear Combinations and the Multiple Comparison Problem\"\n---\n\n\n\n\n## Objectives\n\n- Recognize the limitations of the alternative hypothesis in an ANOVA *F*-test.  \n- Construct and interpret linear combinations of group means.  \n- Understand the need for multiple comparison procedures and when to apply them.  \n- Apply methods such as Bonferroni and Dunnett to test planned and unplanned comparisons.  \n\n\n## Fallacies in Hypothesis Testing\n\n1. **False causality**: A small *p*-value does not indicate causation. Causality can only be inferred from randomized experiments, not observational studies.\n2. **Accepting the null**: We should say there is “no evidence of a difference,” not that the null hypothesis is true. There is not enough evidence to suggest the observed difference is due to anything other than chance.\n3. **Statistical vs. practical significance**: Statistical significance does not imply practical importance. Evaluate practical significance using power, effect size, and confidence intervals.  \n   - Differences that are statistically but not practically significant may arise due to large sample sizes.  \n   - If a difference is practically significant but not statistically significant, more data may be needed.\n4. **Data dredging**: Avoid fishing for significance (data snooping). This can lead to publication bias against negative results.\n5. **Good statistics from bad data**: Biased or non-random data compromise conclusions. Experimental design determines which inferences and statistical methods are valid.\n\n\n## Linear Combination of Group Means\n\n- ANOVA allows only pairwise comparisons of means and does not account for meaningful structure across groups. In practice, you may wish to compare the average of several means to a single group, or to the average of others.  \n- The ANOVA *F*-test serves as an initial screening tool for detecting any overall differences in means. If the *F*-test is significant, follow-up tests can be considered.\n\n### Linear Contrasts\n- A **linear combination** of group means is defined as:\n  $$\n  \\gamma = C_1\\mu_1 + C_2\\mu_2 + \\dots + C_I\\mu_I\n  $$\n  where $\\gamma$ represents the linear combination and the $C_i$ are coefficients. When the coefficients sum to zero, the combination is called a **contrast**.\n- Estimate $\\gamma$ using $g = C_1\\bar{x}_1 + C_2\\bar{x}_2 + \\dots + C_I\\bar{x}_I$, where $\\bar{x}_i$ are group sample means.\n- The standard error of $g$, denoted $SE(g)$, is given by:\n  $$\n  SE(g) = S_p \\sqrt{\\frac{C_1^2}{n_1} + \\frac{C_2^2}{n_2} + \\dots + \\frac{C_I^2}{n_I}}\n  $$\n  where $S_p$ is the pooled standard deviation.  \n  Even when comparing only two groups (i.e., the other coefficients and terms under the square root are zero), the pooled standard deviation is used under the assumption of equal variances across all groups.\n- The *t*-statistic for testing contrasts is:\n  $$\n  t_{\\alpha, df} = \\frac{g - \\gamma}{SE(g)}\n  $$\n  where $g$ is the observed linear combination, $\\gamma$ is the hypothesized value (e.g., $\\gamma = 0$), and $df$ is the degrees of freedom associated with $S_p$.\n\n\n## Steps for a Linear Contrast Test\n\n1. **Summarize the data**: Obtain group means, sample sizes, and the pooled standard deviation from the ANOVA model (e.g., RMSE).\n2. **Specify the coefficients**: Choose $C_i$ values such that $\\sum C_i = 0$.\n3. **Estimate the contrast**: Compute the linear combination $g$ using the sample means and chosen coefficients.\n4. **Compute the standard error**: Calculate $SE(g)$ using the formula above.\n5. **Construct confidence intervals**:\n   $$\n   CI = g \\pm t_{\\alpha, df} \\cdot SE(g)\n   $$\n6. **Perform the test**: Calculate the *t*-statistic to assess the contrast.\n\n### Simultaneous Inferences\n- When testing multiple contrasts or pairwise comparisons, the risk of a **Type I error** increases. Adjustments to significance levels or *p*-values are needed to maintain the overall error rate.\n- Distinguish between:\n  - **Individual confidence level**: The probability that a single interval captures its parameter ($1 - \\alpha$).\n  - **Familywise confidence level**: The probability that all intervals simultaneously capture their parameters. This probability is less than $1 - \\alpha$ unless corrections are applied.\n\n### Common Applications of Linear Contrasts\n1. Comparing average values, e.g., the mean of groups 1 and 2 versus group 3.\n2. Comparing response rates across treatments, such as dietary interventions in animals.\n3. Testing specific hypotheses about structured combinations of group means.\n\n\n## Adjusting for Multiple Comparisons\n\nWhen conducting multiple tests, it is important to control the **familywise error rate** (FWER). Common post-hoc procedures are outlined below.\n\n\n### Procedures for All Pairwise Differences in Means\n\n1. **Bonferroni adjustment**  \n   - Adjusts the significance level:\n     $$\n     \\text{Adjusted } \\alpha = \\frac{\\alpha}{\\# \\text{ comparisons}}\n     $$\n   - Example: For 4 groups, there are $\\binom{4}{2} = 6$ pairwise comparisons (i.e., 4 choose 2):\n     $$\n     \\alpha' = \\frac{0.05}{6} = 0.0083\n     $$\n   - **Strengths**: Simple, widely applicable  \n   - **Weaknesses**: Very conservative; reduces power\n2. **Tukey’s Honestly Significant Difference (HSD)**  \n   - Used to construct simultaneous confidence intervals for all pairwise mean differences  \n   - Based on the **studentized range statistic**:\n     $$\n     q = \\frac{\\bar{X}_{\\text{largest}} - \\bar{X}_{\\text{smallest}}}{\\sqrt{\\frac{MSE}{n}}}\n     $$\n     (for groups with equal $n$)  \n   - Confidence intervals:\n     $$\n     (\\bar{X}_i - \\bar{X}_j) \\pm q_{\\alpha, (k, N-k)} \\cdot \\sqrt{\\frac{MSE}{n}}\n     $$\n     - When group sizes are unequal, use the **harmonic mean** of $n_i$ and $n_j$  (as done in SAS):\n     $$\n     n_{ij} = \\frac{2 \\cdot n_i \\cdot n_j}{n_i + n_j}\n     $$\n   - **Strengths**: Maintains FWER; easy to interpret  \n   - **Weaknesses**: Assumes equal variances (can be extended via Tukey-Kramer)\n3. **Ryan-Elliot-Gabriel-Welsch Q (REGWQ) procedure**  \n   - Recommended by SAS; balances power and Type I error control  \n   - **Method**:\n     1. Order the group means in descending order\n     2. Reject $H_0$ if:\n        $$\n        \\text{Difference} > q_{\\text{critical}} \\cdot \\text{Studentized range}\n        $$\n     3. Adjust the *p*-values for the number of comparisons  \n   - **Strengths**: High power; nominal Type I error  \n   - **Weaknesses**: Algorithmically complex\n\n### Procedures for Pairwise Differences vs. Control\n1. **Dunnett’s procedure**  \n   - Compares each treatment group to a single control group \n   - Uses the **$D$-statistic**:\n     $$\n     D = d_{(k-1, N-k)} \\cdot \\sqrt{\\frac{2 \\cdot MSE}{n_{\\text{harmonic}}}}\n     $$\n   - Degrees of freedom:\n     $$\n     df = (k - 1, N - k)\n     $$\n     where $k - 1$ is the between-groups degrees of freedom and $N - k$ is the within-groups degrees of freedom\n   - **Strengths**: Targets control comparisons directly  \n   - **Weaknesses**: Not designed for all pairwise tests and limited to control-versus-treatment tests\n   \n### Procedures for All Possible Comparisons\n1. **Scheffé’s method**  \n   - Allows for testing *any* linear combination of means \n   - Used for all possible comparisons, including non-pairwise contrasts\n   - Based on the *F*-distribution  \n   - **Strengths**: Flexible, ideal for complex non-pairwise hypotheses, and less conservative than Bonferroni\n   - **Weaknesses**: Less powerful for simple pairwise tests   \n\n\n## Key Considerations for Post-Hoc Comparisons\n\n1. **Type I Error Control**  \n   - Multiple tests increase the chance of a false positive (Type I error).  \n   - Even if all null hypotheses are true, testing too many comparisons increases the likelihood of finding a “significant” result by chance.  \n   - Post-hoc adjustments help control the overall Type I error rate.\n2. **Planned vs. unplanned comparisons**  \n   - Use **planned comparisons** when testing specific hypotheses defined in advance.  \n   - Use **unplanned post-hoc comparisons** when no prior hypotheses were made, but the ANOVA is significant.\n3. **Decision framework after ANOVA**\n   - If the ***F*-statistic is not significant**, no further testing is needed. There is no evidence that group means differ.\n   - If the ***F*-statistic is significant**, then post-hoc comparisons are appropriate. The choice of follow-up method depends on:\n     - Whether there is meaningful structure in the groups (e.g., contrasts like \"average of groups A and B vs. group C\").\n     - Whether pairwise comparisons or structured contrasts best address your research question.\n\n### Summary Table of Post-Hoc Methods\n\n| **Procedure**               | **Purpose**                                | **Strengths**                                                | **Weaknesses**                                   |\n|-----------------------------|--------------------------------------------|------------------------------------------------------------|-------------------------------------------------|\n| Bonferroni Adjustment       | Controls Type I error for pairwise tests   | Simple, widely applicable                                   | Very conservative, reduced power               |\n| Tukey’s HSD                 | All pairwise differences                   | Maintains familywise error rate, easy interpretation        | Assumes equal variances                         |\n| Dunnett’s Procedure         | Compare to control group                   | Focused on control vs. treatments                          | Not suitable for all pairwise comparisons       |\n| REGWQ Procedure             | Pairwise differences                       | Good power, recommended by SAS                             | Requires algorithmic implementation             |\n| Scheffé’s Method            | All possible comparisons (non-pairwise)    | Flexible, good for complex comparisons                     | Less powerful for pairwise differences          |\n\n\n## Reporting Results\n\n- Discuss the research question, design, and assumptions.\n- Summarize the exploratory data analysis (EDA).\n- Report ANOVA results: *F*-statistic, degrees of freedom, and *p*-value.\n- Describe the effect size and post-hoc analysis used.\n- Conclude in the context of the original research question.\n\n\n## Linear Combination of Group Means  \n\n### Handicap & Capability Study: ANOVA and Follow-Up Contrast\n\n#### Global Question of Interest\n\n**Goal**:  \nHow do physical handicaps affect perception of employment qualification?\n\n**Assumptions**:  \n- Independent observations  \n- Normal distributions  \n- Equal variances  \n\nThere is no visual evidence to suggest these assumptions are violated.\n\n**ANOVA Setup**:  \nLet  \n\n- $H_0$: $\\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 = \\mu_5$ (equal means model)  \n- $H_a$: At least one $\\mu_i \\ne \\mu_j$ for some $i \\ne j$\n\n**Results**:  \n\n- *F*-statistic: $F_{4,65} = 2.85$  \n- *p*-value: 0.0301  \n- **Decision**: Reject $H_0$\n\n**Conclusion**:  \nThere is sufficient evidence to suggest that at least two group means differ (*p* = 0.0301 from a one-way ANOVA).\n\n### Targeted Contrast: Comparing Two Group Averages\nWe now test whether the average score of the Amputee and Hearing groups differs from that of the Crutch and Wheelchair groups.\n\n#### Hypotheses\n$$\nH_0: \\frac{\\mu_{\\text{Amp}} + \\mu_{\\text{Hear}}}{2} = \\frac{\\mu_{\\text{Crutch}} + \\mu_{\\text{Wheel}}}{2}\n$$\n$$\nH_A: \\frac{\\mu_{\\text{Amp}} + \\mu_{\\text{Hear}}}{2} \\ne \\frac{\\mu_{\\text{Crutch}} + \\mu_{\\text{Wheel}}}{2}\n$$\n\nRewritten by subtraction:\n$$\nH_0: \\frac{\\mu_{\\text{Amp}} + \\mu_{\\text{Hear}}}{2} - \\frac{\\mu_{\\text{Crutch}} + \\mu_{\\text{Wheel}}}{2} = 0\n$$\n$$\nH_A: \\frac{\\mu_{\\text{Amp}} + \\mu_{\\text{Hear}}}{2} - \\frac{\\mu_{\\text{Crutch}} + \\mu_{\\text{Wheel}}}{2} \\ne 0\n$$\n\nMultiply both sides by 2:\n$$\nH_0: \\mu_{\\text{Amp}} + \\mu_{\\text{Hear}} - \\mu_{\\text{Crutch}} - \\mu_{\\text{Wheel}} = 0\n$$\n$$\nH_A: \\mu_{\\text{Amp}} + \\mu_{\\text{Hear}} - \\mu_{\\text{Crutch}} - \\mu_{\\text{Wheel}} \\ne 0\n$$\n\n#### Defining the Contrast\nThis contrast compares the average of the Amputee and Hearing groups to the average of the Crutch and Wheelchair groups, excluding the None group. The coefficients (weights) are assigned based on the groupings of interest. The group labels are listed alphabetically, which is the default in most statistical software.\n\n$$\n\\gamma = 1\\mu_{\\text{Amp}} - 1\\mu_{\\text{Crutch}} + 1\\mu_{\\text{Hear}} + 0\\mu_{\\text{None}} - 1\\mu_{\\text{Wheel}}\n$$\n\nHere, $\\gamma$ represents the *population value* of the contrast---a linear combination of group means. A value of $\\gamma = 0$ would indicate no difference between the two averaged group sets.\n\n#### Method: Linear Contrast Formulas and Test Statistic\nTo evaluate this question, we use a linear contrast to compare combined group means.\n\n- **Hypotheses**:\n  $$\n  H_0: \\gamma = 0 \\quad \\text{vs.} \\quad H_A: \\gamma \\ne 0\n  $$\n- **General definition of a contrast**:\n  $$\n  \\gamma = C_1\\mu_1 + C_2\\mu_2 + \\dots + C_I\\mu_I\n  $$\n  where $C_i$ are the contrast weights and must satisfy the constraint:  \n  $$\n  \\sum C_i = 0\n  $$\n- **Contrast estimate**:\n  $$\n  g = C_1\\bar{x}_1 + C_2\\bar{x}_2 + \\dots + C_I\\bar{x}_I\n  $$\n  where $C_i$ are the contrast weights and $\\bar{x}_i$ are the sample means.\n- **Standard error**:\n  $$\n  SE(g) = S_p \\sqrt{ \\frac{C_1^2}{n_1} + \\frac{C_2^2}{n_2} + \\dots + \\frac{C_I^2}{n_I} }\n  $$\n  where $S_p = \\sqrt{MSE}$ is the pooled standard deviation (assuming equal variances and independent observations).\n- **Test statistic**:\n  $$\n  t = \\frac{g - \\gamma}{SE(g)}\n  $$\n  This follows a *t*-distribution with degrees of freedom:\n  $$\n  df = N - I\n  $$\n  where $N$ is the total number of observations and $I$ is the number of groups.\n\n::: {.example}\n\n### Worked Example: Difference of Sums\n- **Hypotheses**:\n  $$\n  H_0: \\frac{\\mu_{\\text{Amp}} + \\mu_{\\text{Hear}}}{2} = \\frac{\\mu_{\\text{Crutch}} + \\mu_{\\text{Wheel}}}{2}\n  $$\n  $$\n  H_A: \\frac{\\mu_{\\text{Amp}} + \\mu_{\\text{Hear}}}{2} \\ne \\frac{\\mu_{\\text{Crutch}} + \\mu_{\\text{Wheel}}}{2}\n  $$\n- **Contrast coefficients**:\n  $$\n  \\gamma = 1\\mu_{\\text{Amp}} - 1\\mu_{\\text{Crutch}} + 1\\mu_{\\text{Hear}} + 0\\mu_{\\text{None}} - 1\\mu_{\\text{Wheel}}\n  $$\n- **Estimate $\\gamma$ with $g$ using group means**:\n  $$\n  g = 1\\bar{x}_{\\text{Amp}} - 1\\bar{x}_{\\text{Crutch}} + 1\\bar{x}_{\\text{Hear}} + 0\\bar{x}_{\\text{None}} - 1\\bar{x}_{\\text{Wheel}}\n  $$\n  $$\n  g = (1)(4.43) + (-1)(5.92) + (1)(4.05) + (0)(4.9) + (-1)(5.34) = -2.78\n  $$\n\nThe estimated contrast represents the estimated difference between the sum of the populations means of the Amputee and Hearing groups and the sum of the populations means of of the Crutch and Wheelchair groups.\n\n- **Standard error**:\n  $$\n  SE(g) = \\sqrt{2.6666 \\left( \\frac{1^2}{14} + \\frac{(-1)^2}{14} + \\frac{1^2}{14} + \\frac{0^2}{14} + \\frac{(-1)^2}{14} \\right)} = 0.8729\n  $$\n  where $S_p = \\sqrt{MSE} = \\sqrt{2.6666}$\n\n#### Confidence Interval (95%) for the Difference of Sums\n- **Formula**:\n  $$\n  g \\pm t_{df, 0.975} \\cdot SE(g)\n  $$\n- **Degrees of freedom**:\n  $$\n  df = 70 - 5 = 65 \\quad \\Rightarrow \\quad t_{65, 0.975} = 1.991\n  $$\n- **Calculation**:\n  $$\n  -2.78 \\pm (1.991)(0.8729)\n  $$\n- **Interval for the difference of sums**:\n  $$\n  CI = (-4.518, -1.042)\n  $$\n- **Interval for the difference of means** (averaging over 2 vs. 2 groups):\n  $$\n  CI = \\left( \\frac{-4.518}{2}, \\frac{-1.042}{2} \\right) = (-2.259, -0.521)\n  $$\n\nThere is sufficient evidence that the sum of points assigned to the Amputee and Hearing groups is smaller ($g = -2.8$) than the sum of points assigned to the Crutch and Wheelchair groups at the $\\alpha = 0.05$ confidence level, because the confidence interval does not contain 0.\n\n#### Summary of Findings\n\n- The initial ANOVA showed a statistically significant difference across all five groups (*p* = 0.0301).\n- A targeted contrast comparing the average of the Amputee and Hearing groups to that of the Crutch and Wheelchair groups yielded:\n  - Contrast estimate: $g = -2.78$\n  - 95% CI for difference of means: $(-2.26, -0.52)$\n  - $t(65) = -2.78$, *p* < 0.01\n\nWe conclude that the Amputee and Hearing groups were rated significantly lower, on average, than the Crutch and Wheelchair groups.\n)\n\n:::\n\n::: {.example}\n\n### Hypothesis Test---Sums\n1. **Hypotheses**:\n   $$\n   H_0: \\mu_{\\text{Amp}} + \\mu_{\\text{Hear}} = \\mu_{\\text{Crutch}} + \\mu_{\\text{Wheel}}\n   $$\n   $$\n   H_A: \\mu_{\\text{Amp}} + \\mu_{\\text{Hear}} \\ne \\mu_{\\text{Crutch}} + \\mu_{\\text{Wheel}}\n   $$\n2. **Critical value**:\n   $$\n   t_{65}(0.975) = 1.9971\n   $$\n3. ***t*-statistic**:\n   $$\n   SE(g) = 0.8729 \\Rightarrow t_{\\text{stat}} = \\frac{-2.78}{0.8729} = -3.19\n   $$\n4. ***p*-value**: $p = 0.0022$\n5. **Decision**: Reject $H_0$\n6. **Conclusion**:\n\n   There is strong evidence to suggest that the sum of the means of the Amputee and Hearing groups is less than that of the Crutch and Wheelchair groups (*p*-value = 0.0022). The 95% confidence interval for the difference of sums is $(-4.529, -1.043)$ points.\n\n:::\n\n::: {.example}\n\n### Hypothesis Test---Means (Not Sums)\n\n1. **Hypotheses**:\n   $$\n   H_0: \\frac{\\mu_{\\text{Amp}} + \\mu_{\\text{Hear}}}{2} = \\frac{\\mu_{\\text{Crutch}} + \\mu_{\\text{Wheel}}}{2}\n   $$\n   $$\n   H_A: \\frac{\\mu_{\\text{Amp}} + \\mu_{\\text{Hear}}}{2} \\ne \\frac{\\mu_{\\text{Crutch}} + \\mu_{\\text{Wheel}}}{2}\n   $$\n2. **Critical value**:\n   $$\n   t_{65}(0.975) = 1.9971\n   $$\n3. **Standard error**:\n   $$\n   SE(g) = \\sqrt{2.6666 \\left( \\frac{(0.5)^2}{14} + \\frac{(-0.5)^2}{14} + \\frac{(0.5)^2}{14} + \\frac{0^2}{14} + \\frac{(-0.5)^2}{14} \\right)} = \\sqrt{0.1905} = 0.4364\n   $$\n   **Test statistic**:\n   $$\n   g = -1.393 \\quad \\Rightarrow \\quad t = \\frac{g - \\gamma}{SE(g)} = \\frac{g - 0}{SE(g)} = \\frac{-1.393}{0.4364} = -3.19\n   $$\n   Same $t$-statistic as for sums, but different $g$ because the 0.5 vs. 1 and different SE.\n4. ***p*-value**: $p = 0.0022$\n5. **Decision**: Reject $H_0$\n6. **Conclusion**:\n\n   There is strong evidence to suggest that the *average* of the means of the Amputee and Hearing groups is less than that of the Crutch and Wheelchair groups (*p*-value = 0.0022). The 95% confidence interval for the difference in *means* is (-2.26, -0.52) points.\n\n> Note: This is based on means, not sums.\n\n#### Final Conclusion (Client-Facing---Difference in Group Means)\n\nThere is strong evidence to suggest that the average of the means of the Amputee and Hearing groups is less than the average of the means of the Crutches and Wheelchair groups (*p*-value = 0.0022).\n\nWe are 95% confident that the average of the mean scores of the Crutches and Wheelchair groups is between 0.5215 and 2.2605 points greater than the average of the mean points of the Amputee and Hearing groups.\n\n:::\n\n### Why We Do a Multiple Comparison Correction — The Bonferroni\n\n#### Motivation\n\nWhen conducting multiple hypothesis tests, the probability of making at least one **Type I error** (a false positive, rejecting $H_0$ when $H_0$ is true) increases. Bonferroni correction helps control this overall risk by adjusting the significance level for individual tests.\n\n### Probability Review: Two Coin Flips\n\n#### Prop 1: Independence of Events\n$$\nP(AB) = P(A) \\cdot P(B)\n$$\n\nExample:\n$$\nP(\\text{HH}) = P(\\text{H}) \\cdot P(\\text{H}) = 0.5 \\cdot 0.5 = 0.25\n$$\n\n#### Prop 2: Sample Space of Two Fair Coin Flips\n\n| Outcome | Probability |\n|---------|-------------|\n| HH      | 0.25        |\n| HT      | 0.25        |\n| TH      | 0.25        |\n| TT      | 0.25        |\n\n**Probability of at least one head**:\n\n$$\nP(\\text{at least 1 H}) = 1 - P(\\text{no heads}) = 1 - P(\\text{TT}) = 1 - 0.25 = 0.75\n$$\n\n::: {.example}\n\n### Multiple Testing Analogy: Coin Flips\nThis parallels the logic behind multiple testing:\n\n- Getting two tails = no Type I error\n- Getting at least one head = at least one Type I error\n\nJust like the probability of getting at least one head increases with more coin flips, the probability of making at least one Type I error increases with more hypothesis tests — unless we adjust for it.\n\nThis illustrates how running multiple independent tests increases the probability of rejecting at least one true null---even if *all* nulls are actually true. That’s why corrections like the Bonferroni adjustment are necessary.\n\n:::\n\n### Familywise Error Rate With Multiple Tests\nLet $K = 2$ (number of independent hypothesis tests), and assume the null hypothesis is true in both.\n\n- **TIE** = Type I error  \n- **FTR** = Fail to reject $H_0$\n\n#### Probability of at Least One Type I Error Without Correction\n$$\nP(\\text{at least 1 TIE in 2 tests}) = 1 - P(\\text{no TIE in 2 tests}) = 1-P(\\text{FTR in 2 tests})\n$$\n\nAssuming each test has $\\alpha = 0.05$:\n\n$$\n= 1 - P(\\text{FTR})P(\\text{FTR}) = 1 - P(\\text{FTR})^2 = 1 - (1 - \\alpha)^2 = 1 - (0.95)^2 = 0.0975\n$$\n\n> This means there's a 9.75% chance of incorrectly rejecting at least one null hypothesis when both are actually true.\n\n### Bonferroni Adjustment\n\nTo maintain a **familywise error rate** of $\\alpha = 0.05$, Bonferroni sets a more stringent per-test threshold:\n\n$$\n\\alpha' = \\frac{\\alpha}{K} = \\frac{0.05}{2} = 0.025\n$$\nwhere:\n\n- $\\alpha$ is the desired familywise error rate (e.g., 0.05).\n- $k$ is the number of comparisons.\n- $\\alpha'$ is the adjusted threshold for each test, the per-comparison significance level.\n\nThen the familywise error becomes:\n\n$$\nP(\\text{at least 1 TIE in 2 tests}) = 1 - P(\\text{no TIE in 2 tests}) = 1 - P(\\text{FTR})^2 = 1 - (1 - \\alpha')^2 = 1 - (0.975)^2 = 0.0494\n$$\n\nSo Bonferroni successfully lowers the overall chance of making a Type I error across all tests back to the desired 0.05 level.\n\n### Interpreting the Diagram\n\n![**Bell curve showing original $\\alpha$ and Bonferroni-corrected rejection regions.**\nThe Bonferroni-adjusted regions lie farther from 0, reflecting the more stringent critical values.](images/fch07_bonferroni.png)\n\nThe shaded areas represent rejection regions under two scenarios:\n\n- Outer shaded tails: Original $\\alpha = 0.05$ rejection regions (2.5% in each tail)\n- Inner shaded tails: Bonferroni-adjusted $\\alpha' = 0.025$ rejection regions (1.25% in each tail)\n\nThe critical value for Bonferroni (CVB) is the $t$- or $z$-value that corresponds to $\\alpha'$, which lies further from 0 than the uncorrected critical value. This makes it harder to reject $H_0$.\n\n### Key Concepts Illustrated\n- Without correction:\n  $$\n  P(\\text{at least one TIE}) = 1 - (1 - \\alpha)^K = 1 - (0.95)^2 = 0.0975\n  $$\n- With Bonferroni correction:\n  $$\n  \\alpha' = \\frac{0.05}{2} = 0.025 \\quad \\Rightarrow \\quad P(\\text{at least one TIE}) = 1 - (0.975)^2 = 0.0494\n  $$\n- CVB (critical value for Bonferroni) moves farther from 0, tightening the rejection region.\n- Bonferroni maintains a familywise error rate at or below 0.05.\n\n### Summary\n- When we conduct multiple comparisons, the overall Type I error rate is inflated.\n- The Bonferroni correction addresses this by adjusting the per-comparison significance level:\n  $$\n  \\alpha' = \\frac{\\alpha}{K}\n  $$\n- Pros:\n  - Simple and widely applicable\n  - Works for any number of tests\n  - Makes no assumptions about the test structure\n  - Can be applied directly to *p*-values\n- Cons:\n  - Can be overly conservative\n  - Reduces power, making it harder to reject $H_0$\n\n### Multiple Comparison Procedures — Handicap Study\n\nWe now explore multiple comparison questions using the Handicap study data. These comparisons build on the need for error rate control, such as the Bonferroni adjustment, after identifying overall group differences.\n\n#### Questions of Interest (QOIs)\n\n1. **Are any pairs of group means different?**  \n2. **Does the Amputee group have a different mean score than the None group?**  \n3. **Which specific pairs of groups differ?**  \n4. **Do the means of the four Handicap groups differ from the Non-Handicap (None) group?**\n\nEach of these questions involves multiple comparisons and may require adjustment for familywise error rates.\n\n#### QOI 1: Are Any Group Means Different?\n\nWe begin with an overall test to determine if there are any mean differences across the five groups. This involves an ANOVA and is the first step in any further group comparisons.\n\n- **Assumptions**:\n  - Normality  \n  - Equal variances (e.g., visual checks and Brown–Forsythe test)  \n  - Independence (assumed from study design)\n\n1. **Hypotheses**:\n$$\nH_0: \\mu_1 = \\mu_2 = \\dots = \\mu_5 \\quad \\text{(All group means are equal)}\n$$\n$$\nH_A: \\text{At least two group means differ}\n$$\n2. **Critical value**: $F_{\\alpha, 4, 65}$\n3. **ANOVA test statistic ($F$-statistic)**: $F = 2.85$\n4. **p-value**: $p = 0.0301$\n5. **Decision**: Reject $H_0$ (*needle in the haystack*)\n6. **Conclusion**: There is sufficient evidence at $\\alpha = 0.05$ to conclude that at least two group means differ ($p = 0.0301$, from a standard ANOVA).\n\n- Since the omnibus ANOVA test is significant, we proceed to investigate which group differences are driving the result. Multiple comparison procedures will be necessary to identify specific contrasts.\n\n**Model fitting R code (for context)**:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- aov(Score ~ Handicap, data = Handicap)\n```\n:::\n\n\n\n\n#### QOI 2: Does the Amputee Group Differ from the None Group?\n\nWe now test whether the mean score for the Amputee group differs from that of the None group.\n\n- This contrast was identified *before* looking at the data (i.e., a **planned comparison**), so we do *not* need to apply a multiple comparison correction.\n\n**Hypotheses**:\n\n$$\nH_0: \\mu_{\\text{Amp}} = \\mu_{\\text{None}}\n$$\n$$\nH_A: \\mu_{\\text{Amp}} \\ne \\mu_{\\text{None}}\n$$\n\n**Options for Testing the Contrast**:  \n\nWe could use one of the following approaches:\n\n1. **Two-sample $t$-test** (just Amputee vs. None):\n   - Pooled standard deviation  \n   - Degrees of freedom: $df = 26$  \n   - $t$-distribution \n   - $p$-value: 0.4678\n\n2. **Fit a GLM model using only Amputee and None**:\n   - Equivalent $F$-statistic  \n   - Same result as above: $df = 1$, $26$  \n   - $F$-distribution\n   - $p$-value: 0.4678\n\n3. **Best option**: Use **all 70 observations**:\n   - Use the **pooled standard deviation** from the full model  \n   - Conduct the planned contrast within the **global ANOVA model**\n   - Critical value: $F_{\\alpha, 4, 65}\n   - $F$-statistic with extended degrees of freedom: $df = 65$  \n   - $p$-value: 0.4477  \n\n**Conclusion**:\n\n- **Decision**: Fail to reject $H_0$\n- **Interpretation**: There is not sufficient evidence to suggest that the mean rating of the Amputee group is different from the None group (*p* = 0.4477 from a contrast using all the data).\n\n#### QOI 3: Which Specific Pairs of Groups Differ?\n\nWe now explore whether there is evidence that any specific pair of group means differ. These are **not planned comparisons**, so we must adjust for the increased chance of Type I error due to multiple testing.\n\n- There are 5 groups, which gives $K = \\binom{5}{2} = 10$ pairwise comparisons.\n- Without correction, the risk of at least one false positive (rejecting $H_0$ when it is true) increases.\n- The Bonferroni correction provides a simple solution by adjusting the significance threshold.\n\n**Unadjusted Procedure**  \nCompare each group pair using pairwise comparisons without correction:\n\n\n\n\n::: {.cell}\n\n```{.sas .cell-code}\nproc glm data=Handicap;\n  class Handicap;\n  model Score = Handicap;\n  means Handicap / hovtest=bf;\n  ls means Handicap / pdiff;\nrun;\n```\n:::\n\n\n\n\n- Compare each pair’s raw *p*-value to $\\alpha = 0.05$\n- Inflated Type I error risk due to multiple tests\n\n**Bonferroni Adjustment and Procedure**:  \nTo control the familywise error rate, adjust $\\alpha$ using the Bonferroni correction:\n\n- Adjusted threshold: $\\alpha' = \\frac{\\alpha}{K} = \\frac{0.05}{10} = 0.005$\n- Use Bonferroni-adjusted confidence intervals and *p*-values to control the error rate across the family of comparisons.\n\nThe Bonferroni-adjusted analysis in SAS uses the adjust=bon option:\n\n\n\n::: {.cell}\n\n```{.sas .cell-code}\nproc glm data=Handicap;\n  class Handicap;\n  model Score = Handicap;\n  means Handicap / hovtest=bf;\n  ls means Handicap / pdiff adjust=bon cl; *cl = confidence level;\nrun;\n```\n:::\n\n\n\n\n- The `cl` option provides Bonferroni-adjusted confidence intervals.\n- These intervals are wider because:\n  - $\\alpha$' is smaller than $\\alpha$.\n  - The critical value is larger.\n  - The multiplier increases.\n  - Wider intervals result from a more conservative correction.\n- The `cldiff` option gives both directions (e.g., $A - B$ and $B - A$).\n- Each *p*-value is multiplied by $K$ before comparison to the original $\\alpha$:\n  $$\n  K \\cdot p_{\\text{adj}} \\leq \\alpha\n  $$\n\n**Interpretation**:\n\n- This procedure protects against Type I error across the entire family of tests.\n- It is conservative but appropriate when many unplanned comparisons are being made.\n\n**Conclusion**:\n\nAfter applying the Bonferroni adjustment,only 1 of the 10 tests produced a statistically significant result. Therefore, there is evidence (*p*-value = 0.0035 from a *t*-test) that the Crutches and Hearing groups have different mean ratings. A 95% Bonferroni-adjusted confidence interval for the difference in means between the Crutches and Hearing groups is $(0.0779,\\ 3.6649)$.\n\n#### QOI 4: Do the Means of the Four Handicap Groups Differ from the Non-Handicap (None) Group?\n\nAssume we are interested in testing whether the means of the four handicap groups differ from the mean of the non-handicap group (i.e., the None group).\n\n- Use **Dunnett’s procedure** because we are comparing the control group (None) to all the others — a pairwise set of **dependent** comparisons (repeating the control group).\n\nOption 1: Dunnett-adjusted confidence intervals\n\n- Specify the control group on the right side using `dunnett('None')`\n- Also provides Dunnett-corrected confidence intervals\n\n\n\n\n::: {.cell}\n\n```{.sas .cell-code}\nproc glm data=Handicap;\n  class Handicap;\n  model Score = Handicap;\n  means Handicap / hovtest=bf dunnett('None');\nrun;\n```\n:::\n\n\n\n\nOption 2: Dunnett-adjusted *p*-values only\n\n- Use `control('None')` to specify the control group\n- Provides Dunnett-adjusted *p*-values for all pairwise comparisons with the None group\n\n\n\n\n::: {.cell}\n\n```{.sas .cell-code}\nproc glm data=Handicap;\n  class Handicap;\n  model Score = Handicap;\n  ls means Handicap / pdiff=control('None');\nrun;\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}