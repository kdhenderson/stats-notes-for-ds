{
  "hash": "bda2a184a0274190db2896f5f552edb6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Quantifying Uncertainty: Confidence, Prediction and Calibration Intervals\"\n---\n\n\n\n\n## Objectives\n\n- Calculate and interpret confidence intervals for the slope of a regression line.\n- Predict future values using the regression model.\n- Obtain and interpret confidence intervals for predicted responses.\n- Use a regression model to calibrate one measurement against another.\n\n## Useful Resources\n\n- [Rossman Chance Applets: Regression Shuffle](https://www.rossmanchance.com/applets/2021/regshuffle/regshuffle.htm)\n\n\n## Statistical Relationships\n\n- Key Insight: The value of the explanatory variable determines the mean response value.\n- Variability exists in the response variable for a given explanatory value.\n- The slope and intercept each have their own sampling distribution and standard error.\n\n\n## Formal Assumptions\n\n- **Linearity**: A linear relationship exists between the means of the response variable distributions and the explanatory variable.\n- **Independence**: Observations are independent.\n- **Normality**: The response variable $y$ is normally distributed for each fixed $x$ value. \n  - Errors are in $y$, not $x$.\n  - Normality is assumed for $y$ at each fixed $x$, not for $y$ overall.\n- **Constant variance**: The variability in the $y$ distributions is constant across all values of $x$.\n\n\n## Regression Model\n\n- **Theoretical model**:\n  $$\n  y = \\beta_0 + \\beta_1 x + \\varepsilon\n  $$\n  - $y =$ mean $\\pm$ residual, where the residual represents the difference between the observed value and the mean.\n  - Residuals ($\\varepsilon$) follow a normal distribution with mean zero and constant variance ($\\sigma^2$).\n- **Regression line (estimated values)**: $\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x$\n- **Residuals**: $e_i = y_i - \\hat{y}_i = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)$\n- **Distribution of residuals**: $\\varepsilon \\sim N(0, \\sigma^2)$  \n  - Residuals are normally distributed around zero, with estimated standard deviation $\\hat{\\sigma}$, i.e., the standard deviation of each $y$ distribution.\n\n\n## Using Residual Plots to Check Regression Validity\n\n- Key points:\n  - Residual plots help validate regression assumptions.\n  - Random patterns in residuals suggest a valid model.\n    - Look for randomness, no obvious pattern, constant variability (homoscedasticity), and a mean of zero.\n    - Watch for outliers or patterns that might suggest curvature or unequal spread.\n    - Narrower bands of residuals (i.e., smaller spread) suggest a stronger relationship between $x$ and $y$; wider spread suggests a weaker relationship.\n    - It’s easier to detect deviations from a horizontal line (in residuals) than from a sloped regression line.\n  - A Q–Q plot of residuals assesses normality.\n  - If residuals are randomly distributed with constant variance, and the Q–Q plot shows normality, the model is appropriate for inference.\n\n\n## Analysis of Variance in Regression\n\n### Data Summary\n\n| Level | Score 1 | Score 2 | Score 3 |\n|--------|---------|---------|---------|\n| 1      | 3       | 5       | 7       |\n| 2      | 10      | 12      | 14      |\n| 3      | 20      | 22      | 24      |\n\nSample size: $n = 9$\n\n### Regression Equation\nFitting a linear model:\n$$\n\\text{Score} = -4 + 8.5 \\cdot \\text{Level}\n$$\n\n### Equal Means Model vs. Regression Model Comparison\n\n#### Total Variability Under the Equal Means Model\n\nThe **Equal Means Model (EMM)** assumes a single grand mean across all groups:\n\n$$\n\\bar{y} = 13\n$$\n\nWe compute the squared deviations from the grand mean for each observation. These add up to the **Total Sum of Squares (SST)**.\n\n::: {.mytableblock title=\"Equal Means Model: Squared Deviations from Grand Mean\"}\n\n<div class=\"figure-caption\">\n<strong>Equal Means Model: Squared Deviations from Grand Mean</strong>\n</div>\n\n| $\\textcolor{#BB4444}{\\textbf{Level}}$ | $\\textcolor{#BB4444}{\\textbf{Obs1}}$ | $\\textcolor{#BB4444}{\\textbf{Obs2}}$ | $\\textcolor{#BB4444}{\\textbf{Obs3}}$ |\n|--------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|\n| $\\textcolor{#BB4444}{1}$             | $\\textcolor{#BB4444}{(3 - 13)^2 = 100}$  | $\\textcolor{#BB4444}{(5 - 13)^2 = 64}$   | $\\textcolor{#BB4444}{(7 - 13)^2 = 36}$  |\n| $\\textcolor{#BB4444}{2}$             | $\\textcolor{#BB4444}{(10 - 13)^2 = 9}$   | $\\textcolor{#BB4444}{(12 - 13)^2 = 1}$   | $\\textcolor{#BB4444}{(14 - 13)^2 = 1}$   |\n| $\\textcolor{#BB4444}{3}$             | $\\textcolor{#BB4444}{(20 - 13)^2 = 49}$  | $\\textcolor{#BB4444}{(22 - 13)^2 = 81}$  | $\\textcolor{#BB4444}{(24 - 13)^2 = 121}$ |\n\n:::\n\n$$\n\\text{SST} = \\sum (y_i - \\bar{y})^2 = \\textcolor{#BB4444}{462}\n$$\n\n#### Residual Variability Under the Regression Model\n\nWe now compute residuals from the fitted regression model:\n\n$$\n\\hat{y}_i = -4 + 8.5 \\cdot x_i\n$$\n\n::: {.mytableblock title=\"Regression Model: Squared Residuals\"}\n\n<div class=\"figure-caption\">\n<strong>Regression Model: Squared Residuals</strong>\n</div>\n\n| $\\textcolor{#44AA55}{\\textbf{Level}}$ | $\\textcolor{#44AA55}{\\textbf{Obs1}}$ | $\\textcolor{#44AA55}{\\textbf{Obs2}}$ | $\\textcolor{#44AA55}{\\textbf{Obs3}}$ |\n|--------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|\n| $\\textcolor{#44AA55}{1}$             | $\\textcolor{#44AA55}{(3 - 32.5)^2 = 6.25}$    | $\\textcolor{#44AA55}{(5 - 32.5)^2 = 20.25}$  | $\\textcolor{#44AA55}{(7 - 32.5)^2 = 56.25}$ |\n| $\\textcolor{#44AA55}{2}$             | $\\textcolor{#44AA55}{(10 - 13)^2 = 9}$        | $\\textcolor{#44AA55}{(12 - 13)^2 = 1}$       | $\\textcolor{#44AA55}{(14 - 13)^2 = 1}$       |\n| $\\textcolor{#44AA55}{3}$             | $\\textcolor{#44AA55}{(20 - 19.5)^2 = 0.25}$   | $\\textcolor{#44AA55}{(22 - 19.5)^2 = 6.25}$  | $\\textcolor{#44AA55}{(24 - 19.5)^2 = 20.25}$ |\n\n:::\n\n$$\n\\text{SSE} = \\sum (y_i - \\hat{y}_i)^2 = \\textcolor{#44AA55}{28.5}\n$$\n\n#### Decomposition of Variance\n\n::: {.mytableblock title=\"ANOVA Table: Comparing Regression to Equal Means Model\"}\n\n<div class=\"figure-caption\">\n<strong>ANOVA Table: Comparing Regression to Equal Means Model</strong>\n</div>\n\n| $\\textcolor{#4477DD}{\\textbf{Source}}$ | $\\textcolor{#4477DD}{\\textbf{df}}$ | $\\textcolor{#4477DD}{\\textbf{SS}}$ | $\\textcolor{#4477DD}{\\textbf{MS}}$ | $\\textcolor{#4477DD}{\\textbf{F}}$ | $\\textcolor{#4477DD}{\\mathit{p}\\text{-value}}$ |\n|-------------------------|----------------|------------------|------------------|----------------|----------------------|\n| $\\textcolor{#4477DD}{\\text{Model}}$     | $\\textcolor{#4477DD}{1}$ | $\\textcolor{#4477DD}{433.5}$  | $\\textcolor{#4477DD}{433.5}$            | $\\textcolor{#4477DD}{106.47}$         | $\\textcolor{#4477DD}{< 0.0001}$             |\n| $\\textcolor{#44AA55}{\\text{Error}}$     | $\\textcolor{#44AA55}{7}$ | $\\textcolor{#44AA55}{28.5}$   | $\\textcolor{#AA9933}{4.07}$             |                |                      |\n| $\\textcolor{#BB4444}{\\text{Total}}$     | $\\textcolor{#BB4444}{8}$ | $\\textcolor{#BB4444}{462}$    |                  |                |                      |\n\n:::\n\n> **Note**: The regression model estimates both a slope and an intercept, using 2 degrees of freedom. This reduces the error degrees of freedom from 8 (in the Equal Means Model) to 7:\n>\n> $$\n> \\text{df}_{\\text{Error}} = n - 2 = 9 - 2 = 7\n> $$\n\n#### $R^2$ and RMSE Interpretation\n\nThe $R^2$ value quantifies how much of the total variability in the response is explained by the regression model. It is based on the decomposition of total variability into two parts: variability explained by the model (SSR) and unexplained variability due to error (SSE):\n\n$$\nR^2 = \\frac{\\text{SSR}}{\\text{SST}} = 1 - \\frac{\\text{SSE}}{\\text{SST}}\n$$\n$$\nR^2 = \\frac{\\textcolor{#4477DD}{433.5}}{\\textcolor{#BB4444}{462}} = 1 - \\frac{\\textcolor{#44AA55}{28.5}}{\\textcolor{#BB4444}{462}} = 0.938\n$$\n\nThis means 93.8% of the variability in scores is explained by the regression model.\n\nThe **error variance** is measured by the **Mean Squared Error (MSE)**, and the square root of MSE gives the **Root Mean Squared Error (RMSE)**:\n\n$$\n\\textcolor{#AA9933}{\n  \\text{MSE} = 4.07 \\quad \\Rightarrow \\quad \n  \\text{RMSE} = \\hat{\\sigma} \\text{ of each } y \\text{ distribution} = \\sqrt{4.07} \\approx 2.02\n}\n$$\n\n#### Visual Comparison of Models\n\n![**Visual Comparison of Residuals: EMM vs. Regression Model.** The regression model reduces residual error compared to the EMM. Residuals from both models are shown for a single point to illustrate how $R^2$ captures relative model improvement.](images/fch09_residualAnalysis.png)\n\n#### F Distribution Under the Null Hypothesis\n\n![**F Distribution Under the Null Hypothesis.** Area to the right of $F = 106.47$ is shaded. This corresponds to a *p*-value < 0.0001 under the equal means model.](images/fch09_fdistribution.png)\n\nThe F-test compares how much variability is explained by the regression model relative to unexplained error. A large value like $F = 106.47$ indicates a significantly better fit than the Equal Means Model. \n\nBecause the *p*-value is less than 0.0001, we reject the null hypothesis that all group means are equal. The regression model explains a substantial proportion of the total variability in scores.\n\n\n## ANOVA Output in Software\n\n### SAS Code\n\n\n\n::: {.cell}\n\n```{.sas .cell-code}\nproc glm data=ToyExample;\n  model score = level / solution;\nrun;\n```\n:::\n\n\n\n\n### R Code\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(score ~ level, data = anovaData)\nanova(fit)      # ANOVA table: sums of squares, F-stat\nsummary(fit)    # Coefficient table, t-tests, R², F-stat\n```\n:::\n\n\n\n\n::: {layout-ncol=2}\n\n![](images/fch09_ANOVA_sasOutput.png){fig-title=\"ANOVA Output in SAS.\"}\n\n![](images/fch09_ANOVA_ROutput.png){fig-title=\"ANOVA Output in R.\"}\n\n:::\n\n::: {layout-ncol=2}\n\n![](images/fch09_regression_ROutput.png){fig-title=\"Regression Summary from summary(fit) in R.\"}\n\n:::\n\n<div class=\"figure-caption\"> <strong>ANOVA and Regression Output from SAS and R.</strong> The top row compares ANOVA tables from SAS and `anova(fit)` in R, both showing the F-statistic and *p*-value for the model. The lower R output from `summary(fit)` includes the regression coefficients, $t$-tests, $R^2$, and overall F-test.</div>\n\n\n## Inferential Tools for Predicted Responses\n\nRegression models are used to predict response values given specific explanatory values.\n\nThere is uncertainty in the prediction due to sampling variability and model estimation error.\n\nDo we want:\n\n- a **confidence interval** for the *mean response* at a given $x$ value?  \n- or a **prediction interval** for an *individual value* at that $x$?\n\n\n### Confidence Intervals for the Mean Response\n\nThe regression line is modeling the mean of $Y$ at values of $X$.\n\n**Mean of $Y$, given $X_0$**:\n\n$$\n\\hat{Y}_{\\text{mean} \\mid X_0} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_0\n$$\n\nWe want to know how confident we are that $\\hat{Y}$ is near the true $Y$ value.\n\n**Standard error of the mean at $X = X_0$**:\n\n$$\nSE\\left( \\hat{Y}_{\\text{mean} \\mid X_0} \\right) = \\hat{\\sigma} \\sqrt{ \\frac{1}{n} + \\frac{(X_0 - \\bar{X})^2}{(n - 1) S_X^2} }\n$$\n\nwhere $S_X^2$ is the sample variance of the explanatory variable $X$.\n\nAs $X_0 \\to \\bar{X}$:\n\n$$\nSE\\left( \\hat{Y}_{\\text{mean} \\mid X_0} \\right) = SE\\left( \\bar{Y} \\right) = \\hat{\\sigma} \\sqrt{ \\frac{1}{n} + 0}\n$$\n\n::: {.callout-note}\n\nSee the [Study Hours and Exam Grades example](foundation-chapter08.html#example-study-hours-and-exam-grades) for a worked calculation of \n$\\bar{X}$, $s_X^2$, and $\\hat{\\sigma}$, which are used in computing \n$SE\\left( \\hat{Y}_{\\text{mean} \\mid X_0} \\right)$.\n\n:::\n\n![**Sampling Distribution of the Mean Response at $X_0$.** The dot marks the predicted mean response $\\hat{Y}_{\\text{mean} \\mid X_0}$ from the regression line. The sideways bell curve illustrates its sampling distribution across repeated samples. The vertical lines mark one standard error above and below the mean response; this is narrower than the full 95% confidence interval.](images/fch09_se_meanResponse.png)\n\n**Confidence interval for the mean response**:\n\n$$\n\\text{CI} = \\hat{Y} \\pm t_{\\alpha/2, n - 2} \\cdot SE\\left( \\hat{Y}_{\\text{mean} \\mid X_0} \\right)\n$$\n\nThe interval is wider for values of $X_0$ that are farther from $\\bar{X}$.\n\n### Prediction Intervals for Individual Responses\n\n**Individual value of $Y$, given $X_0$**:\n\n$$\n\\text{Pred}\\{Y \\mid X_0\\} = \\hat{Y}_{\\text{ind} \\mid X_0} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_0\n$$\n\nThis is the predicted *mean* response at $X_0$, obtained from the regression line. To estimate how far an individual observation might deviate from the predicted value, we must account for uncertainty in both the regression model and the individual value.\n\n**Standard error for predicting an individual**:  \n\nCombines uncertainty from estimating the mean (estimation error) and natural variability in individuals (random sampling error):\n\n$$\nSE\\left( \\hat{Y}_{\\text{ind} \\mid X_0} \\right) = \\hat{\\sigma} \\sqrt{ \n1 + \\frac{1}{n} + \\frac{(X_0 - \\bar{X})^2}{(n - 1) S_X^2} }\n$$\n\nTo highlight the two sources of uncertainty:\n\n$$\nSE\\left( \\hat{Y}_{\\text{ind} \\mid X_0} \\right) = \\hat{\\sigma} \\sqrt{ \n\\underbrace{1}_{\\text{individual variability}} + \n\\underbrace{\\frac{1}{n} + \\frac{(X_0 - \\bar{X})^2}{(n - 1) S_X^2}}_{\\text{estimation uncertainty}} }\n$$\n\nNote that the second term under the square root matches the formula for the standard error of the mean response at $X_0$:\n\n$$\nSE\\left( \\hat{Y}_{\\text{mean} \\mid X_0} \\right)^2 = \\hat{\\sigma}^2 \\left( \n\\frac{1}{n} + \\frac{(X_0 - \\bar{X})^2}{(n - 1) S_X^2} \\right)\n$$\nThus:\n\n$$\nSE\\left( \\hat{Y}_{\\text{ind} \\mid X_0} \\right) = \\hat{\\sigma} \n\\sqrt{\\underbrace{1}_{\\text{individual variability}} + \n\\underbrace{\\text{estimation variance}}_{\\text{mean response}}}\n$$\n\n- The 1 reflects random variation in individual responses. Even if we knew the true mean exactly, individuals naturally vary around it.\n- The remaining terms reflect sampling variability in the regression coefficients used to compute $\\hat{Y}_{\\text{ind} \\mid X_0}$.\n\n**Prediction interval**:\n\n$$\n\\text{PI} = \\hat{Y}_{\\text{ind}} \\pm t_{\\alpha/2, n - 2} \\cdot SE\\left( \\hat{Y}_{\\text{ind} \\mid X_0} \\right)\n$$\n\nPrediction intervals are wider than confidence intervals because they include additional uncertainty for individual outcomes.  \nThey are narrowest when $X_0 = \\bar{X}$ and widen as $X_0$ moves farther from the sample mean.\n\n![**Regression Line with Confidence and Prediction Intervals.** The dotted blue line shows the fitted regression line, and the vertical dotted line marks the sample mean $\\overline{X}$. The shaded green band represents the 95% confidence interval (CI) for the mean response $\\hat{Y}$, while the shaded orange band shows the 95% prediction interval (PI) for an individual response. The solid blue bell curve represents the sampling distribution of the predicted mean response at $\\overline{X}$ under repeated sampling. The center dot marks $\\hat{Y}(\\overline{X})$, and the upper and lower green dots indicate the 95% CI bounds, $\\hat{Y} \\pm t \\cdot SE_{\\text{mean}}$. The green bell-shaped curves mirror these bounds, visually suggesting the plausible range of values that the mean response might take across repeated samples.](images/fch09_CI_PI_intervals.png)\n\n**Understanding the Confidence Interval**:\n\nSee supplementary figure below for an illustration of how the confidence interval represents plausible sampling distributions of the *mean* response.\n\n![**Plausible Sampling Distributions for the Mean Response at $\\bar{X}$.** Each curve represents a possible sampling distribution of $\\hat{Y}(\\bar{X})$ if the true population mean were at that location. The central blue distribution corresponds to the sample estimate, while the green curves show other plausible means consistent with the 95% confidence interval. The confidence interval for the mean spans the horizontal segment beneath the distributions.](images/fch09_CI_mean_distributions.png)\n\n\n## Regression for Calibration (Inverse Prediction)\n\nRather than predicting $y$ for a given $x$, **calibration** estimates the $x$ value that would produce a desired $y$ (i.e., solve for $x$ when $y = y_0$).\n\n**Prediction equation**:\n\n$$\n\\text{Pred}\\{Y \\mid X_0\\} = \\hat{Y}_{\\text{ind} \\mid X_0} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_0\n$$\n\n**Calibration equation** (solving for $X_0$ given $y_0$):\n\n$$\n\\hat{X} = \\frac{y_0 - \\hat{\\beta}_0}{\\hat{\\beta}_1}\n$$\n\n### Confidence Interval for the Mean (Calibration Target)\n\nGiven a desired outcome (e.g., a grade of $y = 75$), we can estimate the value of $X$, the number of study hours that would produce it:\n\n$$\n\\hat{X} = \\frac{75 - 40.99}{6.71} = 5.07\n$$\n\nTo construct a confidence interval for the mean $X$, we treat this as an inverse prediction problem and quantify the uncertainty in the estimate of $\\hat{X}$.\n\n**Standard error of the calibration estimate**:\n\n$$\nSE(\\hat{X}) = \\frac{SE\\left( \\hat{Y}_{\\text{mean} \\mid X_0} \\right)}{|\\hat{\\beta}_1|}\n$$\n\nUsing:\n\n- $n = 13$, $\\bar{X} = 3.92$, $S_X^2 = 5.74$\n- $\\hat{\\sigma} = 11.65$ (from MSE $= 135.62$)\n- $\\hat{X} = 5.07$\n- $\\hat{\\beta}_1 = 6.71$\n\nCompute $SE(\\hat{X})$:\n\n$$\nSE(\\hat{X}) = \\frac{11.65}{|6.71|} \\sqrt{ \\frac{1}{13} + \\frac{(5.07 - 3.92)^2}{(13 - 1) \\cdot 5.74} } = \\frac{11.65}{|6.71|} \\cdot \\sqrt{0.0889} = 0.538\n$$\n\n**95% Confidence Interval for $\\hat{X}$**:\n\nUsing $t_{0.975, n-2} \\approx 2.02$:\n\n$$\n\\text{95% CI} = \\hat{X} \\pm t_{0.975, n-2} \\cdot SE(\\hat{X}) = 5.07 \\pm 2.02 \\cdot 0.538 = (3.983,\\ 6.157)\n$$\n\nInterpretation:\n\n> We are 95% confident that the number of study hours a student would need to earn a mean grade of 75 is in the interval (3.983, 6.157) hours.\n\n### Prediction Interval for an Individual $X$\n\nSometimes we want to predict the $X$ value associated with **a single observation** that yields a specific $y$.\n\nWe still estimate:\n\n$$\n\\hat{X} = 5.07\n$$\n\nBut the standard error includes an extra term for individual-level variation:\n\n$$\nSE\\left( \\hat{Y}_{\\text{ind} \\mid X_0} \\right) = \\hat{\\sigma} \\sqrt{ 1 + \\frac{1}{n} + \\frac{(X_0 - \\bar{X})^2}{(n - 1) S_X^2} }\n$$\n\nThen:\n\n$$\nSE(\\hat{X}) = \\frac{SE\\left( \\hat{Y}_{\\text{ind} \\mid X_0} \\right)}{|\\hat{\\beta}_1|}\n$$\nThis yields a wider interval than the confidence interval for the mean, because it reflects both model error and individual-level error.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}