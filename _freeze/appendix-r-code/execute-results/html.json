{
  "hash": "5bb48d263822a751d36db0348c419e4e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Appendix A: R Code Examples for Statistical Foundations\"\nappendix: true\nformat: html\nexecute:\n  eval: false\n---\n\n\n\n\n\nQuick Navigation:\n\n- [1. Data Import & Summary Statistics](#data-import-summary)  \n- [2. Visualization & Assumption Checks](#viz-assumption-checks)  \n- [3. Permutation Test](#permutation-test) \n- [4. t-Tests & Power Analysis](#t-tests-power) \n- [5. One-Way ANOVA & Extra Sum of Squares](#anova-extra-ss)  \n- [6. Multiple Comparisons](#multiple-comparisons)  \n- [7. Contrasts](#contrasts)  \n- [8. Non-Parametric Tests](#nonparam-tests)  \n- [9. Correlation & Simple Linear Regression](#correlation-simple-regression)  \n- [10. Residual Analysis](#residual-analysis)  \n- [11. Log-Log Models & Back-Transformation](#log-log-models)  \n- [12. Multiple Linear Regression (MLR)](#multiple-regression)  \n- [13. Variable Selection](#variable-selection)  \n\n::: {.appendix}\n\n## 1. Data Import & Summary Statistics {#data-import-summary}\n\n### 1A. Import and Explore the Dataset\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the tidyverse (for read, dplyr, ggplot2, etc.)\nlibrary(tidyverse)\n\n# Import the dataset using a file dialog\ndataset = read.csv(file.choose(), header = TRUE, stringsAsFactors = TRUE)\n\n# Explore the structure and contents\nstr(dataset)\nhead(dataset)\n```\n:::\n\n\n\n\n\n### 1B. Summary Statistics by Group\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Summarize response by levels of the explanatory variable\ndataset %>% group_by(explanatory) %>% \n  summarize(n = n(), mean = mean(response), sd = sd(response))\n```\n:::\n\n\n\n\n\n\n## 2. Visualization & Assumption Checks {#viz-assumption-checks}\n\n### 2A. Base R Plots (One Variable or Group Subsets)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visual checks for distributional assumptions (normality and spread)\n\n# Histogram, QQ plot, and Boxplot of a numerical variable\nhist(dataset$numericalColumn, main = \"Histogram\", xlab = \"Numerical Variable\")\nqqnorm(dataset$numericalColumn, main = \"QQ Plot\")\nqqline(dataset$numericalColumn)\nboxplot(dataset$numericalColumn, horizontal = TRUE, main = \"Boxplot\")\n```\n:::\n\n\n\n\n\n### 2B. Base R: Comparison Between Groups\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare two levels of a grouping variable using base R\n\n# Replace level1 and level2 with actual factor levels\nlevel1 = \"A\"\nlevel2 = \"B\"\n\n# Layout: 2 rows, 3 columns\npar(mfrow = c(2, 3))\n\n# Level 1 plots\nhist(subset(dataset, explanatory == level1)$response, main = paste(\"Histogram -\", level1), xlab = \"Response\")\nboxplot(subset(dataset, explanatory == level1)$response, main = paste(\"Boxplot -\", level1), horizontal = TRUE)\nqqnorm(subset(dataset, explanatory == level1)$response, main = paste(\"QQ Plot -\", level1))\nqqline(subset(dataset, explanatory == level1)$response)\n\n# Level 2 plots\nhist(subset(dataset, explanatory == level2)$response, main = paste(\"Histogram -\", level2), xlab = \"Response\")\nboxplot(subset(dataset, explanatory == level2)$response, main = paste(\"Boxplot -\", level2), horizontal = TRUE)\nqqnorm(subset(dataset, explanatory == level2)$response, main = paste(\"QQ Plot -\", level2))\nqqline(subset(dataset, explanatory == level2)$response)\n```\n:::\n\n\n\n\n\n### 2C. ggplot2 + patchwork for Grouped Assumption Plots\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Import ggplot if tidyverse not already imported\n# library(ggplot2) # in tidyverse package\n\n# Create histograms, QQ plots, and boxplots by group\n# Use facet_wrap() to show each level of explanatory separately\n\n# Patchwork allows multi-plot arrangement\nlibrary(patchwork)\n\n# Histogram faceted by group\nhist = dataset %>% \n  ggplot(aes(x = response)) +\n  geom_histogram(bins = 15) + \n  # facet_wrap(~explanatory, scales = \"free_y\") +\n  facet_wrap(~explanatory) +\n  ggtitle(\"Histogram of Response by Group\") +\n  theme_bw()\n\n# QQ plot per group\nqq = dataset %>%\n  ggplot(aes(sample = response)) +\n  geom_qq() +\n  facet_wrap(~explanatory) +\n  ggtitle(\"QQ Plots of Response by Group\") +\n  theme_bw()\n\n# Boxplot per group\nbox = dataset %>% \n  ggplot(aes(y = response, x = explanatory)) +\n  geom_boxplot() +\n  ggtitle(\"Boxplot of Response by Group\") +\n  theme_bw()\n\n# Combine plots using patchwork (arrange histogram above qqplot, next to boxplot)\n(hist / qq) | box\n```\n:::\n\n\n\n\n\n\n## 3. Permutation Test: Generate Permutation Distribution for Group Mean Difference {permutation-test}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Step 1: Calculate the observed difference in sample means\nxbars = dataset %>% group_by(explanatory) %>% summarize(mean = mean(response))\nxbarGrp1minusGrp2 = xbars[2,2] - xbars[1,2] # observed difference\nxbarGrp1minusGrp2\n\n# Step 2: Build the permutation distribution under the null with this loop\n# Make sure nGrp1 and nGrp2 are defined as sample sizes of the groups\nxbarDiffHolder = numeric(10000)\n\nfor (i in 1:10000){\n  scrambledLabels = sample(dataset$explanatory, nGrp1+nGrp2); # shuffle labels\n  \n  datasetTemp = dataset\n  datasetTemp$explanatory = scrambledLabels\n  \n  xbars = datasetTemp %>% group_by(explanatory) %>% summarize(mean = mean(response))\n  xbarGrp1minusGrp2 = xbars[2,2] - xbars[1,2] # observed difference\n  xbarGrp1minusGrp2\n  xbarDiffHolder[i] = xbarGrp1minusGrp2$mean\n}\n\n# Step 3: Plot the permutation distribution\ndf = data.frame(xbarDiffs = xbarDiffHolder)\n\ndf %>% ggplot(mapping = aes(x = xbarDiffs)) + \n  geom_histogram(bins = 25, fill = \"cornflowerblue\", linewidth = 0.1) +\n  ggtitle(\"Permutation Distribution of the Difference of Sample Means\") +\n  xlab(\"xbarGrp1 - xbarGrp2\")\n\n# Step 4: Calculate the p-value (two-tailed)\nnum_more_extreme = sum((abs(xbarDiffHolder)) >= abs(xbarGrp1minusGrp2))\n\npvalue = num_more_extreme / 10000\npvalue\n```\n:::\n\n\n\n\n\n\n## 4. $t$-Tests and Power Analysis {#t-tests-power}\n\n### 4A. Basic $t$-Tests\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Critical value for a two-sided t-test (95% CI)\n# Use df = n - 1 (one-sample) or df = n1 + n2 - 2 (two-sample)\nqt(0.975, df - 2)\n\n# One-sample t-test\nt.test(x=dataset, mu = underTheNull, conf.int = \"TRUE\", alternative = \"two.sided\")\n\n# Paired t-test (one-sample, df-1, within-subject comparison)\nt.test(x = dataset$explantoryGrp2, y = dataset$explantoryGrp1, paired = TRUE) # this is probably easiest\n# Or alternative formula syntax\nt.test(response ~ explanatory, data = dataset, paired = TRUE) # alternative = \"two-sided\", mu = 0, conf.level = 0.95, var.equal(doesn't apply, one-sample)\n# Check the order of the treatment groups\nlevels(dataset$explanatory)\n\n# Two-sample t-test with equal variance\nresults = t.test(response ~ explanatory, data = dataset, var.equal = TRUE, alternative = \"two.sided\")\nresults\n\n# Two-sample t-test with unequal variance (Welch's)\nresults = t.test(response ~ explanatory, data = dataset, var.equal = FALSE, alternative = \"two.sided\")\nresults\n\n# Extract test statistic and degrees of freedom\nresults = t.test(response ~ explanatory, data = dataset)\ntstat = results$statistic\ndf = results$parameter\n\n# Manual two-sided p-value\npt(abs(tstat), df,lower.tail = FALSE) * 2\n```\n:::\n\n\n\n\n\n### 4B. Power Analysis\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute the power of a t-test (one-sample or two-sample)\npower.t.test(n = nPerGrp, delta = effectSize, sd = sd, sig.level = alpha, power = NULL, type = \"one.sample\", alternative = \"one.sided\")\n# or \"two.sample\", can also just leave power (or unknown variable off)\n\n# Find sample size to get 80% power (leave n blank)\npower.t.test(n = , delta = effectSize, power = .8, sd = s, sig.level = .05, type = \"one.sample\", alternative = \"one.sided\")\n\n# Unequal sample sizes using Cohen’s d (if you have different sample sizes in each of two samples)\nlibrary(pwr)\npwr.t2n.test(n1 = n1, n2 = n2, d = effectSize/stdev, sig.level = 0.05, alternative = \"two.sided\") # alternative = \"greater\"\n\n# Power calculation with unequal standard deviations (Welch’s)\npower.welch.t.test(n = n, delta = effectSize, power = , sd1 = s1, sd2 = s2, alternative = \"two-sided\")\n```\n:::\n\n\n\n\n\n### 4C. Power Curve\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create power curve across a range of sample sizes\nsamplesizes = seq(nlower, nhigher, by = 1)\npowerholder = numeric(length(samplesizes))\n\nfor(i in 1:length(samplesizes))\n{\n  powerholder[i] = power.t.test(n = samplesizes[i], delta = effectSize, sd = s, sig.level = .05, type = \"one.sample\", alternative = \"one.sided\")$power\n}\n\n# Combine into a data frame\npowerdf = data.frame(samplesizes, powerholder)\n\n# Plot power vs sample size to create the power curve\npowerdf %>% ggplot(aes(x = samplesizes, y = powerholder)) +\n  geom_line(color = \"blue3\", linewidth = 1.5) +\n  ggtitle(\"Power Curve\") +\n  ylab(\"Power\") +\n  xlab(\"Sample Sizes\") +\n  ylim(0.75, 1.0) +\n  theme_bw()\n```\n:::\n\n\n\n\n\n\n## 5. One-Way ANOVA and Extra Sum of Squares {#anova-extra-ss}\n\n### 5A. One-Way ANOVA\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit a one-way ANOVA model (make sure groups is a factor variable)\nfit = aov(response ~ groups, data = dataset)\nsummary(fit)\n\n# Find the critical value for the F-distribution (one-sided test)\n# critical_value = qf(alpha, dfn, dfd, lower.tail = FALSE)\nqf(0.05, dfn, dfd, lower.tail = FALSE)\n```\n:::\n\n\n\n\n\n### 5B. Extra Sum of Squares Test\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare a full model to a reduced model using extra sum of squares\n# Example: comparing CTRL and D and testing whether they can be combined\n\n# To confirm at lease two groups are different: one-way ANOVA first (make sure groups is a factor variable)\nfit = aov(response ~ groups, data = dataset)\nsummary(fit)\n\n# Full model: all group levels (e.g., CTRL, A, B, C, D, E)\nfit_full = aov(response ~ explanatory, data = dataset)\nsum_fit_full = summary(fit_full)\nsum_fit_full\n# Extract the degrees of freedom for the error\ndfd = sum_fit_full[[1]][\"Residuals\", \"Df\"]\n# Extract the sum of squares for the error\nssFull = sum_fit_full[[1]][\"Residuals\", \"Sum Sq\"]\n\n# Reduced model: O, O, A, B, C, E (combine CTRL & D)\nfit_reduce = aov(response ~ explanatoryReduced, data = dataset)\nsum_fit_reduce = summary(fit_reduce)\nsum_fit_reduce\n# Extract the degrees of freedom for the error\ndfTotal = sum_fit_reduce[[1]][\"Residuals\", \"Df\"]\n# Extract the sum of squares for the error\nssRed = sum_fit_reduce[[1]][\"Residuals\", \"Sum Sq\"]\n\n#F-Statistic and P-Value for Model Comparison\n# BYOA table calculations (F-test to compare reduced vs full model)\nalpha = 0.05\ndfn = dfTotal - dfd                     # numerator df\nssModel = ssRed - ssFull                # difference in SS\nmse = ssFull / dfd                      # mean square error\nmsModel = ssModel / dfn                 # mean square model\nfstat = msModel / mse                   # F-statistic\nfstat\n\n# P-value (one-tailed test)\np_value = pf(fstat, dfn, dfd, lower.tail = FALSE)\np_value\n\n# Critical F value\ncritical_value = qf(alpha, dfn, dfd, lower.tail = FALSE)\ncritical_value\n\n# Print results\ncat(\"alpha:\", alpha, \"\\n\")\ncat(\"dfTotal (reduced):\", dfTotal, \"\\n\")\ncat(\"dfd (full):\", dfd, \"\\n\")\ncat(\"dfn:\", dfn, \"\\n\")\ncat(\"Sum of Squares for Reduced Model:\", ssRed, \"\\n\")\ncat(\"Sum of Squares for Error (Full):\", ssFull, \"\\n\")\ncat(\"Sum of Squares for Model (SS explained):\", ssModel, \"\\n\")\ncat(\"Mean Square Error:\", mse, \"\\n\")\ncat(\"Mean Square Model (MS explained):\", msModel, \"\\n\")\ncat(\"Critical Value:\", critical_value, \"\\n\")\ncat(\"F-Statistic:\", fstat, \"\\n\")\ncat(\"p-Value F-test:\", p_value, \"\\n\")\n```\n:::\n\n\n\n\n\n\n## 6. Multiple Comparisons {#multiple-comparisons}\n\n### 6A. Tukey HSD using `multcomp` (Tukey-Kramer Adjustment)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Use multcomp to conduct pairwise comparisons with Tukey-Kramer multiple comparison correction\nlibrary(multcomp)\n\n# Fit must come from an ANOVA or linear model with a factor\ngfit = glht(fit, linfct = mcp(groups = \"Tukey\")) # 'groups' is your factor variable\n\n# Summary of pairwise tests\nsummary(gfit)\n\n# Extract confidence intervals for all pairwise comparisons\nconfint_gfit = confint(gfit)\nconfint_gfit\n```\n:::\n\n\n\n\n\n### 6B. Tukey HSD using agricolae\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tukey using agricolae\n# Different way to extract half_width\nlibrary(agricolae)\n\n# Run Tukey HSD test\ntukey_half = HSD.test(fit, 'groups') # put groups variable in\n\n# Extract minimum significant difference (half-width of CI)\ntukey_half$statistics$MSD\n```\n:::\n\n\n\n\n\n### 6C. Tukey HSD using Base R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Alternative method using base R function TukeyHSD()\n# Note: This function works correctly on models fit with aov()\n# (I haven't used this before, so confirm it is the correct function.)\n\n# This will also give you the adjusted CIs and p-values\ntukey_result = TukeyHSD(fit)\ntukey_result\n```\n:::\n\n\n\n\n\n\n## 7. Contrasts {#contrasts}\n\n> Only use planned contrasts **after** a significant ANOVA result.\n\n### 7A. Setup and Specifying Contrasts\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Contrast of the average of the mean of 2 groups of 2\n\nlibrary(emmeans)\n\n# Check the order of factor levels - for assigning contrast coefficients\nunique(dataset$groups)\n\n# Fit linear model with categorical explanatory variable\nfit = lm(response ~ groups, data = dataset)\n\n# Get least-squares means for each group\nleastsquare = lsmeans(fit, \"groups\") # put groups variable in\n\n# Define contrast weights: compare (A+B) vs (C+D)\nContrasts = list(Grp1and2vsGrp3and4 = c(.5, .5, -.5, -.5))\n```\n:::\n\n\n\n\n\n### 7B. Run Contrast Tests\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# With adjustment\ncontrastResultsCorr = contrast(leastsquare, Contrasts, adjust = \"sidak\") # slightly less conservative than bonferroni\nsum_contrastResultsCorr = summary(contrastResultsCorr)\nsum_contrastResultsCorr\n\n# Without adjustment\ncontrastResults = contrast(leastsquare, Contrasts) # no adjustment\nsum_contrastResults = summary(contrastResults)\nsum_contrastResults\n```\n:::\n\n\n\n\n\n### 7C. Manual Confidence Interval from Estimate\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Find critical value for 95% CI (df from ANOVA)\ncriticalVal = qt(0.975, df)\ncriticalVal\n\n# CI = estimate ± criticalValue*SE (estimate is the difference of means from the original t-test)\n# Estimate ± margin of error\nestimate = sum_contrastResultsCorr$estimate\nSE = sum_contrastResultsCorr$SE\n\nCI_lower = estimate - criticalVal * SE\nCI_upper = estimate + criticalVal * SE\n\nCI_lower\nCI_upper\n```\n:::\n\n\n\n\n\n\n## 8. Non-Parametric Tests {#nonparam-tests}\n\n### 8A. Wilcoxon Rank-Sum Test (Mann-Whitney)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare two independent samples\n\n# Get the EXACT p-value (one-sided test)\nwilcox.test(response ~ explanatory, data = dataset, alternative = \"less\", exact = TRUE)\n# Get the exact matching CI (note it is not alpha, it is conf.level)\n# ‘conf.int’ option provides the HL confidence limits (like SAS)\nwilcox.test(response ~ explanatory, data = dataset, alternative = \"two.sided\", exact = TRUE, conf.level = 0.90, conf.int = TRUE)\n\n# Get the NORMAL APPROXIMATION p-value (with continuity correction)\nwilcox.test(response ~ explanatory, data = dataset, alternative = \"less\", exact = FALSE, correct = TRUE)\n# Get the normal approximation matching CI\nwilcox.test(response ~ explanatory, data = dataset, alternative = \"two.sided\", exact = FALSE, correct = TRUE, conf.level = 0.90, conf.int = TRUE)\n```\n:::\n\n\n\n\n\n### 8B. Signed-Rank Test (Paired Samples)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get critical Z value for one-sided test\ncritVal = qnorm(0.95)\ncritVal\n\n# Run the paired test\nsignedRank = wilcox.test(dataset$before, dataset$after, paired = TRUE, alternative = \"greater\", exact = FALSE, correct = TRUE)\nsignedRank\n\n# Get the 90% matching CI\nsignedRankCI = wilcox.test(dataset$before, dataset$after, paired = TRUE,alternative = \"two.sided\", exact = FALSE, correct = TRUE, conf.level = 0.90, conf.int = TRUE)\nsignedRankCI\n```\n:::\n\n\n\n\n\n### 8C. Manual Z-Approximation for Signed-Rank\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# This is extra code for the by-hand calculations.\n# Extract the test statistic (S)\nS = signedRank$statistic\nS\n\n# Calculate sample size (n)\nn = length(dataset$before)\nn\n\n# Calculate the expected value (mean) of S under the null hypothesis\nmean_S = n * (n + 1) / 4\nmean_S\n\n# Calculate the standard deviation of S under the null hypothesis\nsd_S = sqrt(n * (n + 1) * (2 * n + 1) / 24)\nsd_S\n\n# Calculate the Z-statistic with continuity correction\nCC = ifelse(S > meanS, -0.5, 0.5)\nCC\nZ = (S + CC - mean_S) / sd_S\nZ\n\n# Get the p-value for a one-tailed test (upper tail)\np_value_one_tailed = 1 - pnorm(Z)\np_value_one_tailed\n```\n:::\n\n\n\n\n\n\n## 9. Correlation and Simple Linear Regression {#correlation-simple-regression}\n\n### 9A. Pearson Correlation\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a sample dataset\ndataset = data.frame(response = c(1, 2, 3, 4, 5), \n                     explanatory = c(1, 2, 3, 4, 5))\n\n# Make a scatter plot to visualize the relationship\nplot(dataset$explanatory, dataset$response, \n     xlab = \"Explanatory\", ylab = \"Response\", \n     main = \"Scatterplot of Response vs. Explanatory\", pch = 15)\n\n# Alternatively, without making a dataframe\nplot(response, explanatory)\n\n# Get Pearson correlation coefficient (three ways)\ncor(dataset) # returns correlation matrix\ncor(dataset$response, dataset$explanatory)\ncor(x = explanatory, y = response)\n\n# Hypothesis test for correlation (t-test)\n# Get r, the test statistic, p-value, and confidence interval\ncor.test(dataset$response, dataset$explanatory)\n\n# Get the critical value\nqt(0.975,n-2) # df = n-2, for 95% two-sided test\n```\n:::\n\n\n\n\n\n### 9B. Simple Linear Regression: Fitting and Diagnostics\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the linear model\nfit = lm(response ~ explanatory, data = dataset)\nsummary(fit)  # Shows coefficients, t-tests, R-squared\n\n# Plot data and fitted line (base R)\nplot(dataset$explanatory, dataset$response, \n     xlab = \"Explanatory\", ylab = \"Response\", \n     main = \"Linear Regression\", pch = 15)\nlines(dataset$explanatory, fit$fitted.values, col = \"blue\")\n\n# ANOVA for regression model\nanova(fit)\n\n\n# --- Residual Diagnostics ---\n\n# Default residual plots (base R)\nplot(fit)  # Residuals vs Fitted, QQ Plot, etc.\n\n# Save residuals and fitted values for custom plots\ndataset$residuals = residuals(fit)\ndataset$fittedVals = fitted(fit)\n\n# ggplot2 residuals vs fitted plot\ndataset %>% \n  ggplot(aes(x = fittedVals, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ggtitle(\"Residuals vs Fitted Values\") +\n  theme_bw()\n\n# QQ plot of residuals (base R)\nresiduals = residuals(fit)\nqqnorm(residuals)\nqqline(residuals, col = \"black\")\n```\n:::\n\n\n\n\n\n### 9C. Confidence & Prediction Intervals\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# --- Confidence Intervals for Coefficients ---\n\nfit = lm(response ~ explanatory, data = dataset)\n\n# Show summary\nsummary(fit)\n\n# Confidence interval for the parameter estimates (intercept and slope)\nconfint(fit)\n\n# CI for just the slope\nconfint(fit, \"explanatory\")\n\n# CI at different confidence level\nconfint(fit, level = 0.99) # to change alpha\n\n\n# --- Confidence & Prediction for New Observations ---\n\n# Add a new data point\nnew_data = data.frame(response = NA, explanatory = 2.5)\n# or\nnew_data = data.frame(explanatory = 2.5)\n\n# Confidence interval (mean response at x)\n# 95% CI estimating the mean value of y expected at value of x\npredictionCI = predict(fit, newdata = new_data, interval = \"confidence\")\npredictionCI\n\n# Prediction interval (individual response at x)\n# 95% CI estimating the individual value of y expected at value of x, more error\npredictionPI = predict(fit, newdata = new_data, interval = \"prediction\")\npredictionPI\n\n\n# --- Plot CI and PI Around Fitted Line ---\n\n# Predicted values + intervals (for the full dataset)\npredictions = predict(fit, interval = \"confidence\", level = 0.95, se.fit = TRUE)\nprediction_intervals = predict(fit, interval = \"prediction\", level = 0.95)\n\n# Add intervals to original data for plotting\nmovies = movies %>%\n  mutate(fit = predictions$fit,\n         lwr_conf = predictions$fit[, \"lwr\"],\n         upr_conf = predictions$fit[, \"upr\"],\n         lwr_pred = prediction_intervals[, \"lwr\"],\n         upr_pred = prediction_intervals[, \"upr\"])\n\n# Plot fitted line with CI and PI ribbons\nggplot(data, aes(x = explanatory, y = response)) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_ribbon(aes(ymin = lwr_conf, ymax = upr_conf), alpha = 0.5, fill = \"lightblue\") +\n  geom_line(aes(y = lwr_pred), linetype = \"dashed\", color = \"black\") +\n  geom_line(aes(y = upr_pred), linetype = \"dashed\", color = \"black\") +\n  geom_hline(yintercept = 210, color = \"darkred\") +\n  geom_point() +\n  labs(title = \"Regression Line with 95% Confidence and Prediction Intervals\",\n       x = \"Explanatory\", y = \"Response\") +\n  theme_bw()\n\n\n# --- Calibration Intervals (Reverse Prediction) ---\n\n# For estimating values of x from given y\nlibrary(investr)\n\n# Calibration interval for the mean budget (Estimate x for a given y0, mean response)\ncalibrate(fit,y0 = 210, interval = \"Wald\", mean.response = TRUE, limit = FALSE)\n\n# Calibration interval for a single movie budget (estimate x for an individual response)\ncalibrate(fit,y0 = 210, interval = \"Wald\", mean.response = FALSE, limit = FALSE)\n\n\n# --- R-squared Interpretation ---\n# R-squared = measure of the proportion of variation in the response that is accounted for by the explanatory variable\n\n# Get the summary of the model\nsummary_fit = summary(fit)\n\n# Extract R-squared\nR_squared = summary_fit$r.squared\n\n# Interpretation\ncat(\"The R-squared value is\", R_squared, \"\\n\")\ncat(\"This means that\", round(R_squared * 100, 2), \"% of the variability in Response is explained by Explanatory.\")\n```\n:::\n\n\n\n\n\n\n## 10. Residual Analysis {#residual-analysis}\n\n### 10A. Fit Model and Add Residuals\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit a linear model\nfit = lm(response ~ explanatory, data = dataset)\n\n# Log transform the data if needed\ndataset = dataset %>% mutate(\n  log_explanatory = log(explanatory),\n  log_response = log(response)\n)\n\n# Add residuals and fitted values to the dataframe\ndataset$residuals = residuals(fit)\ndataset$fittedVals = fitted(fit)\n\n# Studentized residuals (internal)\nlibrary(car)  # for rstudent(), studentized residuals\ndataset$studentized_residuals = rstudent(fit) \n\n# Another way to get studentized residuals (studentized deleted residuals)\n# External: excluding the data point being tested. More accurate for identifying outliers or influential points, because the data point doesn't bias its own standard error.\nlibrary(MASS)\n# Calculate studentized residuals (external studentized: ti=ei/σhati*sqrt(1−hii))\ndataset$StudentizedResiduals = rstudent(fit)\n```\n:::\n\n\n\n\n\n### 10B. Residual Plots\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Base R: full residual diagnostics (residuals vs fitted, QQ, sqrt(std residuals), Cook’s)\nplot(fit)\n\n# ggplot: residuals vs fitted scatterplot\nggplot(data = dataset, aes(x = fittedVals, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"blue\") +\n  labs(x = \"Fitted Values\", y = \"Residuals\", title = \"Scatterplot of Residuals\") + \n  theme_bw()\n```\n:::\n\n\n\n\n\n### 10C. Normality Checks for Residuals\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Base R: QQ plot of residuals\nqqnorm(residuals(fit), main = \"QQ Plot of Residuals\")\nqqline(residuals(fit), col = \"blue\")\n\n# or using residuals saved to dataset\nqqnorm(dataset$residuals, main = \"QQ Plot of Residuals\")\nqqline(dataset$residuals, col = \"blue\")\n\n# car package: enhanced QQ plot\nlibrary(car)\nqqPlot(dataset$residuals, main = \"QQ Plot (car::qqPlot)\")\n\n# ggplot2 QQ plot\nggplot(dataset, aes(sample = residuals)) +\n  stat_qq() +\n  stat_qq_line(col = \"blue\") +\n  labs(title = \"QQ Plot of Residuals\", x = \"Theoretical Quantiles\", y = \"Sample Quantiles\") + \n  theme_bw()\n```\n:::\n\n\n\n\n\n### 10D. Histogram of Residuals\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ggplot2 histogram with normal curve\nggplot(data = dataset, aes(x = residuals)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 15, fill = \"lightblue\", color = \"gray30\") +\n  stat_function(fun = dnorm, args = list(mean = mean(dataset$residuals), sd = sd(dataset$residuals)), color = \"blue\") +\n  labs(x = \"Residuals\", y = \"Density\", title = \"Histogram of Residuals with Normal Curve\") + \n  theme_bw()\n\n# Base R version with overlaid normal curve\nhist(residuals(fit), breaks = 15, probability = TRUE, col = \"lightblue\", border = \"gray30\", main = \"Histogram of Residuals with Normal Curve\", xlab = \"Residuals\")\ncurve(dnorm(x, mean = mean(residuals(fit)), sd = sd(residuals(fit))), col = \"blue\", lwd = 2, add = TRUE)\n```\n:::\n\n\n\n\n\n### 10E. Regression Line with Confidence & Prediction Intervals\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Base R scatterplot and regression line\nplot(dataset$explanatory, dataset$response, \n     xlab = \"Explanatory\", ylab = \"Response\", \n     main = \"Linear Regression of Response & Explanatory\", pch = 16)\n# Add the regression line\nabline(fit, col = \"blue\", lwd = 2)\n\n# ggplot2 scatterplot + regression line\nggplot(dataset, aes(x = explanatory, y = response)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(x = \"Explanatory\", y = \"Response\", \n       title = \"Linear Regression of Response & Explanatory\") +\n  theme_bw()\n\n\n# --- Optional: Add Intervals and Highlighted Points ---\n\n# Identify specific data points to highlight\ndataToHighlight = dataset %>%\n  filter(columnName == \"Value\") %>% \n  select(\"col1\", \"col2\", \"col3\")\ndataToHighlight\n\n# Build prediction data frame (example using log-log model structure)\nnew_data = data.frame(logGDP = InfantVGDP_clean$logGDP, logInfantMort = InfantVGDP_clean$logInfantMort)\n\n# Generate confidence and prediction intervals using log-log model fit\nconfInt = predict(fitLogLog, newdata = new_data, interval = \"confidence\")\npredInt = predict(fitLogLog, newdata = new_data, interval = \"predict\")\n\n# Add intervals to new_data\nnew_data$fit = confInt[, \"fit\"]\nnew_data$lwr_conf = confInt[, \"lwr\"]\nnew_data$upr_conf = confInt[, \"upr\"]\nnew_data$lwr_pred = predInt[, \"lwr\"]\nnew_data$upr_pred = predInt[, \"upr\"]\n\n# Plot CI and PI with highlighted point(s)\nggplot(dataset, aes(x = explanatory, y = response)) +\n  # Prediction interval ribbon (widest)\n  geom_ribbon(data = new_data, aes(ymin = lwr_pred, ymax = upr_pred), alpha = 0.3, fill = \"gray70\") +\n  # Confidence interval ribbon (narrower)\n  geom_ribbon(data = new_data, aes(ymin = lwr_conf, ymax = upr_conf), alpha = 0.6, fill = \"lightblue\") +\n  # Raw data points\n  geom_point() +\n  # Highlighted point(s)\n  geom_point(data = dataToHighlight, aes(x = explanatory, y = response), color = \"red3\", size = 4, stroke = 1.25, shape = 21) +\n  # Regression line\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(x = \"Explanatory\", y = \"Response\", \n       title = \"Linear Regression with 95% CI, PI, and Highlighted Points\") +\n  theme_bw()\n```\n:::\n\n\n\n\n\n\n## 11. Log-Log Models & Back-Transformation {#log-log-models}\n\n### 11A. Back-Transform the Slope and Interpret\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract slope from log-log model\nintercept = coef(fitLogLog)[1]\nslope = coef(fitLogLog)[2]\n\n# Get confidence interval for slope\nconf_intervals = confint(fit)\nslope_conf_lower = conf_intervals[\"log_explanatory\", 1]\nslope_conf_upper = conf_intervals[\"log_explanatory\", 2]\n\n# Back-transform the slope and its CI\nback_transformed_slope = 2^slope\nback_transformed_conf_lower = 2^slope_conf_lower\nback_transformed_conf_upper = 2^slope_conf_upper\n\n# Percentage change interpretation\npercentage_change = (1 - back_transformed_slope) * 100\npercentage_change_conf_lower = (1 - back_transformed_conf_lower) * 100\npercentage_change_conf_upper = (1 - back_transformed_conf_upper) * 100\n\n# Display results\ncat(\"Slope:\", slope, \"\\n\")\ncat(\"Back-transformed Slope (2^b1):\", back_transformed_slope, \"\\n\")\ncat(\"Percentage Change:\", percentage_change, \"%\\n\")\ncat(\"CI for Slope: (\", slope_conf_lower, \", \", slope_conf_upper, \")\\n\")\ncat(\"Back-transformed CI for Slope: (\", back_transformed_conf_lower, \", \", back_transformed_conf_upper, \")\\n\")\ncat(\"Percentage Change CI: (\", percentage_change_conf_lower, \"%, \", percentage_change_conf_upper, \"%)\\n\")\n```\n:::\n\n\n\n\n\n### 11B. Back-Transform the Intercept\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get CI for intercept\nintercept_conf_lower = conf_intervals[\"(Intercept)\", 1]\nintercept_conf_upper = conf_intervals[\"(Intercept)\", 2]\n\n# Back-transform using exponential\nback_transformed_intercept = exp(intercept)\nback_transformed_intercept_conf_lower = exp(intercept_conf_lower)\nback_transformed_intercept_conf_upper = exp(intercept_conf_upper)\n\n# Display results\ncat(\"Intercept:\", intercept, \"\\n\")\ncat(\"Back-transformed Intercept (exp(b0)):\", back_transformed_intercept, \"\\n\")\ncat(\"CI for Intercept: (\", intercept_conf_lower, \", \", intercept_conf_upper, \")\\n\")\ncat(\"Back-transformed CI: (\", back_transformed_intercept_conf_lower, \", \", back_transformed_intercept_conf_upper, \")\\n\\n\")\n```\n:::\n\n\n\n\n\n### 11C. Compare Predicted vs Observed Values\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimate log(Y) and then back-transform to original scale\nest_log_response = intercept + slope * dataset$explanatory\nest_response = exp(est_log_response)\n\n# Display for comparison\ncat(\"Estimated log(response):\", est_log_response, \"\\n\")\ncat(\"Estimated response:\", est_response, \"\\n\")\ncat(\"Actual log(response):\", dataset$obs_log_response, \"\\n\")\ncat(\"Actual response:\", dataset$obs_response, \"\\n\")\n```\n:::\n\n\n\n\n\n### 11D. Lack of Fit Test\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extra sum of squares F-test (lack of fit)\n# H0: Linear regression fits well (no lack of fit)\n# HA: Separate means model fits better (LRM is lacking fit)\n\n# Critical value\nqf(1-alpha, dfn, dfd)\n\n# Find p-value for f-distribution\npf(fstat, dfn, dfd, lower.tail = FALSE)\n\n# Template interpretation:\n# \"There is [overwhelming/sufficient/insufficient] evidence at the alpha = 0.05 level of significance to suggest the linear regression model has a lack of fit compared to the separate-means model (p- value = XYZ from an extra-sum-of-squares F-test).\"\n```\n:::\n\n\n\n\n\n\n## 12. Multiple Linear Regression (MLR) {#multiple-regression}\n\n### 12A. MLR with Only Numeric Predictors (Same Slopes)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Subset to just the numerical columns of interest\nsubset_scores = scores[ , c(\"science\", \"math\", \"read\")]\n\n# Visualize relationships: matrix scatterplot\nplot(subset_scores,\n     main = \"Matrix Scatterplot\", \n     pch = 19, # point character\n     col = \"darkblue\")\n\n# Use GGally:GGpairs to make a matrix scatterplot\nlibrary(GGally)\nggpairs(subset_scores,\n        title = \"Matrix Scatterplot\")\n\n# Fit the multiple linear regression model\nfit = lm(science ~ math + read, data = scores)\nsummary(fit)      # Includes t-tests, R², etc.\nconfint(fit)      # Confidence intervals for coefficients\n\n# Plot residuals to check assumptions (normality, linearity, equal variance)\nplot(fit)\n```\n:::\n\n\n\n\n\n### 12B. MLR with Categorical Predictors (Indicator Variables, Same Slopes)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize response by numeric and categorical explanatory variables\ndataset %>% \n  ggplot(aes(x = explanatoryNum, y = response, color = explanatoryCat)) +\n  geom_point() +\n  labs(title = \"Response vs Numeric and Categorical Predictors\",\n       x = \"Explanatory (Numeric)\",\n       y = \"Response\",\n       color = \"Explanatory (Category)\") +\n  theme_bw()\n\n# Convert category variable to a factor if needed\nscores$ses = as.factor(scores$ses)\n\n# Fit the model, set reference level if necessary\nfit = lm(response ~ explanatoryNum + relevel(explanatoryCat, ref = \"3\"), data = dataset)\nsummary(fit)\nconfint(fit)\n\n# View the covariance matrix (advanced)\nvcov(fit)\n\n# Check assumptions with residual plots\nplot(fit)\n```\n:::\n\n\n\n\n\n### 12C. MLR with Interaction (Different Slopes per Group)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Include interaction term between numeric and categorical variables\nfit = lm(response ~ explanatoryNum * relevel(factor(explanatoryCat), ref = \"3\"), data = dataset) # can convert to factor here instead\nsummary(fit)\nconfint(fit)\n\n# Equivalent fully expanded form\nfit2 = lm(response ~ \n            explanatoryNum + \n            relevel(factor(explanatoryCat), ref = \"3\") + \n            explanatoryNum*relevel(factor(explanatoryCat), ref = \"3\"), \n          data = dataset)\nsummary(fit2)\nconfint(fit2)\n\n# Plot residuals to check model fit\nplot(fit)\n```\n:::\n\n\n\n\n\n\n## 13. Variable Selection {#variable-selection}\n\n### 13A. Stepwise Selection by $p$-Value\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Use olsrr for stepwise selection based on p-values (significance level)\n# install.packages(\"olsrr\")\nlibrary(olsrr)\n\n# Full model with all predictors\nfit = lm(response ~ ., data = dataset)\n\n# Forward selection (add variables one at a time)\na = ols_step_forward_p(fit, p_val = 0.15, details = TRUE)\n\n# Backward elimination (start with full model, remove variables)\nb = ols_step_backward_p(fit, p_val = 0.15, details = TRUE) # bigger p-remove, more variables in the model; smaller p-remove, fewer\n\n# Stepwise (both directions)\nc = ols_step_both_p(fit, p_enter = 0.15, p_remove = 0.15, details = TRUE)\n```\n:::\n\n\n\n\n\n### 13B. Forward Selection by AIC\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Refit full model\nfit = lm(response ~ ., data = dataset)\n\n# Forward selection using AIC as criterion\nd = ols_step_forward_aic(fit, details = TRUE)\n```\n:::\n\n\n\n\n\n### 13C. Cross-Validation and Interaction Terms\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit model with all two-way interactions\nfit = lm(response ~ .^2, data = dataset)\n\n# Use caret for model tuning with LOOCV\nlibrary(caret)\n\n# Define training control method\ntrain_control = trainControl(method=\"LOOCV\")\n\n# Train linear model with specified predictors\nmodel = train(response ~ explanatory1 + explanatory2 + explanatory3 + explanatory4 + explanatory5, data = dataset, trControl = train_control, method = \"lm\")\n\n# Output cross-validated model results\nmodel\n```\n:::\n\n\n\n\n\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}