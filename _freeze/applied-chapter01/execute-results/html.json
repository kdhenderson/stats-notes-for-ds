{
  "hash": "fa55b2fdf83a7ed940bf9614b9db5e93",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Multiple Linear Regression (MLR) Revisited\"\n---\n\n\n\n\n## Objectives\n\n- Understand regression problems and distinguish them from classification problems.\n- Identify when to use explanatory versus predictive modeling.\n- Compare parametric and nonparametric regression models and their trade-offs.\n- Review multiple linear regression (MLR) and its applications.\n- Evaluate key assumptions of MLR and correctly interpret model coefficients.\n- Explore techniques to increase model complexity when needed.\n- Detect and address multicollinearity in regression models.\n\n\n## Key Terms\n\n- The **response variable** (dependent variable) is what we aim to explain or predict.  \n- **Explanatory variables** (independent variables) are the factors that may influence the response.\n- **Regression**: The response is a continuous numeric variable (e.g., predicting car mileage).  \n- **Classification**: The response variable is categorical and can have two (binary) or more than two levels (e.g., predicting where a car was made: North America, Asia, or Europe).\n\n\n### Example: Regression Problem - MPG Dataset  \nA common regression problem is predicting a car's miles per gallon (MPG) using various features:\n\n- Response Variable: `mpg` (miles per gallon)\n- Explanatory Variables: `cylinders`, `horsepower`, `origin` (3 levels), `year`, etc.\n\n### Regression Modeling Workflow\nWhen analyzing a regression problem, we typically follow these steps:\n\n1. Exploratory Data Analysis (EDA) \n   - Visualize relationships between the response and each explanatory variable.\n   - Identify potential trends, outliers, and transformations.\n\n2. Key Questions to Consider  \n   - Which variables explain `mpg`, and how strong are these relationships?  \n   - Are there specific hypotheses to test (e.g., does a car's origin significantly impact `mpg`)?  \n   - Can we use this model to predict the `mpg` of a new car?\n\n\n## Regression Problems\n\nA regression problem occurs when the response variable is continuous. The goal can either be **explanation** (understanding relationships) or **prediction** (making accurate future estimates).\n\n### To Explain or To Predict?\n\n1. Explaining Relationships  \n   - Hypothesis testing and confidence intervals  \n   - Identifying relationships between response and predictors  \n   - Adjusting for confounding (lurking) variables\n\n2. Predicting Future Outcomes  \n   - Focused on accuracy, not interpretation  \n   - More complex models can be used\n\nTo determine the appropriate approach, ask what is the primary goal—explanation, prediction, or both?\n\n\n## Parametric vs. Nonparametric Regression Tools\n\n### General Regression Model Structure\n\nThe regression model follows:\n\n$$\nY = f(X) + \\varepsilon\n$$\nwhere: \n\n- $X = (x_1, x_2, ..., x_p)$ represents the explanatory variables.  \n- $f(X)$ is the deterministic component.  \n- $\\varepsilon$ is the random error.  \n\n### Key Terminology\n- Population function: $f$ (true but unknown function in the population).  \n- Sample function: $\\hat{f}$ (estimate of $f$ using observed data).  \n\n### Estimating $f$: Parametric vs. Nonparametric Approaches  \n\nThere are two primary approaches for estimating $f$:  \n\n1. Parametric  \n2. Nonparametric  \n\nThe choice depends on how the response and predictor variables relate mathematically.  \n\nFor example:  \n\n  - A numeric predictor like horsepower may follow a quadratic function.  \n  - A categorical predictor like cylinders may follow a piecewise function.  \n\n### Parametric Approach\n- Requires specifying the functional form of $f(X)$.  \n- Assumes additional conditions about the error term.  \n- Allows hypothesis testing.  \n- Works well when the model is correctly specified.  \n- Simpler models can be more interpretable.  \n\n### Nonparametric Approach\n- No need to specify $f(X)$.  \n- Fully data-driven and flexible.  \n- Can capture complex relationships.  \n- More difficult to interpret.  \n- May require larger datasets for reliable estimation.  \n- Does not support traditional hypothesis testing frameworks.  \n\n### Tradeoffs: Interpretability vs. Flexibility\n\n| Approach       | Pros | Cons |\n|---------------|------|------|\n| Parametric    | Interpretable  <br> Allows hypothesis testing  <br> Works well for small datasets  <br> Easier for high $p$, low $n$ settings | Cannot model complex relationships well  <br> Sensitive to misspecification |\n| Nonparametric | Adapts to complex patterns  <br> Less restrictive assumptions  <br> Often performs better for large datasets | Requires large datasets  <br> No hypothesis testing  <br> Harder to interpret |\n\n### Challenges with Parametric Models\n1. Specifying highly complex forms of $f(X)$ is difficult.  \n2. For large datasets, nonparametric models often perform as well or better.  \n3. Additional diagnostic checks are required, such as:  \n\n   - Checking model assumptions (linearity, normality, independence).  \n   - Addressing multicollinearity and influential points.  \n\n\n## A Review of Multiple Linear Regression (MLR)\n\n### Model Structure\n\n$$\nY = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p + \\varepsilon\n$$\n\nwhere:  \n\n- $Y$ is the response variable.  \n- $X_1, X_2, ..., X_p$ are the explanatory variables.  \n- $\\varepsilon$ is the random error term.\n\n### Assumptions of MLR\n- **Linearity**: The relationship between predictors and response is linear.  \n- **Independence**: Errors are independent.  \n- **Homoscedasticity**: Errors have constant variance.  \n- **Normality**: Errors follow a normal distribution. \n  \n### Residual Diagnostic Checks  \nThese checks help assess whether assumptions hold:  \n\n- **Residual vs. Fitted Plot** – Examines constant variance (homoscedasticity).  \n- **Q-Q Plot and/or Histogram** – Assesses whether residuals follow a normal distribution.  \n- **Variance Inflation Factor (VIF)** – Identifies multicollinearity among predictors.  \n- **Data Collection Process Review** – Ensures independence.  \n- Violations often occur in repeated measures on the same subject or in time series data.  \n\n### Basic Interpretations\nWhen each predictor is included in the model only once, interpretations follow these general rules:  \n\n- **Continuous predictors**: A one-unit increase in $X$ results in a change of $\\beta$ in $Y$, holding all other variables constant. \n- **Categorical variables**: Represented using dummy variables. \n\nExample: Suppose $X_1$ is categorical with three levels (A, B, C), while $X_2$ and $X_3$ are continuous. The model:  \n\n$$\nY = \\beta_0 + \\beta_1\\text{LevelB} + \\beta_2\\text{LevelC} + \\beta_3X_2 + \\beta_4X_3\n$$\n\n- $\\beta_1$: Difference in mean response between group B and reference group A, holding other variables constant.  \n- $\\beta_2$: Difference in mean response between group C and reference group A, holding other variables constant.  \n- The reference group A is not specifically listed in the model.\n\n**Why \"holding all other variables constant\" is important**  \nA key advantage of MLR is its ability to **control for confounding variables**.  \n\nExample: **Sex discrimination in pay**  \n\n- If we suspect a gender-based wage gap, we should control for factors like position level, education, and job role.  \n- A more precise interpretation:  \n  *\"A male with the same IQ and education level (instead of generically 'holding all other variables constant') is estimated to earn $28,463 more than a female counterpart.\"* \n\n\n## Adding Model Complexity\n\n### Transformations\nWhen to transform variables:  \n\n- **Non-constant variance** → Transform $Y$.\n- **Nonlinear trends in residual plots** → Transform $X$ or add polynomial or interaction terms.\n\n### Pros and Cons of Transformations  \n- Helps satisfy model assumptions.  \n- Allows reliable statistical inference on regression coefficients.  \n- Log transformations retain interpretability.  \n- More complex transformations may hinder interpretation (less of a concern for predictive models).  \n\nExample: Polynomial regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(y ~ poly(x1, 3), data=mydata) # poly(predictor, degree)\n```\n:::\n\n\n\n\n### Ways to Increase Model Complexity\n- **Modeling nonlinear trends**: Introducing higher-order terms (with multiple coefficients) for a single predictor (e.g., $X^2$ or $X^3$).\n- **Multiple linear trends by category**: Using interaction terms to model relationships (between multiple predictors) that vary by group.\n- **Adding predictors one at a time**: Results in an additive model where the intercept changes, but the slope remains the same.\n- **Adding interactions**: Results in a non-additive model where both the intercept and slope change depending on another variable.\n\n\n## Interactions in MLR\n\nIf $X_1$ is numeric and $X_2$ is categorical with two levels:\n\n- **Reference group** (where $X_2=0$): $Y = \\beta_0 + \\beta_1 X_1$\n- **Non-reference group** (where $X_2=1$): $Y = (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3)X_1$\n\n\n## Hypothesis Testing for Interactions\n\nTo test for the significance of an interaction term and generate a confidence interval around the non-reference category, we use a contrast test:\n\n$$\nH_0: c_0\\beta_0 + c_1\\beta_1 + c_2\\beta_2 + c_3\\beta_3 = 0\n$$\n\n\n## Multicollinearity\nMulticollinearity occurs when two or more explanatory variables are highly correlated, leading to:\n\n- Difficulty in holding other variables fixed.\n- Increased uncertainty in coefficient estimates (wider intervals, larger *p*-values).\n- Drastic changes in conclusions when variables are added or removed.\n\n### Variance Inflation Factor (VIF)\nVIF measures how much a predictor is correlated with other predictors:\n\n- Higher correlation between predictors results in a higher VIF and larger standard errors ($SE(\\beta)$).\n- $SE(\\beta)$ is a function of the relationship between the variability of the response, predictors, and the VIF.\n- **VIF $\\approx$ 1**: No collinearity.\n- **$5 < \\text{VIF} < 10$**: Mild collinearity, investigate further.\n- **VIF > 10**: Severe collinearity, investigate adjustments.\n\nHigh VIFs are not always concerning.\n\n- Expected with polynomial terms and interaction terms.\n- Common for categorical variables with with more than two levels.\n- If interpreting only one coefficient, and it has a low VIF, multicollinearity may not be an issue (i.e., if you are just accounting for the other variables).\n\n  \n## Addressing Multicollinearity\n\n### Scenario 1: Predefined Research Questions\n- Start with a hypothesis and a plan to test it.\n- Fit model and check assumptions.\n- Check for multicollinearity using VIFs and graphics. Consider secondary analysis if multicollinearity is a concern.\n- Report findings including and excluding variables with multicollinearity concerns.\n- Possible solutions:\n  - Aggregate correlated variables into a single variable.\n  - Use data reduction strategies such as Principal Component Analysis (PCA).\n\n### Scenario 2: Exploratory Model Building\n- Used when the goal is an interpretable model, but no predefined hypothesis exists.\n- Identify key predictors.\n- Use model selection techniques to minimize collinearity.\n\n### Scenario 3: Predictive Modeling\n- Multicollinearity is *not* a concern when the goal is pure prediction.\n- It only affects coefficient standard errors (*SE*) and the hypothesis testing framework.\n- The focus is on model performance, not coefficient interpretation.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}