{
  "hash": "eacbcb8cf0689a89206ebbe91d0b71be",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Repeated Measures\"\n---\n\n\n\n\n## Objectives\n\nThis chapter explores repeated measures, which allow us to relax the independence assumption.\n\n- Understand when repeated measures are appropriate.  \n- Learn about correlation structures, how to visualize them, and select the appropriate structure.  \n- Explore generalized least squares (GLS) and weighted least squares (WLS).  \n- Follow a repeated measures workflow to structure analysis.  \n  \n  \n## What is Repeated Measures?  \n\n### Family Tree of Linear Models\n\n::: {}\n\n\n\n```{mermaid}\n\nflowchart TD\n  A[\"Linear models<br>(constant variance,<br>normality assumptions)\"] --> B[\"Errors are<br>independent\"]\n  B --> D[\"*t*-tests\"]\n  B --> E[\"ANOVA\"]\n  B --> F[\"SLR\"]\n  A --> C[\"Errors are<br>correlated\"]\n  C --> G[\"Time series\"]\n  C --> H[\"Repeated measures\"]\n\n  %% Arrow styling\n  linkStyle default stroke:#333,stroke-width:1.5px;\n\n  %% Node styling\n  style A fill:#d1c4e9,stroke:#512da8,stroke-width:1.25px,color:#000\n  style B fill:#bbdefb,stroke:#1976d2,stroke-width:1.25px,color:#000\n  style C fill:#ffcdd2,stroke:#c62828,stroke-width:1.25px,color:#000\n\n  style D fill:#e3f2fd,stroke:#64b5f6,stroke-width:1.25px,color:#000\n  style E fill:#e3f2fd,stroke:#64b5f6,stroke-width:1.25px,color:#000\n  style F fill:#e3f2fd,stroke:#64b5f6,stroke-width:1.25px,color:#000\n\n  style G fill:#ffebee,stroke:#ef5350,stroke-width:1.25px,color:#000\n  style H fill:#ffebee,stroke:#ef5350,stroke-width:1.25px,color:#000\n\n```\n\n\n\n:::\n\n\n## Repeated Measures\n\n- Linear models assume constant variance and normality with independent errors.\n  - Examples: *t*-tests, ANOVA, simple linear regression (SLR) \n- When errors are correlated:\n  - Examples: time series, repeated measures  \n  - Ignoring correlation can produce misleading *p*-values and confidence intervals.  \n  - We need to model the correlation among errors (i.e., include additional parameters in the model) for valid inference and better predictions. \n- Repeated measures refers to multiple measurements on the same subject (i.e., dependent observations).\n  - For example, if a subject starts with high (or conversely low) values, their measurements tend to remain high (or low) over time.  \n\n### Common Hypotheses in Repeated Measures:\n\n- Compare every time point to a baseline (i.e., control), for each group (e.g., asymptomatic vs. symptomatic).\n- Compare each time point between groups. (Baseline may not differ, but other time points might.)\n\n### Regression Perspective\n\nTreat time as a categorical variable because measurements are made at discrete time points:\n\n$$\nY = \\text{Time} + \\text{Status} + \\text{Time} \\times \\text{Status} + \\varepsilon\n$$\n\n- Include an interaction since trends depend on status.  \n- Regression and contrasts help compare groups---we just need to model correlated errors.\n\n### Identifying Repeated Measures\n\n- Ask how the data were collected.  \n- Repeated measures can apply to:\n  - Repeated measures one-way ANOVA\n  - Repeated measures MLR\n  \n\n## Correlation Structures\n\n### Assessing Error Dependence\n\n- Some methods include residual diagnostic tools to visualize correlated errors.\n- Similar tools exist for repeated measures, but they aren't as commonly used.\n- The key idea is to explain how residuals are correlated using a correlation structure.\n\n#### Correlogram (pseudo-code approach)\n\n1. Obtain residuals from a regular MLR (some from the same subject).\n2. For residuals that are one unit apart in time:\n   - Create a scatterplot (earlier time point on the $x$-axis, later time point on the $y$-axis).\n   - Compute the correlation, and store the result.\n3. Repeat for residuals that are two, three, four, $\\dots, k$ units apart.\n4. Plot the correlation values against the time lag.\n\n#### Takeaway \nCorrelation tends to decrease as the distance between residuals increases and eventually levels off. Large correlations at short time lags are a clear indication of correlated errors.\n  \n### Correlogram Interpretation\n\n- Observations closer in time are more similar (i.e., more correlated).\n- Observations farther apart in time may still be mildly correlated.\n\n### Common Correlation Structures\n\nCorrelation structures are theoretical models that describe the expected trend in the correlogram.\n\n#### Compound symmetry (CS)\n- Correlation is constant (i.e., flat, horizontal line) regardless of time.\n- Often used when time ordering is not meaningful (e.g., students within the same school).\n\n#### Autoregressive (AR(1))\n- High correlation for nearby time points.\n- Correlation decreases as time lag increases.\n- Eventually approaches zero (like independent errors).\n\n#### Gaussian\n- Similar to AR(1), but the drop-off in correlation is even faster.\n- Often used when time is continuous.\n  \n### Generalized Least Squares (GLS)\n- GLS generalizes OLS technique for MLR to handle correlated errors.\n- You specify:\n  - A model with response and predictors\n  - A correlation structure (parameters estimated by software)\n- GLS updates both:\n  - Regression coefficients\n  - Standard errors\n- These estimates are more reliable than standard MLR if the chosen correlation structure is approximately correct.\n\n### Estimating Correlation in Practice\n- Variograms and semivariograms (often used in spatial models) can be used as visual alternatives to correlograms.\n- In practice, repeated measures datasets may be too small to visualize correlation clearly through the high variability.\n- Analysts typically:\n  - Use theoretical justification for choosing a structure.\n  - Fit multiple structures and compare using AIC (like feature selection).\n  - Use correlograms as a visual guide.\n- Instructor note (Turner’s experience):  \n  - Compound symmetry works well for biological or human-based data.  \n  - Use structures with rapid decay (e.g., Gaussian) sparingly unless time points are equally spaced and numerous.  \n\n   \n## Repeated Measures Workflow\n\n1. Identify that you are in a repeated measures setting.\n2. Perform EDA based on the equivalent MLR framework (determine the general theme: ANOVA, SLR, or MLR):\n   - Use boxplots or mean plots in ANOVA-style settings.\n   - Use scatterplots of predictor vs. response in SLR-style settings.\n   - Combine approaches for general MLR settings.\n   - Apply feature selection tools from MLR to assess model complexity and the bias-variance trade-off.\n3. Residual diagnostics:\n   - Check for normality and constant variance using residual plots.\n4. Update MLR to GLS:\n   - Try multiple correlation structures.\n   - Use AIC to select the best-fitting model.\n   - Optionally use a correlogram to guide your choice.\n5. Conduct inference on regression coefficients:\n   - Report *p*-values and confidence intervals.\n   - Results will be more trustworthy when correlation is accounted for.\n   \n\n## Additional Notes on Correlation Structures\n\n- Some correlation structures **require a time component**:\n  - Autoregressive (AR; exponential decay)\n  - Gaussian\n  - Linear\n  - Spherical\n  - Matérn (A general class that includes AR and Gaussian as special cases; more flexible but may overfit.)\n- Some correlation structures **do not require a time component**:\n  - Compound symmetry (CS)\n  - Variance components\n- Examples that don't involve time:\n  - Texas STAR exam: multiple test scores from the same schools/school districts\n  - Hereditary studies: family studies where siblings or parents/offspring are clustered\n\n\n## Technical Details\n\n### Matrix Representation of MLR\n\n$$\nY = X\\beta + \\varepsilon\n$$\n\nwhere:\n\n- $Y$ is the response vector.\n- $X$ is the design matrix.\n- $\\beta$ is the regression coefficient vector.\n- $\\varepsilon$ is the error vector.\n\nThe model assumes properties of the errors:\n\n$$\n\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)\n$$\n\nthat is:\n\n- $\\varepsilon$ is multivariate normal\n- where $\\mathcal{N}$ denotes the multivariate normal distribution (MVN)\n- Mean vector = $0$, a $n x 1$ matrix of $0$s\n- Covariance matrix = $\\sigma^2 I$\n\nThe covariance matrix:\n\n$$\n\\Sigma = \\begin{bmatrix}\n\\sigma^2 & 0 & 0 \\\\\n0 & \\sigma^2 & 0 \\\\\n0 & 0 & \\sigma^2 \\\\\n\\end{bmatrix} = \\sigma^2I\n$$\n\nInterpretation:\n\n- Each individual error has the same variance $\\sigma^2$\n- Traditional multiple linear regression (MLR) assumptions:\n  - Errors are independent, which implies they are also uncorrelated, so all off-diagonal covariances are 0.\n\n### Generalized Least Squares (GLS) and Correlated Errors\nIn repeated measures models, we no longer assume that errors are uncorrelated.\n\nTo account for correlated errors, we extend the standard MLR assumptions using generalized least squares (GLS). This involves specifying a more flexible error structure.\n\nWe now assume:\n\n$$\n\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 R)\n$$\n\nwhere:\n\n- $\\varepsilon$ is multivariate normal with mean zero.\n- $\\sigma^2 R$ is the **variance–covariance matrix** of the errors.\n- $R$ is the **correlation matrix**, which specifies how errors are related (e.g., through time, subject grouping, etc.).\n- When $R = I$, we recover the standard MLR case with independent errors.\n\nThis updated formulation allows us to model within-subject correlation (e.g., repeated observations) by directly encoding the assumed pattern of correlation into $R$.\n\n### Visualizing the Correlation Structure in $\\sigma^2 R$\nWe can visualize the error term in repeated measures models as:\n\n$$\nY = X\\beta + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 R)\n$$\n\nEach row in $\\varepsilon$ corresponds to a specific subject and time point:\n\n| Subject | Time | Error Term |\n|---------|------|------------|\n| 1       | T1   | $\\varepsilon_1$ |\n| 1       | T2   | $\\varepsilon_2$ |\n| 1       | T3   | $\\varepsilon_3$ |\n| 2       | T1   | $\\varepsilon_4$ |\n| 2       | T2   | $\\varepsilon_5$ |\n| 2       | T3   | $\\varepsilon_6$ |\n\nThe corresponding variance–covariance matrix $\\sigma^2 R$ has a block-diagonal structure and might look like this:\n\n$$\n\\sigma^2 R = \\sigma^2 \\begin{bmatrix}\n1 & \\rho_{12} & \\rho_{13} & 0 & 0 & 0 \\\\\n\\rho_{12} & 1 & \\rho_{23} & 0 & 0 & 0 \\\\\n\\rho_{13} & \\rho_{23} & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & \\rho_{45} & \\rho_{46} \\\\\n0 & 0 & 0 & \\rho_{45} & 1 & \\rho_{56} \\\\\n0 & 0 & 0 & \\rho_{46} & \\rho_{56} & 1 \\\\\n\\end{bmatrix}\n$$\n\nKey points:\n\n- The upper-left 3×3 block corresponds to Subject 1 and correlations among $\\varepsilon_1$, $\\varepsilon_2$, $\\varepsilon_3$.\n- The lower-right 3×3 block corresponds to Subject 2 and correlations among $\\varepsilon_4$, $\\varepsilon_5$, $\\varepsilon_6$\n- **Off-diagonal correlations exist within subjects** (e.g., $\\rho_{12}, \\rho_{13}, \\rho_{23}$ for subject 1).\n- **Between-subject correlations are zero**, indicated by the block-diagonal structure.\n- The values of the $\\rho$ terms are determined by the assumed **correlation structure** (e.g., compound symmetry, AR(1), etc.).\n\nThis block structure is how repeated measures models encode dependency within subjects but maintain independence between subjects.\n\n### Correlation Structure Matrices\nDifferent correlation structures determine the form of the matrix $R$ in the repeated measures model.\n\n#### Autoregressive (AR(1))\nThis structure assumes correlation decreases with increasing time lag:\n\n$$\n\\sigma^2 \\begin{bmatrix}\n1 & \\rho & \\rho^2 & \\rho^3 \\\\\n\\rho & 1 & \\rho & \\rho^2 \\\\\n\\rho^2 & \\rho & 1 & \\rho \\\\\n\\rho^3 & \\rho^2 & \\rho & 1 \\\\\n\\end{bmatrix}\n$$\n\n- High correlation for adjacent time points\n- Correlation decays exponentially with time separation\n\n#### Compound Symmetry (CS)\nThis structure assumes constant correlation across all time points:\n\n$$\n\\sigma^2 \\begin{bmatrix}\n1 & \\rho & \\rho & \\rho \\\\\n\\rho & 1 & \\rho & \\rho \\\\\n\\rho & \\rho & 1 & \\rho \\\\\n\\rho & \\rho & \\rho & 1 \\\\\n\\end{bmatrix}\n$$\n\n- Often used when time ordering is less meaningful or measurements are equally spaced\n- Simpler structure, fewer parameters\n\n#### Gaussian\n- Similar to AR(1), but correlation drops off more quickly\n- Often used when time is continuous or spacing between time points varies\n\n#### Spherical Power\n- Same mathematical form as AR(1), just different context or terminology\n- Useful when modeling spatial or continuous time-based correlation\n\n### Model Fitting\nFitting repeated measures models typically uses restricted maximum likelihood (REML), a technique that improves estimation of variance components.\n\nDuring model fitting, the software estimates:\n\n- $\\sigma^2 R$: the variance–covariance matrix of the errors\n- $\\beta$: the regression coefficient vector\n\nThe estimate of $\\beta$ under GLS is:\n\n$$\n\\hat{\\beta} = \\left(X^T R^{-1} X\\right)^{-1} X^T R^{-1} Y\n$$\n\n- This formula generalizes the OLS estimator\n- When $R = I$, it reduces to the standard OLS solution\n\n### Variance of the GLS Estimator\nIn addition to estimating the coefficients $\\hat{\\beta}$, we can also estimate their variances using the expression:\n\n$$\n\\widehat{\\text{Var}}(\\hat{\\beta}) = \\hat{\\sigma}^2 \\left( X^T R^{-1} X \\right)^{-1}\n$$\n\nThis is the generalized version of the variance formula used in OLS.\n\n- When $R = I$, this reduces to the traditional multiple linear regression (MLR) variance estimator:\n\n$$\n\\widehat{\\text{Var}}(\\hat{\\beta}) = \\hat{\\sigma}^2 \\left( X^T X \\right)^{-1}\n$$\n\nThus, the MLR estimator is a special case of GLS where the correlation structure is identity (i.e., no correlation among errors).\n\n### Summary: MLR as a Special Case of GLS\nThe general linear model framework used in GLS relaxes the independence assumption of traditional MLR and allows for arbitrary correlation structures among errors (via the matrix $R$).\n\n- MLR is a special case of GLS when the correlation structure is $R = I$.\n- The structure of $R$ determines how residuals are related (e.g., repeated measurements, clustered observations).\n- Under GLS, we estimate:\n  - Regression coefficients $\\hat{\\beta}$\n  - Standard errors of $\\hat{\\beta}$\n  - The correlation structure (via estimated parameters in $R$)\n\nKey idea: Accounting for correlation leads to valid statistical inference and improved prediction.\n\n\n## Weighted Least Squares (WLS)\n\nGLS can also be applied when independence holds, but the assumption of constant variance is violated (i.e., heteroscedasticity).\n\nIn this case, the **error covariance matrix** no longer looks like $\\sigma^2 I$. Instead, each observation has its own variance:\n\n$$\n\\Sigma = \\begin{bmatrix}\n\\sigma_1^2 & 0 & 0 \\\\\n0 & \\sigma_2^2 & 0 \\\\\n0 & 0 & \\sigma_3^2 \\\\\n\\end{bmatrix}\n$$\n\n- Each variance term $\\sigma_i^2$ is unique.\n- Independence still holds (covariances are 0), but the variances are not constant across observations.\n\nWe can re-express each unique variance in terms of a common variance component and a weight:\n\n$$\n\\sigma_1^2 = \\frac{1}{w_1} \\sigma^2,\\quad\n\\sigma_2^2 = \\frac{1}{w_2} \\sigma^2,\\quad\n\\sigma_3^2 = \\frac{1}{w_3} \\sigma^2\n$$\n\nEach $w_i$ is a **weight term** that serves as a multiplicative factor determining how different the variance of each observation is from the baseline $\\sigma^2$.\n\n### Weighted Error Structure in Matrix Form\nUsing the weights, the variance–covariance matrix of the errors can be written as:\n\n$$\n\\Sigma = \n\\begin{bmatrix}\n\\sigma_1^2 & 0 & 0 \\\\\n0 & \\sigma_2^2 & 0 \\\\\n0 & 0 & \\sigma_3^2 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\frac{\\sigma^2}{w_1} & 0 & 0 \\\\\n0 & \\frac{\\sigma^2}{w_2} & 0 \\\\\n0 & 0 & \\frac{\\sigma^2}{w_3} \\\\\n\\end{bmatrix}\n=\n\\sigma^2\n\\begin{bmatrix}\n\\frac{1}{w_1} & 0 & 0 \\\\\n0 & \\frac{1}{w_2} & 0 \\\\\n0 & 0 & \\frac{1}{w_3} \\\\\n\\end{bmatrix}\n= \\sigma^2 R\n$$\n\n- This is a special case of GLS where the correlation matrix $R$ is diagonal.\n- WLS is GLS with unequal variances but uncorrelated errors.\n\nWe can use GLS techniques to obtain good estimates of the regression coefficients and their standard errors under this structure.\n\n### Estimating Weights in Practice\nYou can only apply WLS if you know (or can estimate) the weights. There are numerous strategies for estimating the weights, but the basic approach follows a common pattern:\n\n1. Fit a regular regression model.\n2. Plot the absolute values of your residuals against:\n   - One of your predictors that you suspect may explain the changing variance, or  \n   - The predicted values from your model  \n   Look for a pattern or trend in the spread of the residuals.\n3. Fit a regression model to the absolute value of the residuals to estimate the mean behavior (i.e., trend) of the variability.\n4. Use the predicted values from that model to compute weights:  \n   $$\n   w_i = \\frac{1}{(\\widehat{\\text{predicted}})^2}\n   $$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweight.model = lm(abs(model1$residuals) ~ data1$x)\nwts = 1 / fitted(weight.model)^2\nwls.model = lm(y ~ x, data1, weights = wts)\n```\n:::\n\n\n\n\n### Interpretation\n- Observations with higher variance are down-weighted.\n- The fitted line will more closely follow observations with lower variance.\n- The biggest difference is in the standard error estimates of the regression coefficients.\n\n### When to Use WLS\n- When transformations do not fix the constant variance problem.\n- When an exotic transformation works but makes the model hard to interpret.\n\n### Additional Notes\n- Raw residual vs. fitted plots may look the same for OLS and WLS.\n- Use studentized residuals to check whether the variance has been stabilized.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}