{
  "hash": "887f51e6d31392418aafe94e66893b29",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Matrices\"\n---\n\n\n\n\n## Objectives\n\nThis chapter explores fundamental matrix concepts, including operations, applications in summarizing data, and their role in multiple linear regression.\n\n- Review fundamental matrix operations and properties.  \n- Understand special cases of matrix multiplication.  \n- Apply matrix concepts to summarize multivariate data (e.g., multivariate normal distribution).  \n- Explore how matrix algebra supports multiple linear regression (MLR).\n\n\n## Review of Matrix Operations\n\n### Symmetric Matrices  \n- A symmetric matrix has the same number of rows and columns (i.e, a square matrix).  \n\n### Vectors  \n- A vector is a matrix with a single column.  \n- Variables in vector format: $X = (X_1, X_2, X_3, X_4)$\n\n### Transposing Matrices  \n- The transpose of a matrix swaps its rows and columns.  \n- If $A$ is a matrix, then its transpose is denoted as $A'$ or $A^T$.  \n- The first column becomes the first row in the transpose.  \n\n### Matrix Addition and Subtraction  \n- Matrices must have the same dimensions for addition or subtraction.  \n- Operations are performed elementwise. \n \n### Matrix Multiplication  \n- Not all matrices can be multiplied---matrix multiplication is only defined if the number of columns in the first matrix matches the number of rows in the second matrix.  \n- If $A$ is an $m \\times n$ matrix and $B$ is an $n \\times p$ matrix, then the product $AB$ is an $m \\times p$ matrix.  \n- Each element in $AB$ is obtained by computing the dot product of a row from $A$ and a column from $B$.  \n\n#### What is a Dot Product?  \nThe dot product of two vectors is the sum of the element-wise multiplications of their corresponding entries.  \n\nFor matrix multiplication, to compute the element at row $i$, column $j$ of $AB$, we take:  \n1. Row $i$ from $A$  \n2. Column $j$ from $B$  \n3. Multiply corresponding elements and sum them:  \n   $$\n   AB_{i,j} = A_{i,1}B_{1,j} + A_{i,2}B_{2,j} + \\dots + A_{i,n}B_{n,j}\n   $$  \n \n#### Step-by-Step Example  \n\nLet:  \n$$\nA =\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}\n$$\n($2 \\times 3$ matrix)\nand  \n$$\nB =\n\\begin{bmatrix}\n7 & 8 \\\\\n9 & 10 \\\\\n11 & 12\n\\end{bmatrix}\n$$\n($3 \\times 2$ matrix)\n\nSince $A$ has **3 columns** and $B$ has **3 rows**, multiplication is valid, and the resulting matrix $AB$ has dimension $2 \\times 2$.  \n\nTo compute the element in **row 1, column 1** of $AB$:  \n$$\nAB_{1,1} = (1 \\times 7) + (2 \\times 9) + (3 \\times 11) = 7 + 18 + 33 = 58\n$$\n\nTo compute the element in **row 1, column 2**:  \n$$\nAB_{1,2} = (1 \\times 8) + (2 \\times 10) + (3 \\times 12) = 8 + 20 + 36 = 64\n$$\n\nTo compute the element in **row 2, column 1**:  \n$$\nAB_{2,1} = (4 \\times 7) + (5 \\times 9) + (6 \\times 11) = 28 + 45 + 66 = 139\n$$\n\nTo compute the element in **row 2, column 2**:  \n$$\nAB_{2,2} = (4 \\times 8) + (5 \\times 10) + (6 \\times 12) = 32 + 50 + 72 = 154\n$$\n\nThus, the final matrix product is:  \n$$\nAB =\n\\begin{bmatrix}\n58 & 64 \\\\\n139 & 154\n\\end{bmatrix}\n$$  \n\n### Identity Matrices  \n- An **identity matrix** is always symmetric, with:  \n  - All diagonal elements of **1**  \n  - All off-diagonal elements of **0** \n- The identity matrix behaves like **1** in scalar multiplication:  \n- If $A$ is an $r \\times c$ matrix and $I$ is a $c \\times c$ identity matrix, then $AI = A$.\n- If $I$ is an $r \\times r$ identity matrix, then $IA = A$.\n\n### Matrix Inverses  \n- If a matrix is square and meets certain conditions, an **inverse matrix** exists.  \n- The inverse of $A$ is denoted as $A^{-1}$.  \n- The property: $AA^{-1} = A^{-1}A = I$\n- Inverse operations are the matrix equivalent of division. \n  \n### Tips for Reading Matrix Formulas  \n- Always check the dimension of the final result when interpreting matrix expressions.  \n\n\n## Special Cases of Matrix Multiplication\n\nLet $C_{n \\times 1}$ be a column vector of chosen numbers, and let $Y_{n \\times 1}$ be a column vector representing a sample of data. The computation $C' Y$ (the transpose of $C$ multiplied by $Y$) allows us to compute:\n\n### Averages\nIf all elements of $C$ are equal to $\\frac{1}{n}$, then the matrix multiplication:\n$$\nC' Y =\n\\begin{bmatrix}\n\\frac{1}{n} & \\frac{1}{n} & \\dots & \\frac{1}{n}\n\\end{bmatrix}\n\\begin{bmatrix}\ny_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n\\end{bmatrix}\n= \\frac{1}{n} (y_1 + y_2 + \\dots + y_n) = \\frac{1}{n} \\sum_{i=1}^{n} y_i = \\bar{Y}\n$$\nThis shows that $C' Y$ computes the **sample mean** $\\bar{Y}$ using matrix multiplication.\n\n### Weighted Averages\nIf $C$ contains weights that sum to 1 and are all positive, then the matrix multiplication:\n$$\nC' Y =\n\\begin{bmatrix}\nw_1 & w_2 & \\dots & w_n\n\\end{bmatrix}\n\\begin{bmatrix}\ny_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n\\end{bmatrix}\n= w_1 y_1 + w_2 y_2 + \\dots + w_n y_n = \\sum_{i=1}^{n} w_i Y_i = \\bar{Y}_w\n$$\nThis shows that $C' Y$ computes the **weighted mean** $\\bar{Y}_w$ using matrix multiplication.\n\n### Summation\nIf all elements of $C$ are set to 1, then the matrix multiplication:\n$$\nC' Y =\n\\begin{bmatrix}\n1 & 1 & \\dots & 1\n\\end{bmatrix}\n\\begin{bmatrix}\ny_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n\\end{bmatrix}\n= y_1 + y_2 + \\dots + y_n = \\sum_{i=1}^{n} y_i\n$$\nThis shows that $C' Y$ computes the **sum** of all values in $Y$ using matrix multiplication.\n\n### Extending to Matrices\nIf $C$ is not a vector but a matrix, then multiple computations can be performed simultaneously using matrix multiplication:\n$$\nC' Y =\n\\begin{bmatrix}\n\\frac{1}{n} & \\frac{1}{n} & \\dots & \\frac{1}{n} \\\\\n1 & 1 & \\dots & 1\n\\end{bmatrix}\n\\begin{bmatrix}\ny_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\bar{Y} \\\\\n\\sum Y\n\\end{bmatrix}\n$$\nThis shows that $C' Y$ stores the mean in the first row and the sum in the second, efficiently computing both.\n\n### Sums of Squares\nIf $Y$ is multiplied by its own transpose, then the matrix multiplication:\n$$\nY' Y =\n\\begin{bmatrix}\ny_1 & y_2 & \\dots & y_n\n\\end{bmatrix}\n\\begin{bmatrix}\ny_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n\\end{bmatrix}\n= y_1^2 + y_2^2 + \\dots + y_n^2 = \\sum_{i=1}^{n} Y_i^2\n$$\nThis shows that $Y' Y$ computes the **sum of squares** of $Y$ using matrix multiplication, which is useful for variance and regression calculations.\n\n\n## Summarizing Multiple Variables\n\nThis section describes how to summarize numerical data when working with multiple variables, leading up to the **multivariate normal distribution (MVN)** and how to estimate its parameters.\n\n### One-Variable Summaries \nBasic summaries for individual variables:\n\n- Mean and standard deviation (if normally distributed)  \n- Five-number summary: min, 1st quartile, median, 3rd quartile, max \n\n### Multiple Variables  \nWhen working with multiple variables:\n\n- Compute the mean and standard deviation for each variable  \n- Some variables may be correlated  \n- Matrices provide a structured way to organize this information \n\n### Parameters for Two Variables  \nFor two variables $X_1$ and $X_2$:\n\n$X_1$  \n\n- Mean: $\\mu_1$  \n- Standard deviation: $\\sigma_1$  \n\n$X_2$  \n\n- Mean: $\\mu_2$  \n- Standard deviation: $\\sigma_2$  \n\nIn addition to individual means and standard deviations, we also measure **covariance** ($\\sigma_{12}$), which describes how the two variables vary together. Covariance is directly related to **correlation**.\n\n  \n## Matrix Representation\n\nMatrices provide a compact way to represent the relationships between multiple variables.\n\n$X_1$ and $X_2$ follow a multivariate normal distribution (MVN):  \n\n- **Mean vector**:\n  $$\n  \\mu =\n  \\begin{bmatrix}\n  \\mu_1 \\\\\n  \\mu_2\n  \\end{bmatrix}\n  $$\n- **Covariance matrix**:\n  $$\n  \\Sigma =\n  \\begin{bmatrix}\n  \\sigma_1^2 & \\sigma_{12} \\\\\n  \\sigma_{12} & \\sigma_2^2\n  \\end{bmatrix}\n  $$\n- The diagonal elements are variances; the off-diagonal elements are covariances.\n- The covariance matrix must be symmetric.\n\n## Covariance and Correlation\n\n- **Covariance** ($\\sigma_{12}$) measures how two variables move together but depends on scale.\n- **Correlation** is the standardized version of covariance.\n  $$\n  \\text{COR}(X_1,X_2) = \\frac{\\text{COV}(X_1, X_2)}{\\sigma_1 \\sigma_2}\n  $$\n- Properties:.\n  - Covariance has units, correlation does not.\n  - A covariance of zero means the variables are not linearly related.\n  - Correlation ranges from -1 to 1, making it easier to interpret.\n\n\n## Multivariate Normal Distribution (MVN)  \n\nA multivariate normal distribution (MVN) extends the normal distribution to multiple variables, describing their relationships through both their individual distributions and their dependencies. It is defined by:  \n\n- A mean vector, which gives the expected values of the variables.  \n- A covariance matrix, which describes how the variables vary together.  \n- A joint probability distribution, which specifies the likelihood of different combinations of variable values.  \n  \n### Theoretical Properties\n- Each individual variable follows a normal distribution.\n- The relationships between variables are linear.\n- Data points are denser near the mean vector.\n\n\n## Estimating Parameters from Data\n\nIn practice, we estimate MVN parameters from a sample:\n\n**Sample Mean Vector**:\n$$\n\\bar{X} =\n\\begin{bmatrix}\n\\bar{X}_1 \\\\\n\\bar{X}_2\n\\end{bmatrix}\n$$\n**Sample Covariance Matrix**:\n$$\nS =\n\\begin{bmatrix}\ns_1^2 & s_{12} \\\\\ns_{12} & s_2^2\n\\end{bmatrix}\n$$\n\nwhere:\n\n- $s_1^2$ and $s_2^2$ are **sample variances**.\n- $s_{12}$ is the **sample covariance**.\n\n   \n## Assessing Multivariate Normality \n\nTo check if data follows an MVN distribution:\n\n- Each variable should be normally distributed.  \n- Density plots should be bell-shaped.\n- QQ plots (e.g., `mqqnorm(dataset)`) should be linear with slight tail deviations.\n- Check scatterplot matrices for pairwise relationships.  \n- Start with these visualizations.\n- Patterns should be elliptical or circular, and denser in the middle.\n- Linear relationships suggest MVN, while unusual trends indicate deviations.\n- Perform hypothesis tests where the null is MVN:  \n  - Royston’s test\n  - Mardia’s test\n  - No single test is definitive \n  \n\n## Applications of MVN\n\nThe multivariate normal distribution is widely used in statistics and machine learning:\n\n- Classification:\n  - Discriminant analysis\n- Regression:\n  - Multiple linear regression (MLR) assumes MVN.\n  - Hypothesis testing in regression uses MVN properties.\n  - Repeated measures and time series analysis rely on MVN structure.\n\n\n## Multiple Linear Regression (MLR) Revisited\n\nHow does linear regression use matrix operations and MVN assumptions?\n\n### Simple Linear Regression  \n- Model: $y = \\beta_0 + \\beta_1x + \\epsilon$\n- This relationship is assumed to hold for *each observation*, meaning the model consists of $n$ equations. For $n$ observations:\n  $$\n  \\begin{aligned}\n  y_1 &= \\beta_0 + \\beta_1x_1 + \\epsilon_1 \\\\\n  &\\vdots \\\\\n  y_n &= \\beta_0 + \\beta_1x_n + \\epsilon_n\n  \\end{aligned}\n  $$\n- Assumptions:.\n  - Errors $\\epsilon_i$ are independent, normally distributed, and have constant variance.  \n  - The error terms form a vector $\\epsilon$ that follows a multivariate normal distribution.  \n- These equations can be rewritten in matrix form.\n\n### Big Picture: Computing Regression Coefficients  \nTo estimate and test coefficients in MLR:  \n\n1. Estimate coefficients using: $\\hat{\\beta} = (X^T X)^{-1} X^T Y$  \n2. Compute the covariance matrix of estimates: $\\text{Var}(\\hat{\\beta}) = \\sigma^2 (X^T X)^{-1}$  \n   - Take the square root of diagonal elements to get standard errors.\n3. Use $\\hat{\\beta}$ and standard errors to compute *t*-statistics, *p*-values. and confidence intervals. \n\n### The Matrix Advantage  \n- The matrix formula $\\hat{\\beta} = (X^T X)^{-1} X^T Y$ works regardless of the number of predictors or sample size.\n- Matrix form provides a general framework for linear regression.\n\n\n## Code Examples (in R)\n\n### Matrix Operations\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nM = matrix(c(0,1,2,3), 2, 2)  # 2x2 (r x c) matrix, filled by column\nN = matrix(c(4,3,2,1), 2, 2)\n\nt(M)        # Transpose\nM * N       # Elementwise multiplication\nM %*% N     # Matrix multiplication\nMinv = solve(M)     # Matrix inverse\nM %*% Minv          # Should return identity matrix c(1,0,0,1): confirms inverse\n```\n:::\n\n\n\n\n### Multiple Linear Regression\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predict y from x using matrix algebra\n\nx = c(1,2,3,4,5)\ny = c(6.5, 10.8, 14, 21.2, 26.8) # Response vector\n\n# Design matrix: column of 1s for intercept, then x\nbigX = cbind(rep(1, 5), x)\n\n# Estimate beta using matrix multiplication\nbeta_hat = solve(t(bigX) %*% bigX) %*% t(bigX) %*% y\nbeta_hat\n\n# Compare to lm()\nfit = lm(y ~ x)\ncoef(summary(fit))[,1]   # Estimated coefficients\nsummary(fit)\n\n\n# Estimate standard errors manually\nsigma2 = 1.261^2  # From residual standard error in model summary\ncov_beta = sigma2 * solve(t(bigX) %*% bigX)\nsqrt(diag(cov_beta))      # Standard errors\n\n# Compare to lm() standard errors\ncoef(summary(fit))[,2]\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}