---
title: "Comparisons Among Several Samples"
---

## Objectives
- ANOVA tests to test for differences in means between 3 or more groups.
- How to examine model assumption.
- Sample size necessary to achieve a particular effect size in the ANOVA.
- How to make statistical inferences for multiple samples and interpret the results in context.
- The extra sum of squares principle and using it to obtain an analysis of variance table.
- How to apply the concept of full and reduced models to test different combinations of means.
- The Kruskal - Wallis test, a non-parametric version of the ANOVA.
Random versus fixed effects.

## ANOVA: Analysis of Variance
- Used for testing the quality of means from more than two populations.
- For comparing variability within groups to variability between groups.
- Assumptions:
  - All populations have normal distributions (reasonable symmetry in samples).
  - Populations standard deviations are equal  (must be met because otherwise differences in variability could be responsible for difference in means rather than actual difference).
  - Independence within samples (automatic with random samples) and between samples.
- Null: all populations mean are equal.
- Alternative: at least one of the population means is not equal to at least one other.
- ANOVA performs the first test for differences, subsequent tests follow to find which means are different from the others.

**images (notes_6_1, notes_6_2, notes_6_3)**

ANOVA: 6-step hypothesis test
H0: m1 = m2 = m3 â€¦ (all means are equal, equal means model); HA: at least one pair is different (separate means model).
If the null is true, the data all come from the same distribution with the same mean (the grand mean).
We are testing whether the data come from the same distribution (one mean, grand mean, equal means) vs. different distributions (separate means).
Good fitting model will have smaller residuals (observed â€“ predicted). Comparing models by finding the sum of squared residuals. 
Get critical value
Comes from an F-distribution: we need degrees of freedom. There are 2 df numbers.
F-statistic measures the ratio of two variances (extra SS/ extra df / MSE = MS between / MS within = variation explained by the full model / variation left to be explained).
In R: qf(alpha, dfmodel, dferror). qf() is the quantiles from an f-distribution. R-tailed test -> 1-alpha.

ANOVA 6-step hypothesis test
Get F-statistic -> F in SAS ANOVA table
p-value
Reject or FTR
If reject H0, there is evidence that at least one pair of the group means are different (p-value from an anova).

**images (notes_6_4, notes_6_5)**

SAS code for ANOVA
* generalized (generalization is that we are going to # of groups) linear model;
proc glm data = dataSet;
class group;
model responseVar = group; * fits an anova;
run;
R code for ANOVA
# aov = analysis of variance
# numerical response ~ factor groupings
fit = aov(x ~ group, data = anovaCT)
summary(fit)
R-Squared =(ğ‘‰ğ‘ğ‘Ÿğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ¸ğ‘¥ğ‘ğ‘™ğ‘ğ‘–ğ‘›ğ‘’ğ‘‘ ğ‘ğ‘¦ ğ¹ğ‘¢ğ‘™ğ‘™ ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™)/(ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™ ğ‘‰ğ‘ğ‘Ÿğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘› )=(ğ¸ğ‘¥ğ‘¡ğ‘Ÿğ‘ ğ‘†ğ‘¢ğ‘š ğ‘œğ‘“ ğ‘†ğ‘ğ‘¢ğ‘ğ‘Ÿğ‘’ğ‘ )/(ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™ ğ‘†ğ‘¢ğ‘š ğ‘œğ‘“ ğ‘†ğ‘ğ‘¢ğ‘ğ‘Ÿğ‘’ğ‘ )
Percentage of the total variance that was explained by group membership.
It is estimated that R-squared % of the variation in the response can be explained by group membership.
R2 = coefficient of determination
R = correlation coefficient
RMSE = sqrt(MSE) = sqrt(unexplained variation)

Coefficient of Variation  = RMSE / grand (score) mean * 100
Relative standard deviation, how much error is left relative to the mean
ANOVA Assumptions and Robustness
Normality per group, though ANOVA is robust to normality assumption with a big enough sample size. dependent on the magnitude of violation.
Equal standard deviation. This is very important; ANOVA is not as robust even as t-tools.
Independence within and between groups is crucial.
Same visual checks as t-tools with more groups using  histograms and boxplots for normality and variance and Q-Q plots for normality. 
More about standard deviation:
The only way the confidence intervals are true is when the standard deviations are equal.
Confidence intervals are the widest when the sample with the widest standard deviation also has the largest sample size.

Welchâ€™s ANOVA
Same assumption of normality.
Useful when standard deviations are not equal.
As in the t-test, degrees of freedom are adjusted lower.
Kruskal-Wallis Test
Is a non-parametric alternative to an ANOVA on the difference of medians. 
Can be used when the assumptions arenâ€™t met for the ANOVA or Welchâ€™s ANOVA, in the case of significant deviations from normality or small sample sizes.
Good for non-symmetric distributions.
Is basically an ANOVA after transformation to ranks.
P-values are from a chi-square distribution.

Power Analysis for ANOVA â€“ SAS code
SAS: proc power;
onewayanova test-overall;
groupmeans

Random (vs. Fixed) Effects
Observations are a random sample from a large population of possible levels.
Levels of observations are meant to be representative of the population of things you could sample (even though you can/may not be sampling every possible level).
Desired inferences are on the (whole) population of levels (more than those in the study).
Statistically, to avoid increasing the probabilities of type 1 and 2 error, the model needs to include variance in the full model (residual variance, variance of each observation from the overall mean) and error associated with random effects. There is more error associated with random effects model.
Inferences in a fixed effects model are limited to the specific levels in the study.

Writing Up Results
ANOVA tells us if the between group variability is larger than the within group variability, that there is evidence that means are different.
F-test doesnâ€™t provide answer to the QOI (only tells us at least a pair of means is different, not which or how). Other tests are required.
What to include in a write-up:
What is the question/problem, treatment groups and context.
How was the data gathered, with sample sizes and treatments.
Results of assumption check, relying on the graphics.
Descriptive statistics for each group.
Effect size, statistic (F in this case), df, p-value.
Residual diagnostics, information about transformations, other tests.
Decision and interpretation of it in the context of the problem.

