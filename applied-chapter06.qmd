---
title: "Repeated Measures"
---

## Objectives

This chapter explores repeated measures, which allow us to relax the independence assumption.

- Understand when repeated measures are appropriate  
- Learn about correlation structures, how to visualize them, and select the appropriate structure  
- Explore generalized least squares (GLS) and weighted least squares (WLS)  
- Follow a repeated measures workflow to structure analysis  
  
  
## What is Repeated Measures?  

```{r out.width="60%", echo = FALSE}
knitr::include_graphics("appCh06_familyTreeOfLinearModels.png")
```

## Repeated Measures

- Linear models assume constant variance and normality with independent errors.
  - Examples: t-tests, ANOVA, simple linear regression (SLR) 
- When errors are correlated:
  - Examples: time series, repeated measures  
  - Ignoring correlation can produce misleading $p$-values and confidence intervals.  
  - We need to model the correlation among errors (i.e. include additional parameters in the model) for valid inference and better predictions. 
- Repeated measurse refers to multiple measurements on the same subject (i.e. dependent observations).
  - For example, if a subject starts with high (or conversely low) values, their measurements tend to remain high (or low) over time.  

### Common Hypotheses in Repeated Measures:

- Compare every time point to a baseline (i.e. control), for each group (e.g. asymptomatic vs. symptomatic).
- Compare each time point between groups (baseline may not differ, but other time points might)

### Regression Perspective

Treat time as a categorical variable because measurements are made at discrete time points:

$$
Y = \text{Time} + \text{Status} + \text{Time} \times \text{Status} + \varepsilon
$$

- Include an interaction since trends depend on status  
- Regression and contrasts help compare groups—we just need to model correlated errors

### Identifying Repeated Measures

- Ask how the data were collected  
- Repeated measures can apply to:
  - Repeated measures one-way ANOVA
  - Repeated measures MLR
  

## Correlation Structures

### Assessing Error Dependence

- Some methods include residual diagnostic tools to visualize correlated errors.
- Similar tools exist for repeated measures, but they aren't as commonly used.
- The key idea is to explain how residuals are correlated using a correlation structure.

### Correlogram (pseudo-code approach):

1. Obtain residuals from a regular MLR (some from the same subject).
2. For residuals that are one unit apart in time:
   - Create a scatterplot (earlier time point on the $x$-axis, later time point on the $y$-axis).
   - Compute the correlation, and store the result.
3. Repeat for residuals that are two, three, four, ..., $k$ units apart.
4. Plot the correlation values against the time lag

**Takeaway**:  
Correlation tends to decrease as the distance between residuals increases and eventually levels off. Large correlations at short time lags are a clear indication of correlated errors.
  
### Correlogram Interpretation

- Observations closer in time are more similar (i.e. more correlated).
- Observations farther apart in time may still be mildly correlated.

### Common Correlation Structures

Correlation structures are theoretical models that describe the expected trend in the correlogram:

- **Compound symmetry (CS)**:
  - Correlation is constant (i.e. flat, horizontal line), regardless of time.
  - Often used when time ordering is not meaningful (e.g. students within the same school).

- **Autoregressive (AR(1))**:
  - High correlation for nearby time points.
  - Correlation decreases as time lag increases.
  - Eventually approaches zero (like independent errors).

- **Gaussian**:
  - Similar to AR(1), but the drop-off in correlation is even faster.
  - Often used when time is continuous.
  
### Generalized Least Squares (GLS)

- GLS generalizes OLS technique for MLR to handle correlated errors.
- You specify:
  - A model with response and predictors
  - A correlation structure (parameters estimated by software)

GLS updates both:
- Regression coefficients
- Standard errors

These estimates are more reliable than standard MLR if the chosen correlation structure is approximately correct.

### Estimating Correlation in Practice

- Variograms and semivariograms (often used in spatial models) can be used as visual alternatives to correlograms.
- In practice, repeated measures datasets may be too small to visualize correlation clearly through the high variability.
- Analysts typically:
  - Use theoretical justification for choosing a structure.
  - Fit multiple structures and compare using AIC (like feature selection).
  - Use correlograms as a visual guide.

Instructor note (Turner’s experience):
- Compound symmetry works well for biological or human-based data.
- Use structures with rapid decay (e.g. Gaussian) sparingly unless time points are equally spaced and numerous.

---
    
- Repeated Measure Workflow  
  1. Identify that you are in a repeated measures setting.  
  2. Perform EDA based on whatever MLR equivalent setting you are in.  
    - Determine the general theme and conduct EDA accordingly: ANOVA/SLR/MLR...  
    - Boxplots, mean plots for ANOVA type settings 
    - Scatterplot of predictor and response for SLR  
    - Combinations of above for general MLR setting  
    - Feature selection tools from regular MLR can be used to assess bias-variance trade-off of complex models.  
    - General idea: we still want to model the trend out.  
  3. Residual diagnostics (regular MLR)  
    - Normality and constant variance checks  
  4. Update MLR to GLS  
    - Multiple correlation structures  
    - Select the best fit using AIC  
    - Can use correlogram to help decide  
  5. Conduct inference on the regression coefficients to answer the research question.  
    - p-values and confidence intervals are more trustworthy.  
    
- Other notes
  - The correlogram and some correlation structures MUST have a time component.  
  - Some repeated measures data sets may not be collected over time.  
    - State of Texas STAR exam (multiple test scores from the same schools, from the same school districts)  
    - Hereditary studies (1 family, 5 observations)  
  - Common correlation structures requiring a time component  
    - Autoregressive (exponential) and Gaussian  
    - Linear and spherical  
    - Matern (AR and Gaussian as special cases, more flexible/overfit)
  - Common correlation structures that do not require a time component 
    - Compound symmetry (CS)  
    - Variance components  

## Technical Details

```{r out.width="60%", echo = FALSE}
knitr::include_graphics("appCh06_technicalDetails_pg01.jpg")
knitr::include_graphics("appCh06_technicalDetails_pg02.jpg")
knitr::include_graphics("appCh06_technicalDetails_pg03.jpg")
knitr::include_graphics("appCh06_technicalDetails_pg04.jpg")
knitr::include_graphics("appCh06_technicalDetails_pg05.jpg")
```  

- General linear model: relaxes the independence assumption and allows for any specification of correlation between the error terms ($R$)  
- It includes multiple linear regression as a special case. You just set $R = I$.  
- Generalization term comes from the fact that MLR can be completed as a special case of general least squares (GLS).  
- The correlation structures dictate what $R$ looks like mathematically.  

## Weighted Least Squares

- GLS can be used in situations where independence is met but there is a constant variance problem.  

```{r out.width="60%", echo = FALSE}
knitr::include_graphics("appCh06_weightedLeastSquares_pg01.jpg")
knitr::include_graphics("appCh06_weightedLeastSquares_pg02.jpg")
```

- Estimating the weights  
  - We can only run the GLS procedure if we know the weights.  
  - There are numerous strategies for estimating the weights.  
  - Basic strategy follows a theme.  
  
- Common weighting strategy  
  - Fit a regular regression model.  
  - Plot the absolute values of your residuals vs.  
    - one of your predictors that explains the constant variance.  
      - Looking for a trend. Can you use the predictor to explain why the abs(residuals) are going up or down?   
    - the predicted values.  
  - Fit a regression equation to the abs(residuals) to estimate the trend of how the variability is behaving. Get the prediction of the absolute residuals and the weight is $w_i = 1/predicted^2$.  

```{r eval = FALSE}

weight.model = lm(abs(model1$residuals)~data1$x)
wts = 1/fitted(weight.model)^2
wls.model = lm(y~x, data1, weights=wts)

```

- Constant variance issues are handled by "down weighting" ovservations that have the higher variance.  
- Fitted line will more closely fit the low-variance observations.  
- Biggest difference in statistical results is the standard error estimates of the regression coefficients.  
- Excellent solution for constant variance problem when  
  - transformations won't fix it.  
  - exotic transformation works, but you can't explain the model anymore.  
- Raw residual vs. fitted plots will look the same when examining WLS vs. OLS.  
  - Studentized residual plots allow you to see the difference and assess if you stabilized the variance or not.  
