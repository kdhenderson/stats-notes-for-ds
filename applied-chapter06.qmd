---
title: "Repeated Measures"
---

## Objectives

This chapter explores repeated measures, which allow us to relax the independence assumption.

- Understand when repeated measures are appropriate  
- Learn about correlation structures, how to visualize them, and select the appropriate structure  
- Explore generalized least squares (GLS) and weighted least squares (WLS)  
- Follow a repeated measures workflow to structure analysis  
  
  
## What is Repeated Measures?  

```{r out.width="60%", echo = FALSE}
knitr::include_graphics("images/appCh06_familyTreeOfLinearModels.png")
```

## Repeated Measures

- Linear models assume constant variance and normality with independent errors.
  - Examples: t-tests, ANOVA, simple linear regression (SLR) 
- When errors are correlated:
  - Examples: time series, repeated measures  
  - Ignoring correlation can produce misleading $p$-values and confidence intervals.  
  - We need to model the correlation among errors (i.e. include additional parameters in the model) for valid inference and better predictions. 
- Repeated measurse refers to multiple measurements on the same subject (i.e. dependent observations).
  - For example, if a subject starts with high (or conversely low) values, their measurements tend to remain high (or low) over time.  

### Common Hypotheses in Repeated Measures:

- Compare every time point to a baseline (i.e. control), for each group (e.g. asymptomatic vs. symptomatic).
- Compare each time point between groups (baseline may not differ, but other time points might)

### Regression Perspective

Treat time as a categorical variable because measurements are made at discrete time points:

$$
Y = \text{Time} + \text{Status} + \text{Time} \times \text{Status} + \varepsilon
$$

- Include an interaction since trends depend on status  
- Regression and contrasts help compare groups—we just need to model correlated errors

### Identifying Repeated Measures

- Ask how the data were collected  
- Repeated measures can apply to:
  - Repeated measures one-way ANOVA
  - Repeated measures MLR
  

## Correlation Structures

### Assessing Error Dependence

- Some methods include residual diagnostic tools to visualize correlated errors.
- Similar tools exist for repeated measures, but they aren't as commonly used.
- The key idea is to explain how residuals are correlated using a correlation structure.

### Correlogram (pseudo-code approach):

1. Obtain residuals from a regular MLR (some from the same subject).
2. For residuals that are one unit apart in time:
   - Create a scatterplot (earlier time point on the $x$-axis, later time point on the $y$-axis).
   - Compute the correlation, and store the result.
3. Repeat for residuals that are two, three, four, ..., $k$ units apart.
4. Plot the correlation values against the time lag

**Takeaway**:  
Correlation tends to decrease as the distance between residuals increases and eventually levels off. Large correlations at short time lags are a clear indication of correlated errors.
  
### Correlogram Interpretation

- Observations closer in time are more similar (i.e. more correlated).
- Observations farther apart in time may still be mildly correlated.

### Common Correlation Structures

Correlation structures are theoretical models that describe the expected trend in the correlogram:

- **Compound symmetry (CS)**:
  - Correlation is constant (i.e. flat, horizontal line), regardless of time.
  - Often used when time ordering is not meaningful (e.g. students within the same school).

- **Autoregressive (AR(1))**:
  - High correlation for nearby time points.
  - Correlation decreases as time lag increases.
  - Eventually approaches zero (like independent errors).

- **Gaussian**:
  - Similar to AR(1), but the drop-off in correlation is even faster.
  - Often used when time is continuous.
  
### Generalized Least Squares (GLS)

- GLS generalizes OLS technique for MLR to handle correlated errors.
- You specify:
  - A model with response and predictors
  - A correlation structure (parameters estimated by software)

GLS updates both:
- Regression coefficients
- Standard errors

These estimates are more reliable than standard MLR if the chosen correlation structure is approximately correct.

### Estimating Correlation in Practice

- Variograms and semivariograms (often used in spatial models) can be used as visual alternatives to correlograms.
- In practice, repeated measures datasets may be too small to visualize correlation clearly through the high variability.
- Analysts typically:
  - Use theoretical justification for choosing a structure.
  - Fit multiple structures and compare using AIC (like feature selection).
  - Use correlograms as a visual guide.

Instructor note (Turner’s experience):
- Compound symmetry works well for biological or human-based data.
- Use structures with rapid decay (e.g. Gaussian) sparingly unless time points are equally spaced and numerous.

   
## Repeated Measures Workflow

1. Identify that you are in a repeated measures setting.
2. Perform EDA based on the equivalent MLR framework (determine the general theme: ANOVA, SLR, or MLR):
   - Use boxplots or mean plots in ANOVA-style settings.
   - Use scatterplots of predictor vs. response in SLR-style settings.
   - Combine approaches for general MLR settings.
   - Apply feature selection tools from MLR to assess model complexity and the bias-variance trade-off.
3. Residual diagnostics:
   - Check for normality and constant variance using residual plots.
4. Update MLR to GLS:
   - Try multiple correlation structures.
   - Use AIC to select the best-fitting model.
   - Optionally use a correlogram to guide your choice.
5. Conduct inference on regression coefficients:
   - Report $p$-values and confidence intervals.
   - Results will be more trustworthy when correlation is accounted for.
   

## Additional Notes on Correlation Structures


- Some correlation structures **require a time component**:
  - Autoregressive (AR; exponential decay)
  - Gaussian
  - Linear
  - Spherical
  - Matérn (a general class that includes AR and Gaussian as special cases; more flexible but may overfit)

- Some correlation structures **do not require a time component**:
  - Compound symmetry (CS)
  - Variance components

Examples that don't involve time:
- Texas STAR exam: multiple test scores from the same schools/school districts
- Hereditary studies: family studies where siblings or parents/offspring are clustered


## Technical Details

```{r out.width="60%", echo = FALSE}
knitr::include_graphics("images/appCh06_technicalDetails_pg01.jpg")
knitr::include_graphics("images/appCh06_technicalDetails_pg02.jpg")
knitr::include_graphics("images/appCh06_technicalDetails_pg03.jpg")
knitr::include_graphics("images/appCh06_technicalDetails_pg04.jpg")
knitr::include_graphics("images/appCh06_technicalDetails_pg05.jpg")
```  

The general linear model relaxes the independence assumption and allows for any specification of correlation among error terms (with the matrix $R$).

- This includes multiple linear regression (MLR) as a special case when the correlation structure is: $R = I$

- The generalization comes from recognizing that MLR can be written as a special case of generalized least squares (GLS).
- The correlation structure determines what the matrix $R$ looks like mathematically.
- You still estimate:
  - Regression coefficients
  - Standard errors of the coefficients
- But now, you’re also estimating the correlation structure among residuals.

Key idea: When correlation is accounted for, statistical inference is valid, and predictions can be improved.


## Weighted Least Squares (WLS)

GLS can also be applied when independence holds but the assumption of constant variance is violated (i.e. heteroscedasticity).

```{r out.width="60%", echo = FALSE}
knitr::include_graphics("images/appCh06_weightedLeastSquares_pg01.jpg")
knitr::include_graphics("images/appCh06_weightedLeastSquares_pg02.jpg")
```

You can only apply WLS if you know (or can estimate) the weights. There are numerous strategies for estimating the weights, but the basic strategy follows a theme. This is a common approach:

1. Fit a regular regression model.
2. Plot the absolute values of your residuals against:
   - One of your predictors that you suspect may explain the changing variance, or  
   - The predicted values from your model  
   Look for a pattern or trend in the spread of the residuals.
3. Fit a regression model to the absolute value of the residuals to estimate the mean behavior (i.e. trend) of the variability.
4. Use the predicted values from that model to compute weights: $w_i = \frac{1}{(\widehat{\text{predicted}})^2}$

```{r eval = FALSE}

weight.model = lm(abs(model1$residuals) ~ data1$x)
wts = 1 / fitted(weight.model)^2
wls.model = lm(y~x, data1, weights=wts)

```

### Interpretation
- Observations with higher variance are down-weighted.
- The fitted line will more closely follow observations with lower variance.
- The biggest difference is in the standard error estimates of the regression coefficients.

### When to Use WLS
- When transformations do not fix the constant variance problem.
- When an exotic transformation works but makes the model hard to interpret.

### Additional Notes
- Raw residual vs. fitted plots may look the same for OLS and WLS.
  - Use studentized residuals to check whether the variance has been stabilized
