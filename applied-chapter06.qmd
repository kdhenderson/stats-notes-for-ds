---
title: "Repeated Measures"
---

## Objectives

This chapter explores repeated measures, which allow us to relax the independence assumption.

- Understand when repeated measures are appropriate.  
- Learn about correlation structures, how to visualize them, and select the appropriate structure.  
- Explore generalized least squares (GLS) and weighted least squares (WLS).  
- Follow a repeated measures workflow to structure analysis.  
  
  
## What is Repeated Measures?  

### Family Tree of Linear Models

::: {}
```{mermaid}

flowchart TD
  A["Linear models<br>(constant variance,<br>normality assumptions)"] --> B["Errors are<br>independent"]
  B --> D["t-tests"]
  B --> E["ANOVA"]
  B --> F["SLR"]
  A --> C["Errors are<br>correlated"]
  C --> G["Time series"]
  C --> H["Repeated measures"]

  %% Arrow styling
  linkStyle default stroke:#333,stroke-width:1.5px;

  %% Node styling
  style A fill:#d1c4e9,stroke:#512da8,stroke-width:1.25px,color:#000
  style B fill:#bbdefb,stroke:#1976d2,stroke-width:1.25px,color:#000
  style C fill:#ffcdd2,stroke:#c62828,stroke-width:1.25px,color:#000

  style D fill:#e3f2fd,stroke:#64b5f6,stroke-width:1.25px,color:#000
  style E fill:#e3f2fd,stroke:#64b5f6,stroke-width:1.25px,color:#000
  style F fill:#e3f2fd,stroke:#64b5f6,stroke-width:1.25px,color:#000

  style G fill:#ffebee,stroke:#ef5350,stroke-width:1.25px,color:#000
  style H fill:#ffebee,stroke:#ef5350,stroke-width:1.25px,color:#000

```
:::


## Repeated Measures

- Linear models assume constant variance and normality with independent errors.
  - Examples: t-tests, ANOVA, simple linear regression (SLR) 
- When errors are correlated:
  - Examples: time series, repeated measures  
  - Ignoring correlation can produce misleading $p$-values and confidence intervals.  
  - We need to model the correlation among errors (i.e., include additional parameters in the model) for valid inference and better predictions. 
- Repeated measures refers to multiple measurements on the same subject (i.e., dependent observations).
  - For example, if a subject starts with high (or conversely low) values, their measurements tend to remain high (or low) over time.  

### Common Hypotheses in Repeated Measures:

- Compare every time point to a baseline (i.e., control), for each group (e.g., asymptomatic vs. symptomatic).
- Compare each time point between groups. (Baseline may not differ, but other time points might.)

### Regression Perspective

Treat time as a categorical variable because measurements are made at discrete time points:

$$
Y = \text{Time} + \text{Status} + \text{Time} \times \text{Status} + \varepsilon
$$

- Include an interaction since trends depend on status.  
- Regression and contrasts help compare groups---we just need to model correlated errors.

### Identifying Repeated Measures

- Ask how the data were collected.  
- Repeated measures can apply to:
  - Repeated measures one-way ANOVA
  - Repeated measures MLR
  

## Correlation Structures

### Assessing Error Dependence

- Some methods include residual diagnostic tools to visualize correlated errors.
- Similar tools exist for repeated measures, but they aren't as commonly used.
- The key idea is to explain how residuals are correlated using a correlation structure.

#### Correlogram (pseudo-code approach)

1. Obtain residuals from a regular MLR (some from the same subject).
2. For residuals that are one unit apart in time:
   - Create a scatterplot (earlier time point on the $x$-axis, later time point on the $y$-axis).
   - Compute the correlation, and store the result.
3. Repeat for residuals that are two, three, four, $\dots, k$ units apart.
4. Plot the correlation values against the time lag.

#### Takeaway 
Correlation tends to decrease as the distance between residuals increases and eventually levels off. Large correlations at short time lags are a clear indication of correlated errors.
  
### Correlogram Interpretation

- Observations closer in time are more similar (i.e., more correlated).
- Observations farther apart in time may still be mildly correlated.

### Common Correlation Structures

Correlation structures are theoretical models that describe the expected trend in the correlogram.

#### Compound symmetry (CS)
- Correlation is constant (i.e., flat, horizontal line) regardless of time.
- Often used when time ordering is not meaningful (e.g., students within the same school).

#### Autoregressive (AR(1))
- High correlation for nearby time points.
- Correlation decreases as time lag increases.
- Eventually approaches zero (like independent errors).

#### Gaussian
- Similar to AR(1), but the drop-off in correlation is even faster.
- Often used when time is continuous.
  
### Generalized Least Squares (GLS)
- GLS generalizes OLS technique for MLR to handle correlated errors.
- You specify:
  - A model with response and predictors
  - A correlation structure (parameters estimated by software)
- GLS updates both:
  - Regression coefficients
  - Standard errors
- These estimates are more reliable than standard MLR if the chosen correlation structure is approximately correct.

### Estimating Correlation in Practice
- Variograms and semivariograms (often used in spatial models) can be used as visual alternatives to correlograms.
- In practice, repeated measures datasets may be too small to visualize correlation clearly through the high variability.
- Analysts typically:
  - Use theoretical justification for choosing a structure.
  - Fit multiple structures and compare using AIC (like feature selection).
  - Use correlograms as a visual guide.
- Instructor note (Turner’s experience):  
  - Compound symmetry works well for biological or human-based data.  
  - Use structures with rapid decay (e.g., Gaussian) sparingly unless time points are equally spaced and numerous.  

   
## Repeated Measures Workflow

1. Identify that you are in a repeated measures setting.
2. Perform EDA based on the equivalent MLR framework (determine the general theme: ANOVA, SLR, or MLR):
   - Use boxplots or mean plots in ANOVA-style settings.
   - Use scatterplots of predictor vs. response in SLR-style settings.
   - Combine approaches for general MLR settings.
   - Apply feature selection tools from MLR to assess model complexity and the bias-variance trade-off.
3. Residual diagnostics:
   - Check for normality and constant variance using residual plots.
4. Update MLR to GLS:
   - Try multiple correlation structures.
   - Use AIC to select the best-fitting model.
   - Optionally use a correlogram to guide your choice.
5. Conduct inference on regression coefficients:
   - Report $p$-values and confidence intervals.
   - Results will be more trustworthy when correlation is accounted for.
   

## Additional Notes on Correlation Structures

- Some correlation structures **require a time component**:
  - Autoregressive (AR; exponential decay)
  - Gaussian
  - Linear
  - Spherical
  - Matérn (A general class that includes AR and Gaussian as special cases; more flexible but may overfit.)
- Some correlation structures **do not require a time component**:
  - Compound symmetry (CS)
  - Variance components
- Examples that don't involve time:
  - Texas STAR exam: multiple test scores from the same schools/school districts
  - Hereditary studies: family studies where siblings or parents/offspring are clustered


## Technical Details

### Matrix Representation of MLR

$$
Y = X\beta + \varepsilon
$$

where:

- $Y$ is the response vector.
- $X$ is the design matrix.
- $\beta$ is the regression coefficient vector.
- $\varepsilon$ is the error vector.

The model assumes properties of the errors:

$$
\varepsilon \sim \mathcal{N}(0, \sigma^2 I)
$$

that is:

- $\varepsilon$ is multivariate normal
- where $\mathcal{N}$ denotes the multivariate normal distribution (MVN)
- Mean vector = $0$, a $n x 1$ matrix of $0$s
- Covariance matrix = $\sigma^2 I$

The covariance matrix:

$$
\Sigma = \begin{bmatrix}
\sigma^2 & 0 & 0 \\
0 & \sigma^2 & 0 \\
0 & 0 & \sigma^2 \\
\end{bmatrix} = \sigma^2I
$$

Interpretation:

- Each individual error has the same variance $\sigma^2$
- Traditional multiple linear regression (MLR) assumptions:
  - Errors are independent, which implies they are also uncorrelated, so all off-diagonal covariances are 0.

### Generalized Least Squares (GLS) and Correlated Errors

In repeated measures models, we no longer assume that errors are uncorrelated.

To account for correlated errors, we extend the standard MLR assumptions using **generalized least squares (GLS)**. This involves specifying a more flexible error structure.

We now assume:

$$
\varepsilon \sim \mathcal{N}(0, \sigma^2 R)
$$

Where:

- $\varepsilon$ is multivariate normal with mean zero.
- $\sigma^2 R$ is the **variance–covariance matrix** of the errors.
- $R$ is the **correlation matrix**, which specifies how errors are related (e.g., through time, subject grouping, etc.).
- When $R = I$, we recover the standard MLR case with independent errors.

This updated formulation allows us to model within-subject correlation (e.g., repeated observations) by directly encoding the assumed pattern of correlation into $R$.

### Visualizing the Correlation Structure in $\sigma^2 R$

We can visualize the error term in repeated measures models as:

$$
Y = X\beta + \varepsilon, \quad \varepsilon \sim \mathcal{N}(0, \sigma^2 R)
$$

Each row in $\varepsilon$ corresponds to a specific subject and time point:

| Subject | Time | Error Term |
|---------|------|------------|
| 1       | T1   | $\varepsilon_1$ |
| 1       | T2   | $\varepsilon_2$ |
| 1       | T3   | $\varepsilon_3$ |
| 2       | T1   | $\varepsilon_4$ |
| 2       | T2   | $\varepsilon_5$ |
| 2       | T3   | $\varepsilon_6$ |

The corresponding variance–covariance matrix $\sigma^2 R$ has a **block-diagonal structure** and might look like this:

$$
\sigma^2 R = \sigma^2 \begin{bmatrix}
1 & \rho_{12} & \rho_{13} & 0 & 0 & 0 \\
\rho_{12} & 1 & \rho_{23} & 0 & 0 & 0 \\
\rho_{13} & \rho_{23} & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & \rho_{45} & \rho_{46} \\
0 & 0 & 0 & \rho_{45} & 1 & \rho_{56} \\
0 & 0 & 0 & \rho_{46} & \rho_{56} & 1 \\
\end{bmatrix}
$$

Key points:

- The **upper-left 3×3 block** corresponds to Subject 1 and correlations among $\varepsilon_1$, $\varepsilon_2$, $\varepsilon_3$.
- The **lower-right 3×3 block** corresponds to Subject 2 and correlations among $\varepsilon_4$, $\varepsilon_5$, $\varepsilon_6$
- **Off-diagonal correlations exist within subjects** (e.g., $\rho_{12}, \rho_{13}, \rho_{23}$ for subject 1).
- **Between-subject correlations are zero**, indicated by the block-diagonal structure.
- The values of the $\rho$ terms are determined by the assumed **correlation structure** (e.g., compound symmetry, AR(1), etc.).

This block structure is how repeated measures models encode dependency **within subjects** but maintain **independence between subjects**.

::: {layout-ncol=2}
![](images/appCh06_technicalDetails_pg01.jpg)

![](images/appCh06_technicalDetails_pg02.jpg)
:::

---

::: {layout-ncol=2}
![](images/appCh06_technicalDetails_pg03.jpg)

![](images/appCh06_technicalDetails_pg04.jpg)
:::

---

::: {layout-ncol=2}
![](images/appCh06_technicalDetails_pg05.jpg)

:::

The general linear model relaxes the independence assumption and allows for any specification of correlation among error terms (with the matrix $R$).

- This includes multiple linear regression (MLR) as a special case when the correlation structure is $R = I$.
- The generalization comes from recognizing that MLR can be written as a special case of generalized least squares (GLS).
- The correlation structure determines what the matrix $R$ looks like mathematically.
- You still estimate:
  - Regression coefficients
  - Standard errors of the coefficients
- But now, you’re also estimating the correlation structure among residuals.

Key idea: When correlation is accounted for, statistical inference is valid, and predictions can be improved.


## Weighted Least Squares (WLS)

GLS can also be applied when independence holds, but the assumption of constant variance is violated (i.e., heteroscedasticity).

::: {layout-ncol=2}
![](images/appCh06_weightedLeastSquares_pg01.jpg)

![](images/appCh06_weightedLeastSquares_pg02.jpg)
:::

You can only apply WLS if you know (or can estimate) the weights. There are numerous strategies for estimating the weights, but the basic strategy follows a theme. This is a common approach:

1. Fit a regular regression model.
2. Plot the absolute values of your residuals against:
   - One of your predictors that you suspect may explain the changing variance, or  
   - The predicted values from your model  
   Look for a pattern or trend in the spread of the residuals.
3. Fit a regression model to the absolute value of the residuals to estimate the mean behavior (i.e., trend) of the variability.
4. Use the predicted values from that model to compute weights: $w_i = \frac{1}{(\widehat{\text{predicted}})^2}$

```{r eval = FALSE}

weight.model = lm(abs(model1$residuals) ~ data1$x)
wts = 1 / fitted(weight.model)^2
wls.model = lm(y~x, data1, weights=wts)

```

### Interpretation
- Observations with higher variance are down-weighted.
- The fitted line will more closely follow observations with lower variance.
- The biggest difference is in the standard error estimates of the regression coefficients.

### When to Use WLS
- When transformations do not fix the constant variance problem.
- When an exotic transformation works but makes the model hard to interpret.

### Additional Notes
- Raw residual vs. fitted plots may look the same for OLS and WLS.
- Use studentized residuals to check whether the variance has been stabilized.
