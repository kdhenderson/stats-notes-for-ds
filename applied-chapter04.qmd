---
title: "Communicating with Clients"
---

## Objectives

This chapter covers key aspects of communicating statistical analyses effectively, including:

- Consulting and interacting with clients
- Applying models in practice
- Effective communication strategies
- Handling missing data
- Interpreting Mean Squared Error (MSE)


## Consulting

### Statistical Toolset
A consultant’s statistical toolbox includes:
- Group comparisons
- Nonparametric methods
- Regression analysis
- Feature selection (penalized regression, stepwise selection)
- Bootstrap and bagging

### Key Questions Before Starting
- What should I consider before beginning the analysis?
- What should the overall plan be?
- How do I communicate my findings effectively?

### Challenges for New Analysts
- Not fully understanding client needs
- Overlooking what the dataset offers

### Effective Communication with Clients
Good opening lines when talking to clients with limited statistical knowledge:
- "Tell me about your study/project."  
  - Avoid asking, "How can I help?" or "What do you need?" as these may bias your understanding.

### Extracting Key Information

#### Study Goals
- Is the purpose to **explain**, **predict**, or both?
- What is the response variable? (Numerical, categorical, count, etc.)
- Are there specific questions the client wants to answer?
- If predicting, does the client want to quantify how predictors contribute to response changes?
    
#### Population and Data Collection
- What is the target population, and how was data collected?
- Can conclusions apply to a slightly smaller population?
- Are some variables hard to obtain/measure or unreliable?
- Are measurements repeatable?
- Is each observation independent?

#### Practical Considerations
- What are meaningful differences, slopes, or accuracy metrics?
- What actions will be taken based on the results?

#### Why This Information Matters
- Response variable type determines the appropriate method (e.g. regression vs. classification).
- Understanding the goal (explanation vs. prediction) helps select appropriate statistical tools.
  - e.g. selecting a tool that supports hypothesis testing if required.
- Some people use "predict" loosely, meaning correlation or association tests instead.
- Adjusting population definition can impact:
  - Handling missing data
  - Identifying population restrictions (e.g. specific patient subsets like lupus patients prior to treatment)

#### Data Reliability and Independence
- Is the data an accurate reflection of reality?
  - Consider biases in reporting (e.g. self-reported data like race).
  - Instrument reliability and potential degradation over time.
  - If you measure again, will you get the same value?
    - Helps identify important explanatory variables and confounding variables.
- Independence assumption is influenced by data collection methods:
  - Students from the same district may not be independent.
  - Time series data may have autocorrelation.
  - Spatially close observations may be correlated.  

### Practical vs. Statistical Significance
- Ensures results are interpreted correctly and not overemphasized

### Considerations for Predictive Modeling
- Model readiness for deployment
- Need for additional/better predictors
- Redefining response variables
- Collecting more data for refinement
- Identifying whether hypothesis testing is needed for decision-making
- Computation needs for model deployment (beyond the scope of this course)  
    
    
## Big Data Considerations

### Google Flu Trends (GFT) Case Study
- Background: CDC reports flu-related doctor visits with a 1-2 week lag. GFT aimed to predict these visits with a 1-day lag.
- Issue: GFT overestimated doctor visits by almost 2x.

### Big Data Hubris
- Assumes big data fixes all statistical issues, including sampling biases (i.e. sampling that isn't random).
- Many big data sources are not from highly accurate scientific instruments.
- "Garbage in, garbage out" principle applies.  

### Understanding Model Predictions
- Flu season coincides with winter.
- GFT detected both flu and winter, leading to misleading predictors.
  - Important predictors were basketball related (i.e. winter sport related). 

### Temporal Dependencies in Prediction Models  
- Errors in prediction models are often not independent over time.  
- The correlation within prediction errors can be used to improve model accuracy.  

### Data Snooping and Overfitting  
- The more predictors included, the higher the chance of falsely finding statistical significance (Type I error).
- When the number of predictors is much higher than the number of observations, overfitting is much more likely.  

### Algorithm Dynamics and Data Stability  
- Changing the algorithm that produces your dataset can fundamentally alter its properties.  
- Is the measurement of a variable stable and comparable across cases and over time?  
  
### Database Logistics
- Just because data is available doesn’t mean all of it should be used.
- Consider:
  - Sampling appropriately to represent the target population.
  - Measurement error risks (i.e. consider the error risk of variables.)
  - The necessity of specific variables - which ones are practically important? 
    
---

## Regression Models in Practice

### General Workflow

The general workflow for regression is a highly iterative process.

1. **Data Processing**: Cleaning, curation, dropping variables
2. **Exploratory Data Analysis (EDA)**:    - Summary statistics
   - Visualization
   - Light modeling for insight and assumption checking 
3. **Model Building**:
   - Feature selection
   - Tuning (cross-validation)
   - Manual iteration
   - Assumption checking and fixing
4. **Communicating Results**:
   - Hypothesis testing and interpretation
   - Prediction performance metrics, report the validation set results
   - Model comparison, provide a table (e.g. non-parametric vs. parametric) 
   - Deployment strategy

**Advice**: Avoid excessive iteration to prevent overfitting. Ensure models use the same finalized, processed dataset. Consider which population your final data set comes from (e.g. if you handle missing data a certain way).  

---  
    
## Effective Communication of Results

Can be difficult because:  
  - Tt's a highly iterative process with many decisions along the way.   
  - You've don't so much. How much detail to give?  
  - There are many technical things. Know your audience.  
  - Time and page number constraints  
  
Try to summarize your approach and workflow mimicing the theoretical linear workflow.  
Additional details can be put in an appendix at the back of a report or at the back of the ppt.  

**Typical Report Sections**  
1. Problem statement and presentation overview  
2. Data description (variables) and processing summary  
3. Graphics and summary statistics tables that help articulate what your final model is going to say/do  
4. Results/interpretation, highlight final model: interpret and/or evaluate prediction performance  
5. Repeat steps 3 and 4 if there are multiple problem statements  
6. Conclusions (brief summary statements of global conclusions) and WIP (work in progress)  
  
*Problem Statement*  
  - clearly define the objective and goals  
  - brief outline of report (structure)  
    - multiple objectives  
    - multiple approaches  
    - appendices for written documents  
      
*Data Descriptions*  
  - When using coded variables:  
    - provide table mapping coded with actual names  
  - Including data types is also helpful since not all analysts will treat variables the same way (important for written report)  
    - Visual presenters can address these organically  
      
*Processing Data*  
  - You will natural iterate your models along with processing data.  
  - Final processed data should be shown in EDA when presenting and what is used on your final models. (Don't included deleted observations. Include exactly what your model is seeing so the graphics match what model suggests.)  
  - Address the handling of missing data (brief).  
  - High-level summaries are needed here.  
    - Only technical if absolutely critical to the discussion.  
    
*EDA*  
  - **Summary statistics** should always be made available.  
    - Table: mean, median, sd, min, max, 5 number summary, proportions or counts with categorical variables  
    - If you don't talk about, put in appendix or back of slide deck.
    - Provide reader/audience a way to sanity check basic things.  
    - Can help explain what other parts of your models are doing what they are doing  
      - Ex: Why are you running KNN on a set of predictors that are z-scored (or scaled in some other way)?  
  - **Graphics and Tables**  
    - Don't let them do the talking for you. If you don't talk about it, assume your audience will never look at it.  
    - If a graph is too busy:  
      - Focus their attention somehow or make a new graph.  
  - **Show The Trends**  
    - EDA graphics you use for your own model-building process many not be the best for presenting  
    - Focus should be on your response variable and how it relates to the predictors, the trends that help explain why you include an interaction or polynomial term or why you can tell a variable isn't important  
      - You could parse out numerical and categorical variables (with scatterplots and boxplots separated)  
    - Mention but don't let things like multicollinearity dominate the EDA discussion - highlight the trends  
    - You know what the final model looks like:  
      - Build audience's intuition.   
      - Highlight important relationships.  
      - Highlight unimportant ones if that variable was part of the question.  
  - **Regression Tables**
    - Do you really want people to see p-values and t-statistics?  
      - Is a CI and *** for significance enough?  
      - Maybe you don't need the point estimate, the t-statistic, the p-value?  
      - Consider creating results from R and remaking them with only the relevant info.  
      - Don't distract with technical info that isn't utilized.  
        
  
**Missing Data**

Ways that observations can have missing values:  
  - Missing completely at random (MCAR)  
  - Missing at random (MAR)  
  - Missing not at random (MNAR)  

**The Concern with Deletion**  
  - The most basic approach is to delete the observations (rows) that have missing observations in them.  
  - Default by most software.  
  - Could drastically reduce the sample size.  
  - Deleting observations could potentially change the population of which your sample is representative.  

**Missing Completely at Random (MCAR)**  
  - Observations that have missing values are a random subset of all the observations.  
  - Data removed is consistent with data kept.  
    - similar summary statistics, correlation behavior, and distributions  

**Missing at Random (MAR)**  
  - Less restrictive than MCAR.  
  - Observations that are deleted have the same distribution for stratifications of the data based on a known variable.  

**Missing Not at Random (MNAR)**  
  - Catch-all bucket for not MCAR and not MAR.  
  - There is a systematic reason for why the missing data are there.  

**Deletion under MCAR and MAR**  
  - If MCAR or MAR is reasonable, there is some justification for deleting the missing rows.  
  - Maintain awareness that deleting rows could affect population definition.  
    - usually do some summary statistics and data description on data deleted vs. data kept  
  - Imputing the missing values, or incorporating a model that handles missing values, is the approach to take.  
  - Should never delete with MNAR.  

**Imputation**  
  - Imputation is the act of replacing missing values in your data set with an estimate of what the actual value should have been.  
  - New prediction problem: Variable with missing is the response.  
  - *Simple Imputation*  
    - Replace the missing values using the mean or median of the variable of interest.  
    - For categorical variables, replace with the most common categorical level.  
    - Not always a good approach:  
      - under the MCAR setting  
      - large amounts of missing data  
  - *Imputation via Regression*  
    1. Fit a regression model using complete data.  
        - Variable with missing is the response.  
    2. Predict the missing observation using your model.  
    3. Repeat steps 1 and 2 for every variable you want to impute missing values.  
    - Can be done for categorical variables with classification models.  
    - Imputation processes never incorporate the true response variable. Only use the predictors.  
    - You can perform your own regression imputation manually.  
        - Commonsense approach by stratifying data you do have to predict the missing. Example: multiple observations for one country; use simple imputation using data for that country only.  
    - Visual assessment of missing data is helpful.
