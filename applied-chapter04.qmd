---
title: "Communicating with Clients"
---

## Objectives

This chapter covers key aspects of communicating statistical analyses effectively, including:

- Consulting and interacting with clients
- Applying models in practice
- Effective communication strategies
- Handling missing data
- Interpreting Mean Squared Error (MSE)


## Consulting

### Statistical Toolset
A consultant’s statistical toolbox includes:
- Group comparisons
- Nonparametric methods
- Regression analysis
- Feature selection (penalized regression, stepwise selection)
- Bootstrap and bagging

### Key Questions Before Starting
- What should I consider before beginning the analysis?
- What should the overall plan be?
- How do I communicate my findings effectively?

### Challenges for New Analysts
- Not fully understanding client needs
- Overlooking what the dataset offers

### Effective Communication with Clients
Good opening lines when talking to clients with limited statistical knowledge:
- "Tell me about your study/project."  
  - Avoid asking, "How can I help?" or "What do you need?" as these may bias your understanding.

### Extracting Key Information

#### Study Goals
- Is the purpose to **explain**, **predict**, or both?
- What is the response variable? (Numerical, categorical, count, etc.)
- Are there specific questions the client wants to answer?
- If predicting, does the client want to quantify how predictors contribute to response changes?
    
#### Population and Data Collection
- What is the target population, and how was data collected?
- Can conclusions apply to a slightly smaller population?
- Are some variables hard to obtain/measure or unreliable?
- Are measurements repeatable?
- Is each observation independent?

#### Practical Considerations
- What are meaningful differences, slopes, or accuracy metrics?
- What actions will be taken based on the results?

#### Why This Information Matters
- Response variable type determines the appropriate method (e.g. regression vs. classification).
- Understanding the goal (explanation vs. prediction) helps select appropriate statistical tools.
  - e.g. selecting a tool that supports hypothesis testing if required.
- Some people use "predict" loosely, meaning correlation or association tests instead.
- Adjusting population definition can impact:
  - Handling missing data
  - Identifying population restrictions (e.g. specific patient subsets like lupus patients prior to treatment)

#### Data Reliability and Independence
- Is the data an accurate reflection of reality?
  - Consider biases in reporting (e.g. self-reported data like race).
  - Instrument reliability and potential degradation over time.
  - If you measure again, will you get the same value?
    - Helps identify important explanatory variables and confounding variables.
- Independence assumption is influenced by data collection methods:
  - Students from the same district may not be independent.
  - Time series data may have autocorrelation.
  - Spatially close observations may be correlated.  

### Practical vs. Statistical Significance
- Ensures results are interpreted correctly and not overemphasized

### Considerations for Predictive Modeling
- Model readiness for deployment
- Need for additional/better predictors
- Redefining response variables
- Collecting more data for refinement
- Identifying whether hypothesis testing is needed for decision-making
- Computation needs for model deployment (beyond the scope of this course)  
    
    
## Big Data Considerations

### Google Flu Trends (GFT) Case Study
- Background: CDC reports flu-related doctor visits with a 1-2 week lag. GFT aimed to predict these visits with a 1-day lag.
- Issue: GFT overestimated doctor visits by almost 2x.

### Big Data Hubris
- Assumes big data fixes all statistical issues, including sampling biases (i.e. sampling that isn't random).
- Many big data sources are not from highly accurate scientific instruments.
- "Garbage in, garbage out" principle applies.  

### Understanding Model Predictions
- Flu season coincides with winter.
- GFT detected both flu and winter, leading to misleading predictors.
  - Important predictors were basketball related (i.e. winter sport related). 

### Temporal Dependencies in Prediction Models  
- Errors in prediction models are often not independent over time.  
- The correlation within prediction errors can be used to improve model accuracy.  

### Data Snooping and Overfitting  
- The more predictors included, the higher the chance of falsely finding statistical significance (Type I error).
- When the number of predictors is much higher than the number of observations, overfitting is much more likely.  

### Algorithm Dynamics and Data Stability  
- Changing the algorithm that produces your dataset can fundamentally alter its properties.  
- Is the measurement of a variable stable and comparable across cases and over time?  
  
### Database Logistics
- Just because data is available doesn’t mean all of it should be used.
- Consider:
  - Sampling appropriately to represent the target population.
  - Measurement error risks (i.e. consider the error risk of variables.)
  - The necessity of specific variables - which ones are practically important? 
    
---

## Regression Models in Practice

### General Workflow

The general workflow for regression is a highly iterative process.

1. **Data Processing**: Cleaning, curation, dropping variables
2. **Exploratory Data Analysis (EDA)**:    - Summary statistics
   - Visualization
   - Light modeling for insight and assumption checking 
3. **Model Building**:
   - Feature selection
   - Tuning (cross-validation)
   - Manual iteration
   - Assumption checking and fixing
4. **Communicating Results**:
   - Hypothesis testing and interpretation
   - Prediction performance metrics, report the validation set results
   - Model comparison, provide a table (e.g. non-parametric vs. parametric) 
   - Deployment strategy

**Advice**: Avoid excessive iteration to prevent overfitting. Ensure models use the same finalized, processed dataset.  
Be mindful that the way missing data is handled can influence which population your model represents. For example, if missing values are not missing at random and are removed, the final dataset may not be representative of the full population.  


## Effective Communication of Results

### Challenges
- Iterative nature of analysis makes summarization difficult.
- Finding the right level of technical detail for the audience.
- Time and page constraints.

### Structuring Reports and Presentations
Try to summarize your approach and workflow by mimicking a theoretical linear workflow.  
Additional details can be placed in an appendix at the back of a written report or at the end of a presentation.  

A well-structured report typically includes:
1. **Problem Statement and Presentation Overview**
   - Clearly define the objective and goals.
   - Provide a brief outline of the report structure, especially if there are:
     - Multiple objectives.
     - Multiple approaches.
     - Appendices for more detailed information.
2. **Data Description and Processing Summary**
   - Define key variables and their roles in the analysis.
   - When using coded variables, provide a table mapping codes to actual names.
   - Consider including data types (numeric, categorical, etc.), as different analysts may interpret variables differently.
   - Summarize how data processing was conducted.
3. **Exploratory Data Analysis (EDA)**
   - Highlight important relationships between variables.
   - Use summary statistics and visualizations to articulate key insights.
4. **Results and Interpretation**
   - Provide a clear explanation of the final model’s findings.
   - Interpret coefficients and/or evaluate predictive performance.
5. **Conclusions and Work-in-Progress (WIP)**
   - Offer brief global conclusions and next steps.
   - If there are multiple problem statements, repeat steps **3 & 4** as needed.

### Data Processing Considerations
- Ensure that the final processed dataset aligns with EDA and model inputs.
- Summarize missing data handling without excessive technical detail.

### Presenting EDA Findings
- **Summary statistics**: Should always be provided. Include mean, standard deviation, 5-number summary (median, min/max), and category counts or proportions.
  - Provide reader/audience a way to sanity check basic things.
  - Can provide intuition for what parts of the model is doing. For example, why are you running KNN on a set of predictors that are z-scored (or scaled in some other way)? 
- **Graphics and Tables**: 
  - Don’t assume that figures speak for themselves—explicitly reference them. (Additional figures can be included in an appendix.)
  - If a graph is too cluttered, either simplify it or guide the audience’s focus.
- **Showing Trends**:
  - EDA visuals used for model-building may not be the best for presentation.
  - Focus on how the response variable relates to predictors.
  - Highlight trends that justify interaction terms or transformations.
  - Avoid overemphasizing multicollinearity unless it directly affects interpretation.
  - Consider parsing out numerical and categorical variables with either scatterplots or boxplots. 
  - Build the audience's intuition of the final model, highlight important relationships and unimportant ones when a variable was part of the question of interest.
      
### Regression Tables
- Consider whether the audience needs to see p-values and t-statistics.
  - Would confidence intervals and significance markers (**\*\*\***, **\***, etc.) be clearer?
  - Can results be reformatted to show only essential information?
  - Avoid overwhelming the audience with statistics and technical details that aren’t being used in interpretation.

### Interpreting Mean Squared Error (MSE)
- MSE measures prediction error, balancing bias and variance.
- A lower MSE suggests a better model fit, but it must be evaluated alongside interpretability and generalization ability.
    
  
**Missing Data**

Ways that observations can have missing values:  
  - Missing completely at random (MCAR)  
  - Missing at random (MAR)  
  - Missing not at random (MNAR)  

**The Concern with Deletion**  
  - The most basic approach is to delete the observations (rows) that have missing observations in them.  
  - Default by most software.  
  - Could drastically reduce the sample size.  
  - Deleting observations could potentially change the population of which your sample is representative.  

**Missing Completely at Random (MCAR)**  
  - Observations that have missing values are a random subset of all the observations.  
  - Data removed is consistent with data kept.  
    - similar summary statistics, correlation behavior, and distributions  

**Missing at Random (MAR)**  
  - Less restrictive than MCAR.  
  - Observations that are deleted have the same distribution for stratifications of the data based on a known variable.  

**Missing Not at Random (MNAR)**  
  - Catch-all bucket for not MCAR and not MAR.  
  - There is a systematic reason for why the missing data are there.  

**Deletion under MCAR and MAR**  
  - If MCAR or MAR is reasonable, there is some justification for deleting the missing rows.  
  - Maintain awareness that deleting rows could affect population definition.  
    - usually do some summary statistics and data description on data deleted vs. data kept  
  - Imputing the missing values, or incorporating a model that handles missing values, is the approach to take.  
  - Should never delete with MNAR.  

**Imputation**  
  - Imputation is the act of replacing missing values in your data set with an estimate of what the actual value should have been.  
  - New prediction problem: Variable with missing is the response.  
  - *Simple Imputation*  
    - Replace the missing values using the mean or median of the variable of interest.  
    - For categorical variables, replace with the most common categorical level.  
    - Not always a good approach:  
      - under the MCAR setting  
      - large amounts of missing data  
  - *Imputation via Regression*  
    1. Fit a regression model using complete data.  
        - Variable with missing is the response.  
    2. Predict the missing observation using your model.  
    3. Repeat steps 1 and 2 for every variable you want to impute missing values.  
    - Can be done for categorical variables with classification models.  
    - Imputation processes never incorporate the true response variable. Only use the predictors.  
    - You can perform your own regression imputation manually.  
        - Commonsense approach by stratifying data you do have to predict the missing. Example: multiple observations for one country; use simple imputation using data for that country only.  
    - Visual assessment of missing data is helpful.
