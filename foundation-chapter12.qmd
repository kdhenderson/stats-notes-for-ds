---
title: "Inferences for Multiple Linear Regression"
---

## Objectives

- Interpret inferential statistics for various types of regression models, including those with indicator variables, quadratic terms, and interaction terms.
- Interpret and communicate results from a multiple regression analysis.


## Multiple Linear Regression (MLR) with a Single Predictor: Polynomial Regression

- Still a linear model:
  - Captures curvilinear trends.
  - Linear in the $\beta_i X^j$ term: for every one-unit increase in $X^j$, $Y$ changes by a constant $\beta_i$. 
  - If the quadratic coefficient is significant but the linear term is not, still include the linear/lower-order terms as they form a singular idea.
  - Interpretation can be difficult, especially with higher-order terms.

- Interpretation of a quadratic model:
  - Find the vertex (absolute max/min) using:$x_{\text{vertex}} = \frac{-\beta_1}{2\beta_2}$
  - This suggests that the mean $X$ at which $Y$ stops increasing or decreasing is given by $-\beta_1 / 2\beta_2$ based on a quadratic regression model.


## Coefficient of Determination: $R^2$

- A measure of effect size for both SLR and MLR.
- Represents the proportion of variance in the response variable explained by the model.

### Calculation in MLR:
- **Multiple $R^2$:** Find the squared correlation between observed $Y$ and predicted $Y$.
- **Partition of sums of squares:**
  $$
  R^2 = \frac{\text{SS}_{\text{total}} - \text{SS}_{\text{residual}}}{\text{SS}_{\text{total}}} = 1 – \frac{\text{SS}_{\text{residual}}}{\text{SS}_{\text{total}}}
  $$

### Relationship with the Overall F-Test:
- The F-test statistic can be rewritten in terms of $R^2$.
- Testing if all parameters equal zero is equivalent to testing if $R^2 = 0$.

### $R^2$ and Parsimony:
- Adding more predictors increases $R^2$.
- An oversaturated model can have $R^2 = 1$ even if predictors do not contribute meaningfully.
- The goal is a parsimonious model—one that is as simple as possible while still maximizing $R^2$.

### Adjusted $R^2$:
- Accounts for the number of predictors ($k$) and sample size ($n$).
- Formula: $R^2_{\text{adj}} = R^2 – (1 - R^2) \frac{k}{n-k-1}$
- Penalizes unnecessary predictors and small sample sizes. Rewards variables that significantly improve model fit, resulting in a net positive if a predictor variable explains a lot of the variance.
- Is a better measure of effect size in MLR.


## Overfitting

- $R^2$ can always be made 1, but that does not indicate good predictive power.
- Overfitting means the model fits both noise and signal in the sample data.
- A high $R^2$ does not guarantee that the model generalizes well to new data.


## Controlling for Other Variables in MLR

- When holding $X_2$ constant (i.e. looking at a subpopulation with similar $X_2$ values), we can examine the effect of changes in $X_1$ on $Y$.


## Overall F-Test

- Tests whether the model is statistically significant.
- **Hypotheses:**
  - $H_0$: All slopes are equal to 0 (i.e. $R^2 = 0$).
  - $H_a$: At least one slope is not zero.
- **F-statistic formula:** $F = \frac{\text{SS}_{\text{regression}} / k}{\text{SS}_{\text{residual}} / (n - k - 1)} = \frac{\text{MS}_{\text{regression}}}{\text{MS}_{\text{error}}}$
- Can also be expressed in terms of $R^2$.


## Partial F-Test (Extra-Sum-of-Squares)

- Used when a subset of $k$ predictors does not have statistically significant slopes, i.e. to examine a reduced model with $g$ of $k$ predictors.
- **Hypotheses:**
  - $H_0$: The extra $k-g$ predictors do not contribute (all their slopes are 0).
  - $H_a$: At least one of the extra predictors has a nonzero slope.
- Simultaneously tests whether additional predictors improve the model.

![Extra-Sum-of-Squares F-Test Example](images/notes_12_1.png)  
*Figure: Extra sum of squares F-test example, including full and reduced models for log(cost) with and without age and gender, and interpretation.*


## Significance Tests for Each Predictor (t-tests)

- **Hypotheses:**
  - $H_0: \beta_i = 0$ (predictor has no effect)
  - $H_a: \beta_i \neq 0$ (predictor has an effect)
- **Test statistic:** $t = \frac{b_i}{SE_{b_i}}, \quad \text{with } df = n - k - 1$
- **Standard error calculation:** $SE = \frac{\hat{\sigma}}{\sqrt{\sum_{i=1}^{n} (X_i - \bar{X})^2}}, \quad \hat{\sigma} = \sqrt{\frac{\text{SSR}}{(n - k - 1)}}$


## Testing Intercept and Slope Differences in Regression Models (Bat Echolocation Example)

- **Question of Interest 1:**  
  Do echolocating and non-echolocating bats have equal slopes?  
  - Models for echolocating bats, non-echolocating bats, and birds are written out.  
  - Null and alternative hypotheses:
    - $H_0$: Slopes are equal
    - $H_a$: Slopes are not equal

![Bat Echolocation Example](images/notes_12_2.png)  
*Figure: Model setup and hypothesis for testing slope differences in echolocating and non-echolocating bats.*

- **Continuation:**  
  SAS output tables for full and reduced models with interpretation.

![Bat Example - Full and Reduced Models](images/notes_12_3.png)  
*Figure: SAS output tables for full and reduced models, testing slope differences in echolocating and non-echolocating bats.*

- **Next Question of Interest:**  
  Are the intercepts for bat types different?  
  - Null and alternative hypotheses:
    - $H_0$: Intercepts are equal
    - $H_a$: Intercepts are different

![Parameter Estimates for Bat Example](images/notes_12_4.png)  
*Figure: Parameter estimate table with interpretation and p-value.*


## Confidence Intervals for Linear Combinations of Coefficients

- 95% CI for difference in intercepts:  
  $$ (\hat{\beta}_2 + \hat{\beta}_3) - \hat{\beta}_0 $$
- To find the SE (**Variance calculation**):  
  $$ \text{Var}(\hat{\beta}_2 + \hat{\beta}_3) = \text{Var}(\hat{\beta}_2) + \text{Var}(\hat{\beta}_3) + 2 \text{Cov}(\hat{\beta}_2, \hat{\beta}_3) $$
- Compute the CI:
    $$
    (\hat{\beta}_2 + \hat{\beta}_3) \pm t_{0.975, df} \times SE(\hat{\beta}_2 + \hat{\beta}_3)
    $$

![CIs for Linear Combinations of Coefficients](images/notes_12_6.png)  
*Figure: Confidence interval calculations for linear combinations of coefficients, applied to the bat echolocation example.*


## Variance of Sum and Difference of Random Variables

- Formula: $\text{Var}(aX + bY) = a^2 \text{Var}(X) + b^2 \text{Var}(Y) + 2ab \text{Cov}(X, Y)$
- Used to compute the variance of estimates in regression.

![Variance of Sum and Difference of Random Variables](images/notes_12_5.png)  
*Figure: Variance of sum and difference of random variables, discussing covariance and its use in confidence intervals.*


## Interpreting p-values in Regression Output

- Example p-value interpretation for testing equality of mean energy expenditures:
  - $p = 0.7030$ suggests no significant difference.

![Parameter Estimate and Covariance Table](images/notes_12_7.png)  
*Figure: Parameter estimate table and covariance table with interpretation for the bat echolocation example.*


## Prediction in MLR

- Predicted values are the same as estimated means
- To get confidence (predicted mean) and prediction (individual prediction) intervals, use software because all variables need to be in the model.


## Testing Intercept Differences in Regression Models

- Example: Testing whether intercepts differ between bat types:
  - $H_0: \beta_3 = 0$ (no difference)
  - $H_a: \beta_3 \neq 0$ (significant difference)


## Least Squares Estimates and Standard Errors

- The least squares estimates of $\beta$s are in the coefficient column of the parameter estimate table.
- The estimate of $\sigma^2$ is given by:
  $$ \frac{\text{sum of squared residuals (SSE)}}{\text{df} (n-p)} = \text{MSE} $$
- The square root of MSE gives residual SE (or RMSE).
