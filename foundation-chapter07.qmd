---
title: "Linear Combinations and the Multiple Comparison Problem"
---

## Objectives
- Understand weaknesses of the alternative hypothesis for an ANOVA F-test.
- Formulate appropriate linear combinations of means.
- Understand the multiple comparison problem and how to correct for it.
- Use the formulation to make appropriate inferences.


## Fallacies in Hypothesis Testing

1. **False causality**: A small *p*-value does not indicate causation. Causality can only be inferred from randomized experiments, not observational studies.
2. **Accepting the null**: We should say there is “no evidence of a difference,” not that the null hypothesis is true. There is not enough evidence to suggest the observed difference is due to anything other than chance.
3. **Statistical vs. practical significance**: Statistical significance does not imply practical importance. Evaluate practical significance using power, effect size, and confidence intervals.  
   - Differences that are statistically but not practically significant may arise due to large sample sizes.  
   - If a difference is practically significant but not statistically significant, more data may be needed.
4. **Data Dredging**: Avoid fishing for significance (data snooping). This can lead to publication bias against negative results.
5. **Good statistics from bad data**: Biased or non-random data compromise conclusions. Experimental design determines which inferences and statistical methods are valid.


## Linear Combination of Group Means

- ANOVA allows only pairwise comparisons of means and does not account for meaningful structure across groups. In practice, you may wish to compare the average of several means to a single group, or to the average of others.  
- The ANOVA *F*-test serves as an initial screening tool for detecting any overall differences in means. If the *F*-test is significant, follow-up tests can be considered.

### Linear Contrasts
- A **linear combination** of group means is defined as:
  $$
  \gamma = C_1\mu_1 + C_2\mu_2 + \dots + C_I\mu_I
  $$
  where $\gamma$ represents the linear combination and the $C_i$ are coefficients. When the coefficients sum to zero, the combination is called a **contrast**.
- Estimate $\gamma$ using $g = C_1\bar{x}_1 + C_2\bar{x}_2 + \dots + C_I\bar{x}_I$, where $\bar{x}_i$ are group sample means.
- The standard error of $g$, denoted $SE(g)$, is given by:
  $$
  SE(g) = S_p \sqrt{\frac{C_1^2}{n_1} + \frac{C_2^2}{n_2} + \dots + \frac{C_I^2}{n_I}}
  $$
  where $S_p$ is the pooled standard deviation.  
  Even when comparing only two groups (i.e., the other coefficients and terms under the square root are zero), the pooled standard deviation is used under the assumption of equal variances across all groups.
- The *t*-statistic for testing contrasts is:
  $$
  t_{\alpha, df} = \frac{g - \gamma}{SE(g)}
  $$
  where $g$ is the observed linear combination, $\gamma$ is the hypothesized value (e.g., $\gamma = 0$), and $df$ is the degrees of freedom associated with $S_p$.


## Steps for a Linear Contrast Test

1. **Summarize the data**: Obtain group means, sample sizes, and the pooled standard deviation from the ANOVA model (e.g., RMSE).
2. **Specify the coefficients**: Choose $C_i$ values such that $\sum C_i = 0$.
3. **Estimate the contrast**: Compute the linear combination, $g$, using the sample means and chosen coefficients.
4. **Compute the standard error**: Calculate $SE(g)$ using the formula above.
5. **Construct confidence intervals**:
   $$
   CI = g \pm t_{\alpha, df} \cdot SE(g)
   $$
6. **Perform the test**: Calculate the *t*-statistic to assess the contrast.

### Simultaneous Inferences
- When testing multiple contrasts or pairwise comparisons, the risk of a **Type I error** increases. Adjustments to significance levels or *p*-values are needed to maintain the overall error rate.
- Distinguish between:
  - **Individual confidence level**: The probability that a single interval captures its parameter ($1 - \alpha$).
  - **Familywise confidence level**: The probability that all intervals simultaneously capture their parameters. This probability is less than $1 - \alpha$ unless corrections are applied.

### Common Applications of Linear Contrasts
1. Comparing average values, e.g., the mean of groups 1 and 2 versus group 3.
2. Comparing response rates across treatments, such as dietary interventions in animals.
3. Testing specific hypotheses about structured combinations of group means.


## Adjusting for Multiple Comparisons

When conducting multiple tests, it is important to control the **familywise error rate** (FWER). Common post-hoc procedures are outlined below.


### Procedures for All Pairwise Differences in Means

1. **Bonferroni adjustment**  
   - Adjusts the significance level:
     $$
     \text{Adjusted } \alpha = \frac{\alpha}{\# \text{ comparisons}}
     $$
   - Example: For 4 groups, there are $\binom{4}{2} = 6$ comparisons:
     $$
     \alpha = \frac{0.05}{6} = 0.0083
     $$
   - **Strengths**: Simple, widely applicable  
   - **Weaknesses**: Very conservative; reduces power
2. **Tukey’s Honestly Significant Difference (HSD)**  
   - Used to construct simultaneous confidence intervals for all pairwise mean differences  
   - Based on the **studentized range statistic**:
     $$
     q = \frac{\bar{X}_{\text{largest}} - \bar{X}_{\text{smallest}}}{\sqrt{\frac{MSE}{n}}}
     $$
     (for groups with equal $n$)  
   - Confidence intervals:
     $$
     (\bar{X}_i - \bar{X}_j) \pm q_{\alpha, (k, N-k)} \cdot \sqrt{\frac{MSE}{n}}
     $$
   - For unequal $n$, use the **harmonic mean** (as done in SAS):
     $$
     n_{ij} = \frac{2 \cdot n_i \cdot n_j}{n_i + n_j}
     $$
   - **Strengths**: Maintains FWER; easy to interpret  
   - **Weaknesses**: Assumes equal variances (can be extended via Tukey-Kramer)
3. **Ryan-Elliot-Gabriel-Welsch Q (REGWQ) procedure**  
   - Recommended by SAS; balances power and Type I error control  
   - **Method**:
     1. Order the group means in descending order
     2. Reject $H_0$ if:
        $$
        \text{Difference} > q_{\text{critical}} \cdot \text{Studentized range}
        $$
     3. Adjust *p*-values accordingly for the number of comparisons  
   - **Strengths**: High power; nominal Type I error  
   - **Weaknesses**: Algorithmically complex

--- 

### Procedures for Pairwise Differences from a Control:

1. **Dunnett’s Procedure**:  
   - Compares each treatment group to a single control group.  
   - Uses the **d-statistic**:
     $$
     D = d_{(k-1, N-k)} \cdot \sqrt{\frac{2 \cdot MSE}{n_{harmonic}}}
     $$
   - Degrees of freedom:
     $$
     df = (k-1, N-k)
     $$
     where $k-1$ is between-groups df, and $N-k$ is within-groups df.  
   - **Strengths**: Focused on control comparisons.  
   - **Weaknesses**: Limited to control-vs-treatment tests.
   
### Procedures for All Possible Comparisons:

1. **Scheffé’s Method**:  
   - Used for all possible comparisons, including non-pairwise contrasts.  
   - Based on the **F-distribution**, allowing more flexible hypotheses.  
   - **Strengths**: Flexible for complex comparisons. less conservative than Bonferroni. 
   - **Weaknesses**: Less powerful for pairwise comparisons.


## Key Considerations for Post-Hoc Comparisons

1. **Type I Error Control**:  
   Even if the null hypothesis is true, performing too many tests increases the risk of rejecting it by chance. Post-hoc procedures are designed to control this risk.

2. **Planned vs. Unplanned Comparisons**:  
   - Use **planned comparisons** when specific hypotheses about group differences are made (e.g. control vs. treatment).  
   - Use **unplanned post-hoc comparisons** when there are no prior hypotheses, but ANOVA shows significance.

3. **Decision Framework After ANOVA**:
   - If the **F-statistic is not significant**, stop testing (no evidence of group differences).  
   - If the **F-statistic is significant**, proceed with post-hoc methods based on:
     - Is there inherent structure in the data? For example, is it more appropriate to test one group mean against combinations of others (linear contrasts)?
     - The structure of the data may not always justify multiple pairwise comparisons.

### Summary Table of Post-Hoc Methods

| **Procedure**               | **Purpose**                                | **Strengths**                                                | **Weaknesses**                                   |
|-----------------------------|--------------------------------------------|------------------------------------------------------------|-------------------------------------------------|
| Bonferroni Adjustment       | Controls Type I error for pairwise tests   | Simple, widely applicable                                   | Very conservative, reduced power               |
| Tukey’s HSD                 | All pairwise differences                  | Maintains familywise error rate, easy interpretation        | Assumes equal variances                         |
| Dunnett’s Procedure         | Compare to control group                  | Focused on control vs. treatments                          | Not suitable for all pairwise comparisons       |
| REGWQ Procedure             | Pairwise differences                      | Good power, recommended by SAS                             | Requires algorithmic implementation             |
| Scheffé’s Method            | All possible comparisons (non-pairwise)   | Flexible, good for complex comparisons                     | Less powerful for pairwise differences          |


## Reporting Results
- Discuss research question, assumptions, and design.
- Summarize exploratory data analysis.
- Report ANOVA results (F-statistic, degrees of freedom, p-value).
- Describe effect size and any post-hoc analyses.
- Put the conclusion in the context of the problem.


::: {#contrastExample layout-ncol=2}
![](images/notes_7_1_exHandicapContrast_1.png)

![](images/notes_7_2_exHandicapContrast_2.png)
:::

---

::: {#contrastExample layout-ncol=2}
![](images/notes_7_3_exHandicapContrast_3.png)

![](images/notes_7_4_exHandicapContrast_4.png)
:::

---

::: {#contrastExample layout-ncol=2}
![](images/notes_7_5_exHandicapContrast_5.png)

![](images/notes_7_6_exHandicapContrast_6.png)
:::

---

::: {#contrastExample layout-ncol=2}
![](images/notes_7_7_exHandicapContrast_7.png)

![](images/notes_7_8_theoryOfBonferroni_1.png)
:::

---

::: {#contrastExample layout-ncol=2}
![](images/notes_7_9_exMultipleComparison_1.png)

![](images/notes_7_10_exMultipleComparison_2.png)
:::

---

::: {#contrastExample layout-ncol=2}
![](images/notes_7_11_exMultipleComparison_3.png)

![](images/notes_7_12_exMultipleComparison_4.png)
:::

---

::: {#contrastExample layout-ncol=2}
![](images/notes_7_13_exMultipleComparison_5.png)
:::

