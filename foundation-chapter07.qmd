---
title: "Linear Combinations and the Multiple Comparison Problem"
---

## Objectives
- Understand weaknesses of the alternative hypothesis for an ANOVA F-test.
- Formulate appropriate linear combinations of means.
- Understand the multiple comparison problem and how to correct for it.
- Use the formulation to make appropriate inferences.


## Fallacies in Hypothesis Testing

1. **False causality**: A small *p*-value does not indicate causation. Causality can only be inferred from randomized experiments, not observational studies.
2. **Accepting the null**: We should say there is “no evidence of a difference,” not that the null hypothesis is true. There is not enough evidence to suggest the observed difference is due to anything other than chance.
3. **Statistical vs. practical significance**: Statistical significance does not imply practical importance. Evaluate practical significance using power, effect size, and confidence intervals.  
   - Differences that are statistically but not practically significant may arise due to large sample sizes.  
   - If a difference is practically significant but not statistically significant, more data may be needed.
4. **Data dredging**: Avoid fishing for significance (data snooping). This can lead to publication bias against negative results.
5. **Good statistics from bad data**: Biased or non-random data compromise conclusions. Experimental design determines which inferences and statistical methods are valid.


## Linear Combination of Group Means

- ANOVA allows only pairwise comparisons of means and does not account for meaningful structure across groups. In practice, you may wish to compare the average of several means to a single group, or to the average of others.  
- The ANOVA *F*-test serves as an initial screening tool for detecting any overall differences in means. If the *F*-test is significant, follow-up tests can be considered.

### Linear Contrasts
- A **linear combination** of group means is defined as:
  $$
  \gamma = C_1\mu_1 + C_2\mu_2 + \dots + C_I\mu_I
  $$
  where $\gamma$ represents the linear combination and the $C_i$ are coefficients. When the coefficients sum to zero, the combination is called a **contrast**.
- Estimate $\gamma$ using $g = C_1\bar{x}_1 + C_2\bar{x}_2 + \dots + C_I\bar{x}_I$, where $\bar{x}_i$ are group sample means.
- The standard error of $g$, denoted $SE(g)$, is given by:
  $$
  SE(g) = S_p \sqrt{\frac{C_1^2}{n_1} + \frac{C_2^2}{n_2} + \dots + \frac{C_I^2}{n_I}}
  $$
  where $S_p$ is the pooled standard deviation.  
  Even when comparing only two groups (i.e., the other coefficients and terms under the square root are zero), the pooled standard deviation is used under the assumption of equal variances across all groups.
- The *t*-statistic for testing contrasts is:
  $$
  t_{\alpha, df} = \frac{g - \gamma}{SE(g)}
  $$
  where $g$ is the observed linear combination, $\gamma$ is the hypothesized value (e.g., $\gamma = 0$), and $df$ is the degrees of freedom associated with $S_p$.


## Steps for a Linear Contrast Test

1. **Summarize the data**: Obtain group means, sample sizes, and the pooled standard deviation from the ANOVA model (e.g., RMSE).
2. **Specify the coefficients**: Choose $C_i$ values such that $\sum C_i = 0$.
3. **Estimate the contrast**: Compute the linear combination $g$ using the sample means and chosen coefficients.
4. **Compute the standard error**: Calculate $SE(g)$ using the formula above.
5. **Construct confidence intervals**:
   $$
   CI = g \pm t_{\alpha, df} \cdot SE(g)
   $$
6. **Perform the test**: Calculate the *t*-statistic to assess the contrast.

### Simultaneous Inferences
- When testing multiple contrasts or pairwise comparisons, the risk of a **Type I error** increases. Adjustments to significance levels or *p*-values are needed to maintain the overall error rate.
- Distinguish between:
  - **Individual confidence level**: The probability that a single interval captures its parameter ($1 - \alpha$).
  - **Familywise confidence level**: The probability that all intervals simultaneously capture their parameters. This probability is less than $1 - \alpha$ unless corrections are applied.

### Common Applications of Linear Contrasts
1. Comparing average values, e.g., the mean of groups 1 and 2 versus group 3.
2. Comparing response rates across treatments, such as dietary interventions in animals.
3. Testing specific hypotheses about structured combinations of group means.


## Adjusting for Multiple Comparisons

When conducting multiple tests, it is important to control the **familywise error rate** (FWER). Common post-hoc procedures are outlined below.


### Procedures for All Pairwise Differences in Means

1. **Bonferroni adjustment**  
   - Adjusts the significance level:
     $$
     \text{Adjusted } \alpha = \frac{\alpha}{\# \text{ comparisons}}
     $$
   - Example: For 4 groups, there are $\binom{4}{2} = 6$ comparisons:
     $$
     \alpha = \frac{0.05}{6} = 0.0083
     $$
   - **Strengths**: Simple, widely applicable  
   - **Weaknesses**: Very conservative; reduces power
2. **Tukey’s Honestly Significant Difference (HSD)**  
   - Used to construct simultaneous confidence intervals for all pairwise mean differences  
   - Based on the **studentized range statistic**:
     $$
     q = \frac{\bar{X}_{\text{largest}} - \bar{X}_{\text{smallest}}}{\sqrt{\frac{MSE}{n}}}
     $$
     (for groups with equal $n$)  
   - Confidence intervals:
     $$
     (\bar{X}_i - \bar{X}_j) \pm q_{\alpha, (k, N-k)} \cdot \sqrt{\frac{MSE}{n}}
     $$
     - When group sizes are unequal, use the **harmonic mean** of $n_i$ and $n_j$  (as done in SAS):
     $$
     n_{ij} = \frac{2 \cdot n_i \cdot n_j}{n_i + n_j}
     $$
   - **Strengths**: Maintains FWER; easy to interpret  
   - **Weaknesses**: Assumes equal variances (can be extended via Tukey-Kramer)
3. **Ryan-Elliot-Gabriel-Welsch Q (REGWQ) procedure**  
   - Recommended by SAS; balances power and Type I error control  
   - **Method**:
     1. Order the group means in descending order
     2. Reject $H_0$ if:
        $$
        \text{Difference} > q_{\text{critical}} \cdot \text{Studentized range}
        $$
     3. Adjust the *p*-values for the number of comparisons  
   - **Strengths**: High power; nominal Type I error  
   - **Weaknesses**: Algorithmically complex

### Procedures for Pairwise Differences vs. Control
1. **Dunnett’s procedure**  
   - Compares each treatment group to a single control group 
   - Uses the **$D$-statistic**:
     $$
     D = d_{(k-1, N-k)} \cdot \sqrt{\frac{2 \cdot MSE}{n_{\text{harmonic}}}}
     $$
   - Degrees of freedom:
     $$
     df = (k - 1, N - k)
     $$
     where $k - 1$ is the between-groups degrees of freedom and $N - k$ is the within-groups degrees of freedom
   - **Strengths**: Targets control comparisons directly  
   - **Weaknesses**: Not designed for all pairwise tests and limited to control-versus-treatment tests
   
### Procedures for All Possible Comparisons
1. **Scheffé’s method**  
   - Allows for testing *any* linear combination of means 
   - Used for all possible comparisons, including non-pairwise contrasts
   - Based on the *F*-distribution  
   - **Strengths**: Flexible, ideal for complex non-pairwise hypotheses, and less conservative than Bonferroni
   - **Weaknesses**: Less powerful for simple pairwise tests   


## Key Considerations for Post-Hoc Comparisons

1. **Type I error control**  
   Performing multiple tests increases the chance of a false positive (Type I error). Even if all null hypotheses are true, testing too many comparisons increases the likelihood of finding a “significant” result by chance. Post-hoc adjustments help control this overall error risk.
2. **Planned vs. unplanned comparisons**  
   - Use **planned comparisons** when testing specific hypotheses defined in advance  
   - Use **unplanned post-hoc comparisons** when no prior hypotheses were made, but the ANOVA is significant
3. **Decision framework after ANOVA**
   - If the ***F*-statistic is not significant**, no further testing is needed. There is no evidence that group means differ.
   - If the ***F*-statistic is significant**, then post-hoc comparisons are appropriate. The choice of follow-up method depends on:
     - Whether there is meaningful structure in the groups (e.g., contrasts like "average of groups A and B vs. group C")
     - Whether pairwise comparisons or structured contrasts best address your research question

### Summary Table of Post-Hoc Methods

| **Procedure**               | **Purpose**                                | **Strengths**                                                | **Weaknesses**                                   |
|-----------------------------|--------------------------------------------|------------------------------------------------------------|-------------------------------------------------|
| Bonferroni Adjustment       | Controls Type I error for pairwise tests   | Simple, widely applicable                                   | Very conservative, reduced power               |
| Tukey’s HSD                 | All pairwise differences                   | Maintains familywise error rate, easy interpretation        | Assumes equal variances                         |
| Dunnett’s Procedure         | Compare to control group                   | Focused on control vs. treatments                          | Not suitable for all pairwise comparisons       |
| REGWQ Procedure             | Pairwise differences                       | Good power, recommended by SAS                             | Requires algorithmic implementation             |
| Scheffé’s Method            | All possible comparisons (non-pairwise)    | Flexible, good for complex comparisons                     | Less powerful for pairwise differences          |


## Reporting Results

- Discuss the research question, design, and assumptions.
- Summarize the exploratory data analysis (EDA).
- Report ANOVA results: *F*-statistic, degrees of freedom, and *p*-value.
- Describe the effect size and post-hoc analysis used.
- Conclude in the context of the original research question.

---

::: {#contrastExample layout-ncol=2}
![](images/notes_7_1_exHandicapContrast_1.png)

![](images/notes_7_2_exHandicapContrast_2.png)
:::

---

::: {#contrastExample layout-ncol=2}
![](images/notes_7_3_exHandicapContrast_3.png)

![](images/notes_7_4_exHandicapContrast_4.png)
:::

---

::: {#contrastExample layout-ncol=2}
![](images/notes_7_5_exHandicapContrast_5.png)

![](images/notes_7_6_exHandicapContrast_6.png)
:::

---

::: {#contrastExample layout-ncol=2}
![](images/notes_7_7_exHandicapContrast_7.png)

![](images/notes_7_8_theoryOfBonferroni_1.png)
:::

---

::: {#contrastExample layout-ncol=2}
![](images/notes_7_9_exMultipleComparison_1.png)

![](images/notes_7_10_exMultipleComparison_2.png)
:::

---

::: {#contrastExample layout-ncol=2}
![](images/notes_7_11_exMultipleComparison_3.png)

![](images/notes_7_12_exMultipleComparison_4.png)
:::

---

::: {#contrastExample layout-ncol=2}
![](images/notes_7_13_exMultipleComparison_5.png)
:::

