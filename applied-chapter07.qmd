---
title: "Classification"
---

## Objectives

- Understand the distinction between regression and classification.  
- Learn about classification boundaries.  
- Explore classification methods:  
  - k-Nearest Neighbors (KNN)  
  - Linear and Quadratic Discriminant Analysis (LDA/QDA)  
- Evaluate classification performance using error metrics.  


## Regression vs. Classification

Main distinction: The response variable in classification is **categorical**.  
- Binary: Two outcomes (e.g. Yes/No)  
- Multiclass: More than two categories  

### Why Not Use MLR?
- Can't just convert categorical responses into numbers:
  - It doesn't model trends well.
  - It violates assumptions due to the discreteness of the response.  
 

## Predictive Models for Classification

Classification models work in two steps:

1. **Predict the probability** that a categorical response will occur, given a set of explanatory variables: $P(Y = \text{"Default"} \mid X)$
2. **Convert that probability into a classification decision** based on a threshold.

### Comparison
- For continuous responses: $Y = f(X)$
- For categorical responses: $P(Y = \text{"Default"} \mid X) = f(X)$
  - The model predicts probabilities, not the class itself.  
  - Predictions stay between 0 and 1 (unlike MLR, which can go below 0 or above 1).

### Why Predict Probabilities?
- Predictive models give us finer resolution than a simple yes/no.
  - Example: A weather forecast predicting 60% chance of rain is more informative than just "yes" or "no".
- Subtle relationships between predictors and the response lead to variability in predicted probabilities, creating a gray area.
  - For instance:
    - $P(Y = \text{"Default"} \mid X = 100) = 0.82$
    - $P(Y = \text{"Default"} \mid X = 100) = 0.52$
  - These two predictions might both get classified as "Default", even though the model sees them very differently.


## Classification as a Decision Rule

- Classification forces a definitive decision based on the predicted probability:
  $$
  Y = 
  \begin{cases}
  \text{Default}, & \text{if } f(X) \geq \text{threshold} \\
  \text{Not}, & \text{otherwise}
  \end{cases}
  $$
- This means classification doesn’t distinguish between a predicted probability of 0.52 and 0.82 if the threshold is 0.5

### Considerations When Classifying
- The cost of getting the prediction wrong  
- The prevalence of the classes (how often each class occurs in the data)  
 

## Important Realizations

- Assessing how well a model performs is convoluted:
  - How well does the model predict the true probability of the response?
  - How well does your classification rule behave given the costs and context of your problem?

- Metrics commonly reported in software:
  - Often assess classification decisions rather than predicted probabilities  
  - Are calculated without explicit consideration of the cost of mistakes
  - Are often used to assess model fit, even though they may not reflect probability accuracy
 
- It’s up to you to assess whether your decision rule fits your specific context, especially when the costs of different types of errors are not equal.


## Terminology

- Positive class (+): The outcome for which you want to predict the probability  
- Negative class (−): The other possible outcome (in a binary response).  
- These are mathematically arbitrary but can be practically important depending on the question you’re answering.  
    

## Classification Boundaries

### Parametric Models

These models have always been of the **predictive probability** type:
- Logistic Regression  
- Linear Discriminant Analysis (LDA)  
- Quadratic Discriminant Analysis (QDA)  
- Naive Bayes  

**Classification rule:**  
- To convert a predictive probability model into a classification rule, apply a **threshold** (commonly 0.5):
  $$
  \hat{P}(Y = + \mid X) \geq 0.5 \Rightarrow \text{Classify as } +
  $$
- The threshold can be adjusted to account for the cost of making a mistake.


### Nonparametric Models

These models originally focused on making **classification decisions** (not probabilities):
- Decision Trees  
- Random Forests  
- K-Nearest Neighbors (KNN)  

To help standardize functions and workflows, software retroactively added predicted probabilities.

**KNN Classification Rule:**
- Record the response values of the $k$ nearest neighbors to $x_{\text{new}}$
- Estimate: $\hat{P}(Y = + \mid x_{\text{new}}) = \frac{\text{# of } + \text{ neighbors}}{k}$  

**Classification Boundary:**
- Every model, once converted to a classification rule, defines a classification boundary.
- Boundaries help visualize:
  - Parametric vs. nonparametric behavior  
  - Simpler vs. more complex models  
  - Sensitivity to changes in thresholds


## Discriminant Analysis (LDA & QDA)

### Overview
- Discriminant analysis is a parametric approach for classification
- Applies only to numeric predictors.
- Assumes predictors follow a multivariate normal (MVN) distribution within each class.
  
### Common Types
- **Linear Discriminant Analysis (LDA)**:
  - Decision boundaries are linear (hyperplanes)
- **Quadratic Discriminant Analysis (QDA)**:
  - Decision boundaries are conic sections (typically quadratic)

### Assumptions

In discriminant analysis, the assumptions are on the predictors (not the error terms as in MLR).

- $X_+ = (X_1, X_2, \dots, X_p)$ is a set of $p$ numeric predictors for the positive response class.  
- $X_- = (X_1, X_2, \dots, X_p)$ is a set of the same $p$ numeric predictors for the negative response class.  

- **LDA**: $X_+ \sim MVN(\mu_+, \Sigma), \quad X_- \sim MVN(\mu_-, \Sigma)$
  - Equal variance-covariance matrices (same $\Sigma$), i.e. the predictors in each group should have the same variance and correlation.
  - Allows different means, i.e. the predictor sets can have different mean vectors for each response. This is desirable for separation between the classes. 
 
- **QDA**: $X_+ \sim MVN(\mu_+, \Sigma_+), \quad X_- \sim MVN(\mu_-, \Sigma_-)$
  - Covariance matrices differ by class (relaxes assumptions, i.e. allows different $\Sigma$s). 
  - More flexibility, but more parameters to estimate 
  - Also allows different means.

- Technical Insights  
  - LDA/QDA uses Bayes' theorem to perform its predicted probabilities.
  

## Bayes' Theorem & LDA/QDA

Discriminant analysis (LDA/QDA) uses **Bayes’ theorem** to calculate predicted probabilities:

Bayes' theorem for binary classification:
$$
P(+ \mid X) = \frac{P(X \mid +)P(+)}{P(X \mid +)P(+) + P(X \mid -)P(-)}
$$

Updated Bayes' theorem using MVN assumptions:
$$
P(+ \mid X) = \frac{MVN(\mu_+, \Sigma)P(+)}{MVN(\mu_+, \Sigma)P(+) + MVN(\mu_-, \Sigma)P(-)}
$$

- $P(+)$ and $P(-)$ are **prior probabilities** (i.e. class prevalence).
- Classification boundary with threshold = 0.5 results in **linear** (LDA) or **quadratic** (QDA) boundary
- Outliers and class imbalance affect boundaries

### Key Insights
- Bayes’ formula is fully specified using:
  - Prior probabilities
  - Predictor value $X$
  - Class-specific means and covariance matrices  
  - All estimated from the training data

- When using a threshold (e.g. 0.5), LDA’s boundary mathematically reduces to a linear boundary

### Additional Notes on Classification Boundaries
- Outliers affect sample means and variances $\rightarrow$ can distort the boundary.  
- Prior probabilities affect boundary placement, especially when priors are unequal.
- Bayes’ theorem generalizes easily to more than two classes.


## Complexity of QDA

- More parameters: each class gets its own covariance matrix
- The more predictors, the more parameters:
  - In binary classification with $p$ predictors:
    - LDA estimates $p + 1$ parameters. Number of parameters increase at same rate as number of predictors. 
    - QDA estimates $\frac{p(p + 3)}{2} + 1$ parameters  
  - Number of parameters grows much faster than increase in number of predictors.  
  - Greater risk of overfitting, especially with small sample sizes.  
  - Overfitting is mitigated with larger datasets.

## Pros and Cons of LDA/QDA

### Pros
- Handles correlated predictors (i.e as long as the correlation isn't perfect).
- Parametric model that works when logistic regression fails due to complete separation of classes in training data
- Optimal when assumptions are met

### Cons
- Sensitive to outliers.
- Cannot handle categorical predictors due to the MVN assumption.
- No built-in feature selection.
- Suffers from the curse of dimensionality.
  - Poor performance with many irrelevant predictors.
  
---

**Error Metrics for Classification Models**  
- Error metrics can serve different purposes.  
  - Assessing how well the predicted probabilities are to the true probabilities (scoring rules)  
  - Assessing your decision rule to make classifications  

- Log Loss Scoring Rule  
  - Log loss is small when your predicted probabilities are closer to the truth (e.g. predicted prob is 0.9 when truth is yes)  
  - Log loss increases, for example, when probabilities are in the middle and frequency of yes and no are not close to 50%.  

- Brier Scoring Rule  
  - (predicted probability subtracted from truth)^squared -> get sum of squares and take average  
  - mimics the behavior of MSE  
  
- Scoring rules  
  - Log loss (AKA entropy)   
    - Nice mathematical properties that guarantee optimality in certain situations  
    - Logistic regression and trees use it  
  - Briar score  
    - Intuitive like MSE  
    - Decomposes into different sources of error  
  - Ideal to use scoreing rules when:  
    - Comparing models in feature selection  
    - Optimizing tuning parameters (penalized regression, KNN, trees)
    - Estimating regression coefficients in logistric regression  
  - Scoring rules help tune your models, so the predicted probabilities are as close to the true probabilities as possible  
  
- Decision Rule Error Metrics  
  - These metrics are computed by examining your classification decisions  
  - All metrics are derived from a confusion matrix  
  - Important to realize that all confusion matrices depend on the threshold that you pick  
  - Used to compare models/optimize tuning parameters for techniques that do not produce a predicted probability  

- Confusion Matrices  
  - Aggregates and cross references how many times predictions were accurate and how many misclassifications were made using the thresholding strategy.  
  - It will change when the threshold changes (depends on the threshold).  
  - Metrics:  
    - Misclassification error rate (MCR) - global error rate  
      - $MCR = \frac{# Incorrect}{n}$  
    - Accuracy is 1 - MCR or $\frac{# Correct}{n}$. 
      - Accuracy is an unconditional metric; does not take into account any additional info about the classification rule  
    - Conditional metrics compute accuracy with additional information  
      - Sensitivity/specificity  
      - Positive/negative predictive value  
    - Sensitivity: conditions on the fact that you know the observation was a true positive  
      - It is the accuracy of just the true positive class
      - $Sens = \frac{true + correct}{total true +}$
    - Specificity: conditions on the fact that you know the observation was a true negative  
      - It is the accuracy of just the true negative class
      - $Spec = \frac{true - correct}{total true -}$  
    - Positive Predictive Value (PPV): conditions on the fact that you predicted the observation as positive  
      - It is the accuracy among all positive predictions
      - $PPV = \frac{true + correct}{total predicted +}$  
    - Negative Predictive Value (NPV): conditions on the fact that you predicted the observation as negative  
      - It is the accuracy among all negative predictions
      - $NPV = \frac{true - correct}{total predicted -}$  

- Why do we need so many metrics?  
  - If we have a predictive model that predict the probability that person needs a high-risk surgery using clinical and genetic info  
    - + class -> surgery, - class -> no surgery  
    - sensitivity: patients who need surgery, predict 90%
    - specificity: patients who don't need it, predict 90%  
    - those metrics are conditioned on the fact that we know the truth  
    - doc needs to assess if the prediction was positive, how much weight should I give it -> PPV
    - PPV: patients who were predicted to need it, only 47% really did
    - NPV: patients who were predicted to not need it, 98.9% really didn't
    
- Sensitivity and specificity don't take into account the *prevalence* of the +/- classes. Prevalence is the chance of being the + class with no predictor information.  

- Changing the Threshold  
  - Sometimes threshold is chosen to balance sensitivity and specificity.  
  - With a higher threshold, there will be fewer positive predictions made.  

- Takeaways    
  - Thresholding changes the confusion matrix performance metrics  
  - Must consider that the errors we make are not always equal  
  - When it comes to model fitting  
    - The focus should be on thorough EDA and fitting a good model that provides good predicted probability estimates  
    - Once you have a good model, you can start thinking about what threshold to use to meet the needs of the problem
  