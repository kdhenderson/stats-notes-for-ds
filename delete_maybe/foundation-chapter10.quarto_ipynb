{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Regression Diagnostics and Model Refinement\"\n",
        "#execute:\n",
        "#  engine: knitr\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Objectives\n",
        "- Calculate and interpret residuals, standardized residuals, and studentized residuals.\n",
        "- Use residuals to check regression assumptions.\n",
        "- Apply necessary remedies when assumptions are violated.\n",
        "- Understand the robustness of assumptions (i.e. what can you get away with?).\n",
        "\n",
        "\n",
        "## Robustness of Regression Assumptions\n",
        "\n",
        "### Linearity\n",
        "- Parameter estimates will be misleading if a straight-line model is inadequate or fits only part of the data. Predictions will be biased, and confidence intervals (CI) will not appropriately reflect uncertainty.\n",
        "- **Remedy:** Consider adding polynomial terms (quadratic or cubic) to improve model fit.\n",
        "\n",
        "### Normality\n",
        "- Transformations that correct for normality often address constant variance as well.\n",
        "- **Effects of non-normality:**\n",
        "  - **Coefficient estimates & standard errors:** Robust, except with many outliers and small sample sizes.\n",
        "  - **Confidence intervals:** Affected primarily by outliers (long tails).\n",
        "  - **Prediction intervals:** Sensitive to non-normality due to their reliance on normal distributions.\n",
        "\n",
        "### Constant Variance (Homoscedasticity): for every value of x, the spread of y is the same.\n",
        "- Least squares estimates remain unbiased with slight violations. However, large violations cause standard errors to misrepresent uncertainty.\n",
        "- **Remedy:** Large violations should be corrected using a transformation.\n",
        "\n",
        "### Independence\n",
        "- **Parameter estimates:** Not affected by violations.\n",
        "- **Standard errors:** Affected significantly. Smaller standard errors make hypothesis testing misleading (make it easier to reject).\n",
        "- **Remedy:** Serial and cluster effects require different models.\n",
        "\n",
        "### How much deviation from assumptions still give accurate estimates?\n",
        "- Small violations are acceptable.\n",
        "- Transformations can often help.\n",
        "- Only severe departures from linearity or normality (due to outliers) require alternative methods.\n",
        "\n",
        "\n",
        "## Influential and Outlying Observations\n",
        "\n",
        "### Key Concepts\n",
        "- **Influential observations:** Does adding one point change the slope?\n",
        "- **Leverage:** Reflects how far a point is from $\\bar{x}$. Points that are farther have higher leverage.\n",
        "  - Function of distance from $x_i$ to $\\bar{x}$ in standard deviations.\n",
        "  - Function of the proportion of total sum of squares contributed by $x_i$.\n",
        "\n",
        "### Impact of Outliers\n",
        "- **Low leverage, low influence:** Minimal effect on estimates.\n",
        "- **High leverage, low influence:** Far from most data but consistent with trend; minimal effect on correlation, standard error, and estimates.\n",
        "- **High leverage, high influence:** Can distort results by pulling the regression line toward the outlier resulting in different parameter estimates.\n",
        "\n",
        "### How do we detect them? How far is too far?\n",
        "\n",
        "#### Leverage Statistic, $h_{ii}$\n",
        "- Measures the influence of $x$ values on predictions (how far the $x$ values are from the $\\bar{x}$).\n",
        "- **High leverage:** When $h_{ii} > \\frac{2p}{n}$ (where $p$ is the number of parameters).\n",
        "- Calculation:\n",
        "  $$\n",
        "  h_{ii} = \\frac{1}{n} + \\frac{(X_i - \\bar{X})^2}{\\sum_{j=1}^{n} (X_j - \\bar{X})^2}\n",
        "  $$\n",
        "  **or**\n",
        "  $$\n",
        "  h_{ii} = \\frac{1}{n-1} \\left[ \\frac{(X_i - \\bar{X})}{s_x} \\right]^2 + \\frac{1}{n}\n",
        "  $$\n",
        "\n",
        "\n",
        "## Types of Residuals\n",
        "- **Standardized:** $e_i / \\text{RMSE}$\n",
        "- **Studentized:** $e_i / \\sqrt{\\text{MSE} \\cdot (1 - h_{ii})}$\n",
        "- **Studentized-deleted (R-student):**\n",
        "  - Remove an observation, recalculate the regression, and compute the studentized residual.\n",
        "  - Standard deviation is calculated without the point in question.\n",
        "  - Large residual values indicate influential points.\n",
        "  - Formula:\n",
        "    $$\n",
        "    \\text{RSTUDENT} = \\frac{\\text{res}_i}{s_i \\sqrt{1 - h_i}}\n",
        "    $$\n",
        "- **Why different types?** To normalize variance and make residuals comparable.\n",
        "\n",
        "\n",
        "## Leave-One-Out Statistics  \n",
        "$\\hat{Y}_{i(i)}$ is the predicted value of $Y_i$ when the $i$-th observation is left out of the regression model.\n",
        "\n",
        "### PRESS (Predicted Residual Sum of Squares)\n",
        "$$\n",
        "\\text{PRESS}_p = \\sum_{i=1}^n \\left(Y_i - \\hat{Y}_{i(i)}\\right)^2 = \\sum e_{i(i)}^2\n",
        "$$\n",
        "- A smaller PRESS value indicates a better-fitting model.\n",
        "\n",
        "### Cook’s Distance ($D_i$)\n",
        "- Blends residuals and leverage:\n",
        "$$\n",
        "D_i = \\sum_{i=1}^n \\frac{\\left(\\hat{Y}_{i(i)} - \\hat{Y}_i\\right)^2}{p \\cdot \\text{MSE}} = \\frac{1}{p} (\\text{studres}_i)^2 \\left[\\frac{h_i}{1 - h_i}\\right]\n",
        "$$\n",
        "- $p =$ number of parameters.\n",
        "\n",
        "\n",
        "## Durbin-Watson Test for Independence\n",
        "$$\n",
        "d = \\frac{\\sum_{i=1}^n (e_i - e_{i-1})^2}{\\sum_{i=1}^n e_i^2}\n",
        "$$\n",
        "- Values near 0 indicate positive correlation; values near 4 indicate negative correlation. Distributed symmetrically about 2.\n",
        "- Available in R and SAS.\n",
        "\n",
        "\n",
        "## Recap\n",
        "\n",
        "| **Name**                | **Expression**                                       | **Use**                        |\n",
        "|-------------------------|-----------------------------------------------------|--------------------------------|\n",
        "| **Residual**            | $e_i = y_i - \\hat{y}_i$                              | Residual plots                |\n",
        "| **Standardized residual** | $r_i = \\frac{e_i}{s \\sqrt{1 - h_{ii}}}$            | Identify outliers             |\n",
        "| **Studentized residual** | $t_i = \\frac{e_i}{s_{(i)} \\sqrt{1 - h_{ii}}}$       | Test outlying $Y$’s           |\n",
        "| **Deleted residual**     | $e_{i(i)} = y_i - \\hat{y}_{i(i)} = \\frac{e_i}{1 - h_{ii}}$ | Calculate PRESS               |\n",
        "\n",
        "> At $\\alpha = 0.05$, we expect 5% of studentized residuals to be greater than 2 or less than -2.\n",
        "\n",
        "![Residual Panel](images/notes_10_1_residual_panel.png)\n",
        "\n",
        "\n",
        "## Graphical Assessment of Residuals\n",
        "\n",
        "| **Pattern**             | **Potential Issue** | **Solution** |\n",
        "|-------------------------|--------------------|-------------|\n",
        "| Linear means, constant SD | Model fits well | No action needed |\n",
        "| Curved means, equal SD | Nonlinearity | Transform $X$ |\n",
        "| Curved means, increasing SD | Nonlinearity + heteroscedasticity | Transform $Y$ |\n",
        "| Skewed residuals | Non-normality | Can model mean, but CIs/PIs unreliable. Consider transformations. |\n",
        "| Linear means, increasing SD | Heteroscedasticity | Use weighted regression |\n",
        "\n",
        "![Regression Patterns](images/notes_10_2_regression_patterns.png){width=5in fig-align=\"left\"} \n",
        "\n",
        "\n",
        "## Transformation Recommendations\n",
        "1. Log\n",
        "2. Square root\n",
        "3. Other\n",
        "\n",
        "### Interpretation Considerations\n",
        "- Adjust wording for the audience and transformed variables.\n",
        "- Back-transform results if needed.\n",
        "\n",
        "\n",
        "## Remedies for Violations\n",
        "\n",
        "### Nonlinearity: \n",
        "- More complicated models\n",
        "- Transformation on X\n",
        "- Add another variable (might help nonconstant variance too)\n",
        "\n",
        "### Nonconstant variance:\n",
        "- Transformation on Y\n",
        "- Weighted least squares – down weight observations with larger variance, so they don’t influence the regression model as much as observations closer to the line\n",
        "\n",
        "### Correlated errors: In residual plots, you may see small values follow small values, and large follow large.\n",
        "- Serial effects within data -> time series or spatial models\n",
        "\n",
        "### Outliers\n",
        "- Use robust regression procedures\n",
        "- Check for data entry or other errors (only delete in this situation)\n",
        "\n",
        "### Non-normality\n",
        "- Fix last, usually fixed with the above\n",
        "- Transform the data\n",
        "\n",
        "![Residual Patterns](images/notes_10_3_regression_patterns.png){width=6in fig-align=\"left\"} \n",
        "Source: @ramsey2012statistical.\n",
        "\n",
        "\n",
        "## Log Transformation: They Work & Are Easy to Interpret\n",
        "\n",
        "- **Log on response (log-linear):**  \n",
        "  $\\log\\hat{Y}_i = \\beta_0 + \\beta_1 X_i$\n",
        "  - Mean of the response is log-linearly related to the explanatory variable.\n",
        "  - $\\text{Median}\\{Y|X\\} = \\exp(\\beta_0) \\exp(\\beta_1 X)$\n",
        "  - One unit increase in $X$ → multiplicative change in $\\exp(\\beta_1)$ in the median of $Y|X$.\n",
        "  - $\\text{Median}\\{Y|(X + 1)\\} / \\text{Median}\\{Y|X\\} = \\exp(\\beta_1)$\n",
        "- **Log on explanatory variable (linear-log):**  \n",
        "  $\\hat{Y}_i = \\beta_0 + \\beta_1 \\log X_i$\n",
        "  - Depends on log base (e.g. $2$ → doubling, $10$ → ten-fold).\n",
        "- **Both logged (log-log):**  \n",
        "  $\\log\\hat{Y}_i = \\beta_0 + \\beta_1 \\log X_i$\n",
        "  - **Complicated** :/\n",
        "  - Doubling (or tenfold increase) of $X$ is associated with a change of $2^{\\beta_1}$ (or $10^{\\beta_1}$) in the median of $Y$.\n",
        "\n",
        "\n",
        "## Interpretations of Log Transformation Varieties\n",
        "\n",
        "- **Log-linear model:**  \n",
        "  A one unit increase in $X$ is associated with a multiplicative change of $e^{\\beta_1}$ in the median of $Y|X$.\n",
        "- **Linear-log model:**  \n",
        "  A doubling of $X$ is associated with a $\\beta_1 \\log(2)$ unit change in the mean of $Y|X$.\n",
        "- **Log-log model:**  \n",
        "  A doubling of $X$ is associated with a $2^{\\beta_1}$ multiplicative change in the median of $Y|X$.\n",
        "- **Remember:** Logging $Y$ → median.\n",
        "\n",
        "\n",
        "::: {layout-ncol=2}\n",
        "![Log-Linear Transformation](images/notes_10_4_loglinear.png)\n",
        "\n",
        "![Linear-Log Transformation](images/notes_10_5_linearlog.png)\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "::: {layout-ncol=2}\n",
        "![Log-Log Transformation](images/notes_10_6_loglog.png)\n",
        ":::\n",
        "\n",
        "## Formal Test for Lack of Fit\n",
        "- **Use:** different values of Y for replicated observations of X\n",
        "- **F-test for lack of fit** has the usual assumptions:\n",
        "  - Normality of $Y|X$,\n",
        "  - Independence of $(X, Y)$ pairs,\n",
        "  - Constant variance of $Y$ across all values of $X$.\n",
        "\n",
        "- **Procedure**:\n",
        "  1. Fit a linear regression and obtain $SS_{\\text{res}_{LR}}$ (linear fit).\n",
        "  2. Fit an ANOVA and use replicated $X$ values as the treatment variable to obtain $SS_{\\text{res}_{SM}}$ (separate means model).\n",
        "  3. **Null hypothesis**: The linear model fits.  \n",
        "     **Alternative**: Variability cannot be explained by the model.\n",
        "  4. Compute:\n",
        "     $F = \\frac{(SS_{\\text{res}_{LR}} - SS_{\\text{res}_{SM}}) / (\\text{df}_{LR} - \\text{df}_{SM})}{\\text{MSE}_{SM}}$\n",
        "  5. In the example: Reject $\\rightarrow$ Cubic fit, $y' = b_0 + b_1x + b_2x^2 + b_3x^3$.\n",
        "- **Guidance**:\n",
        "  - Even if the model fits well, it may not be (and probably isn’t) the best model.\n",
        "  - **Principle of parsimony**: Find the simplest model (i.e. the model with the smallest number of predictors) that explains the most variation.\n",
        "\n",
        "\n",
        "## Strategy of the Lack of Fit Test\n",
        "- ANOVA compares the equal means model to a linear regression model.\n",
        "- Lack of fit test – when it fails to reject, the model is comparable with the best fitting model.\n",
        "\n",
        "\n",
        "### Example in SAS:\n",
        "\n",
        "\n",
        "\n",
        "```{sas eval=FALSE}\n",
        "/* linear regression model */\n",
        "proc glm data = IronCor;\n",
        "model Corrosion = IronContent / solution;\n",
        "run;\n",
        "\n",
        "/* separate means model (7 groups) */\n",
        "proc glm data = IronCor;\n",
        "class IronContent;\n",
        "model Corrosion = IronContent;\n",
        "run;\n",
        "\n",
        "data critval;\n",
        "criticalValue = finv(0.95, 5, 6);\n",
        "run;\n",
        "proc print data = critval;\n",
        "run;\n",
        "\n",
        "data pval;\n",
        "pValue = 1-probf(9.28, 5, 6);\n",
        "run;\n",
        "proc print data = pval;\n",
        "run;\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "::: {layout-ncol=2}\n",
        "![](images/notes_10_7_sas1.png){width=5in fig-align=\"left\"}\n",
        "\n",
        "![](images/notes_10_8_sas2.png){width=5in fig-align=\"left\"}\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "::: {layout-ncol=2}\n",
        "![](images/notes_10_9_extrasumofsquares.png){width=5in fig-align=\"left\"}\n",
        "\n",
        "![](images/notes_10_10_sas3.png){width=5in fig-align=\"left\"}\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "::: {layout-ncol=2}\n",
        "![](images/notes_10_11_sas4.png){width=5in fig-align=\"left\"}\n",
        ":::\n",
        "\n",
        "- $H_0$: Linear regression model has a good fit. (No lack of fit.)\n",
        "- $H_A$: The SMM fits better. (LRM is lacking fit.)\n",
        "- There is strong evidence to suggest the linear regression model has a lack of fit with respect to the separate means model. (It is not comparable with Messi.)\n",
        "\n",
        "\n",
        "## References"
      ],
      "id": "11e3b7f5"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}