---
title: "Inferences for Multiple Linear Regression"
---

## Objectives

- Interpret inferential statistics for various types of regression models, including those with indicator variables, quadratic terms, and interaction terms.
- Interpret and communicate results from a multiple regression analysis.


## Multiple Linear Regression (MLR) with a Single Predictor: Polynomial Regression

- Still a linear model:
  - Captures curvilinear trends.
  - Linear in the $\beta_i X^j$ term: for every one-unit increase in $X^j$, $Y$ changes by a constant $\beta_i$. 
  - If the quadratic coefficient is significant but the linear term is not, still include the linear/lower-order terms as they form a singular idea.
  - Interpretation can be difficult, especially with higher-order terms.

- Interpretation of a quadratic model:
  - Find the vertex (absolute max/min) using:$x_{\text{vertex}} = \frac{-\beta_1}{2\beta_2}$
  - This suggests that the mean $X$ at which $Y$ stops increasing or decreasing is given by $-\beta_1 / 2\beta_2$ based on a quadratic regression model.


## Coefficient of Determination: $R^2$

- A measure of effect size for both SLR and MLR.
- Represents the proportion of variance in the response variable explained by the model.

### Calculation in MLR:
- **Multiple $R^2$:** Find the squared correlation between observed $Y$ and predicted $Y$.
- **Partition of sums of squares:**
  $$
  R^2 = \frac{\text{SS}_{\text{total}} - \text{SS}_{\text{residual}}}{\text{SS}_{\text{total}}} = 1 – \frac{\text{SS}_{\text{residual}}}{\text{SS}_{\text{total}}}
  $$

### Relationship with the Overall F-Test:
- The F-test statistic can be rewritten in terms of $R^2$.
- Testing if all parameters equal zero is equivalent to testing if $R^2 = 0$.

### $R^2$ and Parsimony:
- Adding more predictors increases $R^2$.
- An oversaturated model can have $R^2 = 1$ even if predictors do not contribute meaningfully.
- The goal is a parsimonious model—one that is as simple as possible while still maximizing $R^2$.

### Adjusted $R^2$:
- Accounts for the number of predictors ($k$) and sample size ($n$).
- Formula:
  $$
  R^2_{\text{adj}} = R^2 – (1 - R^2) \frac{k}{n-k-1}
  $$
- Penalizes unnecessary predictors and small sample sizes. Rewards variables that significantly improve model fit, resulting in a net positive if a predictor variable explains a lot of the variance.
- Is a better measure of effect size in MLR.

---

## Overfitting

- $R^2$ can always be made 1.
- It indicates how well the model fits the sample data (noise and signal).
- It isn’t an indictor of how well the model fits a new sample from the same population.


## ”Controlling” for other variables in the model

- For a fixed $X_2$ value (looking at a subpopulation of $X_2$ of similar values), we can see what happens to $Y$ as as $X_1$ changes. 


## Overall F-Test

- Tests if the model itself is significant
- $H_0$: all slopes are equal to 0 (i.e. $R = 0$).
- $H_a$: at least one slope is not equal to 0.
- F-statistic is the test statistic:
$$
F = \frac{\text{SS}_{\text{regression}} / k}{\text{SS}_{\text{residual}} / [n - (k+1)]} = \frac{\text{MS}_{\text{regression}}}{\text{MS}_{\text{error}}}
$$
- Can be written as a ratio of $R^2$.


## Significance test for each predictor (t-tests)

- $H_0: \beta_i = 0$
- $H_a: \beta_i \neq 0$
- The test statistic: $t = \frac{b_i}{SE_{b_i}}, \quad \text{with } df = n - k - 1$
- Standard error calculation: $SE = \frac{\hat{\sigma}}{\sqrt{\sum_{i=1}^{n} (X_i - \bar{X})^2}}, \quad \hat{\sigma} = \sqrt{\frac{\text{SSR}}{(n - k - 1)}}$


## Prediction in MLR

- Predicted values are the same as estimated means
- To get confidence (predicted mean) and prediction (individual prediction) intervals, use software because all variables need to be in the model.


## Partial F-Test (extra-sum-of-squares)

- When some of $k$ predictors in a complete model don’t have statistically significant slopes, we look at a reduced model with $g$ of $k$ predictors.
- $H_0$: all $k – g$ slopes are equal (to each other and) to 0.
- $H_a$: at least one slope is not equal to 0.
- Simultaneous test that extra $k - g$ predictors aren’t necessary.


## Least Squares Estimates and Standard Errors

- Least squares estimates of the $\beta$s are in the coefficient column of the parameter estimate table.
-The estimate of $\sigma^2$ is the $\frac{\text{sum of squared residuals (sum of squared error (SSE))}}{\text{the df (n-p): MSE}}$.
- The sq rt of the estimate of variance or estimate of $\sigma^2$ is the estimated st. dev. about the regression??. This is called residual SD, residual SE (in R), RMSE.


## Example

![](images/notes_12_1.png){width=5in fig-align="left"}  
![](images/notes_12_2.png){width=5in fig-align="left"}  
![](images/notes_12_3.png){width=5in fig-align="left"}
![](images/notes_12_4.png){width=5in fig-align="left"}
![](images/notes_12_5.png){width=5in fig-align="left"}
![](images/notes_12_6.png){width=5in fig-align="left"}
![](images/notes_12_7.png){width=5in fig-align="left"}