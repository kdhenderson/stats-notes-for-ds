---
title: "Inferences for Multiple Linear Regression"
---

## Objectives

- Interpret inferential statistics for various types of regression models, including those with indicator variables, quadratic terms, and interaction terms.
- Interpret and communicate results from a multiple regression analysis.


## Multiple Linear Regression (MLR) with a Single Predictor: Polynomial Regression

- Still a linear model:
  - Captures curvilinear trends.
  - Linear in the $\beta_i X^j$ term: for every one-unit increase in $X^j$, $Y$ changes by a constant $\beta_i$. 
  - If the quadratic coefficient is significant but the linear term is not, still include the linear/lower-order terms as they form a singular idea.
  - Interpretation can be difficult, especially with higher-order terms.

- Interpretation of a quadratic model:
  - Find the vertex (absolute max/min) using:$x_{\text{vertex}} = \frac{-\beta_1}{2\beta_2}$
  - This suggests that the mean $X$ at which $Y$ stops increasing or decreasing is given by $-\beta_1 / 2\beta_2$ based on a quadratic regression model.


## Coefficient of Determination: $R^2$

- A measure of effect size for both SLR and MLR.
- Represents the proportion of variance in the response variable explained by the model.

### Calculation in MLR:
- **Multiple $R^2$:** Find the squared correlation between observed $Y$ and predicted $Y$.
- **Partition of sums of squares:**
  $$
  R^2 = \frac{\text{SS}_{\text{total}} - \text{SS}_{\text{residual}}}{\text{SS}_{\text{total}}} = 1 – \frac{\text{SS}_{\text{residual}}}{\text{SS}_{\text{total}}}
  $$

### Relationship with the Overall F-Test:
- The F-test statistic can be rewritten in terms of $R^2$.
- Testing if all parameters equal zero is equivalent to testing if $R^2 = 0$.

### $R^2$ and Parsimony:
- Adding more predictors increases $R^2$.
- An oversaturated model can have $R^2 = 1$ even if predictors do not contribute meaningfully.
- The goal is a parsimonious model—one that is as simple as possible while still maximizing $R^2$.

### Adjusted $R^2$:
- Accounts for the number of predictors ($k$) and sample size ($n$).
- Formula: $R^2_{\text{adj}} = R^2 – (1 - R^2) \frac{k}{n-k-1}$
- Penalizes unnecessary predictors and small sample sizes. Rewards variables that significantly improve model fit, resulting in a net positive if a predictor variable explains a lot of the variance.
- Is a better measure of effect size in MLR.


## Overfitting

- $R^2$ can always be made 1, but that does not indicate good predictive power.
- Overfitting means the model fits both noise and signal in the sample data.
- A high $R^2$ does not guarantee that the model generalizes well to new data.


## Controlling for Other Variables in MLR

- When holding $X_2$ constant (i.e. looking at a subpopulation with similar $X_2$ values), we can examine the effect of changes in $X_1$ on $Y$.


## Overall F-Test

- Tests whether the model is statistically significant.
- **Hypotheses:**
  - $H_0$: All slopes are equal to 0 (i.e. $R^2 = 0$).
  - $H_a$: At least one slope is not zero.
- **F-statistic formula:** $F = \frac{\text{SS}_{\text{regression}} / k}{\text{SS}_{\text{residual}} / (n - k - 1)} = \frac{\text{MS}_{\text{regression}}}{\text{MS}_{\text{error}}}$
- Can also be expressed in terms of $R^2$.


## Significance Tests for Each Predictor (t-tests)

- **Hypotheses:**
  - $H_0: \beta_i = 0$ (predictor has no effect)
  - $H_a: \beta_i \neq 0$ (predictor has an effect)
- **Test statistic:** $t = \frac{b_i}{SE_{b_i}}, \quad \text{with } df = n - k - 1$
- **Standard error calculation:** $SE = \frac{\hat{\sigma}}{\sqrt{\sum_{i=1}^{n} (X_i - \bar{X})^2}}, \quad \hat{\sigma} = \sqrt{\frac{\text{SSR}}{(n - k - 1)}}$


## Confidence Intervals for Linear Combinations of Coefficients

- 95% CI for difference in intercepts:
  $$
  (\hat{\beta}_2 + \hat{\beta}_3) - \hat{\beta}_0
  $$
  - To find the SE:
    $$
    \text{Var}(\hat{\beta}_2 + \hat{\beta}_3) = \text{Var}(\hat{\beta}_2) + \text{Var}(\hat{\beta}_3) + 2 \text{Cov}(\hat{\beta}_2, \hat{\beta}_3)
    $$
  - Compute the CI:
    $$
    (\hat{\beta}_2 + \hat{\beta}_3) \pm t_{0.975, df} \times SE(\hat{\beta}_2 + \hat{\beta}_3)
    $$

![Confidence Interval Example](images/notes_12_6.png)


## Variance of Sum and Difference of Random Variables

- Formula:
  $$
  \text{Var}(aX + bY) = a^2 \text{Var}(X) + b^2 \text{Var}(Y) + 2ab \text{Cov}(X, Y)
  $$
- Used to compute the variance of estimates in regression.

![Variance Calulation](images/notes_12_5.png)


## Interpreting p-values in Regression Output

- Example p-value interpretation for testing equality of mean energy expenditures:
  - $p = 0.7030$ suggests no significant difference.


## Prediction in MLR

- Predicted values are the same as estimated means
- To get confidence (predicted mean) and prediction (individual prediction) intervals, use software because all variables need to be in the model.


## Partial F-Test (Extra-Sum-of-Squares)

- Used when a subset of $k$ predictors does not have statistically significant slopes, i.e. to examine a reduced model with $g$ of $k$ predictors.
- **Hypotheses:**
  - $H_0$: The extra $k-g$ predictors do not contribute (all their slopes are 0).
  - $H_a$: At least one of the extra predictors has a nonzero slope.
- Simultaneously tests whether additional predictors improve the model.

![Extra-Sum-of-Squares F-Test Example](images/notes_12_1.png)
![Extra-Sum-of-Squares F-Test Example](images/notes_12_2.png)
![Extra-Sum-of-Squares F-Test Example](images/notes_12_3.png)
![Parameter Estimates and Standard Errors](images/notes_12_4.png)


## Testing Intercept Differences in Regression Models

- Testing whether intercepts differ between bat types:
  - $H_0: \beta_3 = 0$ (no difference)
  - $H_a: \beta_3 \neq 0$ (significant difference)

![Testing Intercepts for Bat Types](images/notes_12_7.png)


## Least Squares Estimates and Standard Errors

- Least squares estimates of the $\beta$s are in the coefficient column of the parameter estimate table.
-The estimate of $\sigma^2$ is the $\frac{\text{sum of squared residuals (sum of squared error (SSE))}}{\text{the df (n-p): MSE}}$.
- The sq rt of the estimate of variance or estimate of $\sigma^2$ is the estimated st. dev. about the regression??. This is called residual SD, residual SE (in R), RMSE.

