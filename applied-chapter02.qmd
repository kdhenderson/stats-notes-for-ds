---
title: "The Bias Variance Trade-off"
---

## Objectives

- Challenges with building models:
  - The bias-variance trade-off
  - Feature selection


## Challenges with Building Models

When faced with a large number of possible predictor variables, deciding where to start and how to choose can be challenging. Important considerations include:
- Many predictors may not be relevant.
- In some domains, such as medicine, prediction accuracy is critical.
- Sample size limitations—having many variables but a small sample size—can lead to overfitting.

## Model Reproducibility

For regression models, the most common accuracy measure is **mean squared error (MSE)**, calculated as the sum of squared error divided by the sample size.
$$ MSE = \frac{\sum (y_i - \hat{y}_i)^2}{n} $$

### Pitfalls of Using Training Data for MSE
If we calculate MSE on the same data used to build the model:
1. The MSE values are unreliable for comparing models:
   - They favor overly complex models.
   - They can be driven artificially to zero.
2. There is no guarantee that MSE will generalize to new data.

### Ensuring Reproducibility
A common approach is:
1. Splitting the data into a **training set** (70-80%) and a **validation set**.
  - Another best practice is to split the data into a training and holdout testing set, and then split the training set into a training and validation set for CV and/or hyperparameter tuning.
2. Using the training set to fit the model.
3. Predicting on the validation set and computing the validation MSE.

**Metrics like $R^2$, Adjusted $R^2$, and Information Criteria:**
- These are computed on the training data and tend to favor complex models.
- Preferred metrics derived from the training data include AIC and BIC for model selection.

**Key Takeaways:**
1. Model evaluation should always be based on an independent validation dataset.
2. Plotting validation MSE for models from least to most complex helps identify the best fit without performance issues.


## Bias-Variance Trade-Off
### Understanding the Trade-Off
Comparing models from least to most complex:
- **Training MSE** always decreases with complexity.
- **Validation MSE** follows a U-shape (this is the **bias-variance trade-off**).

### Decomposing MSE
The expected validation MSE can be decomposed as:
$$ E[\text{MSE}_{\text{valid}}] = \text{Var}(\hat{f}) + \text{Bias}(\hat{f})^2 + \text{Var}(\epsilon) $$
where:
- $ \text{Var}(\hat{f}) $ measures how much the model changes across different datasets.
- $ \text{Bias}(\hat{f}) $ measures how far the model's predictions are from the true values on average.
- $ \text{Var}(\epsilon) $ is the irreducible error due to natural variation (variation around true trend line).

### Scenarios:
1. **High Bias / Low Variance:** Simple models (e.g. linear regression on a cubic relationship) consistently make errors (bias) but are stable across datasets.
2. **Moderate Bias / Moderate Variance:** Quadratic fits for a cubid relationship still have bias but are more stable (fits are consistent).
3. **Low Bias / Low Variance:** Cubic fits (the true model) perform well and are stable.
4. **No Bias / Moderate Variance:** Overly complex models predict well on average but vary significantly across datasets. Poor generalization results in poor accuracy. 
5. **No Bias / High Variance:** Extreme complexity leads to models that fit each dataset uniquely but fail to generalize.

**Overfitting:** When a model has poor validation MSE due to high variance. It fits the training dataset well, but it can't generalize to other datasets.

**Underfitting:** When a model has high bias and fails to capture the true trend.


## K-Fold Cross Validation
### Issues with Train/Validation Splitting
1. Small datasets may not allow for a reliable split.
2. Validation MSE varies depending on the chosen train/validation sets.

### K-Fold Cross Validation Process
1. Partition the dataset into $k$ disjoint subsets.
2. For each subset (fold):
   - Leave it out as a test set.
   - Train on the remaining $k-1$ folds.
   - Compute validation MSE.
3. Compute the average validation MSE.
Notes: In practice, typically 5-10 folds are used to keep computational cost down.

### Advantages of K-Fold Cross Validation
- Provides a better estimate of true MSE.
- Reduces dependence on any single train/validation split.
- Commonly implemented in statistical software to assess the bias-variance trade-off.

#### Leave-One-Out Cross Validation (LOOCV)
- A special case where $k=n$ (each observation is left out once), which is common in statistical methodology.
- Computationally expensive but useful for very small datasets.


## Feature Selection via Penalized Regression
### Why Feature Selection?
- Feature selection helps balance the bias-variance trade-off by reducing complexity while maintaining predictive power.
- It is the process of using an automated procedure to determine what explanatory variable should or should not be in a model.

### Feature Selection Methods of Modeling Tools
When using a new tool, it's important to consider if and how it handles feature selection.
- **Multiple Linear Regression (MLR):** Forward/backward/stepwise selection and penalized regression
- **Tree-based methods (Random Forests):** Built-in feature selection
- **K-Nearest Neighbors (KNN):** No direct feature selection

---

Feature selection with MLR
- Stepwise feature selection was the original approach. Sequentially add or remove predictors looking for the best subset, but usually comparing model fits with some criteria (minimize AIC, BIC, validation MSE, or k-fold CV)
- 4 main versions: forward, backwards, best subset, hybrids
- effectively forces some of the coefficients to be zero so the predictor gets multiplied by zero and removed from the model

Penalized Regression
Three common tools that use penalized regression:
1. Ridge (older)
2. Lasso
3. GLM-NET

Diff between regular regression and penalized regression
- In regular regression, coefficients are estimated by minimizing the residual sum of squares of the training data.
- Penalized estimates them by minimizing RSS+penalty term. The penalty term is controlled by its own parameter, lambda. The coefficient of the slope changes based on the penalty term. The stronger the penalty, the closer the slope gets to 0.

With a low penalty, all the coefficients are turned on, similar to regular MLR. As the penalty gets stronger, the coefficients are dampening down.

Main idea of penalized regression
- overly complicated models will have an artificially low train MSE
- by adding a penalty term, we bias the coefficients towards 0. will force the fit to be worse on the training set, but should reproduce better on future data sets because it isn't so dependent on training data
- some coefficients will be zero so they will be removed from the model entirely
- final question: how do you pick the penalty term?

Penalty control complexity - stronger the penalty, smaller/simpler model

How to pick penalty
- k-fold CV used to determine the optimal penalty and compare an MSE plot

Technically, the penalty is a function of the penalty term and the coefficient terms. A larger penalty term with all the coefficients, will make the penalty (RSS+penalty) bigger and bigger. The only way to to reduce the residual sum of squares overall is to get rid of some of the coefficients.

When alpha (the mixing parameter) = 1, GLM-NET becomes the lasso method. The penalty equation simplifies. 

When alpha = 0, GLM-NET becomes the ridge method. 


# Recap and Consideration

Considerations when using feature selection and the penalized regression strategy:
- bias-variance trade off provides insight into why some models predict better than others on future data sets
- validation set and k-fold validation are both a means of comparing models and assessing the bias-variance trade off
- penalized regression provides a procedure to drop unimportant predictor variables from a model

Which tool should you use?
- penalized works well when you know a lot of the variables considered for the model should not be important
- Specifics:
  - Lasso: only true feature selection approach; dampens VIFs
  - Ridge: never truly forces coefficients to 0; still dampens VIFs
  - If you want to use penalized regression specifically for feature selection, it makes sense to use Lasso over GLM-NET and Ridge, because with those two features are not forced out and they will get left in
- Feature selection usually helps with multicollinearity
- Forward, backward and stepwise are still valid
  - criteria used should be based on AIC, a validation set or k-fold CV
  - it shouldn't be based on p-values or r-squared
  
The algorithm is a tool:
- it is still up to you to add complexity terms.
- graphically access for polynomial fit and interaction terms to fit different slopes
- EDA is still important

(Hypothesis) Testing after feature selection
- penalized regression has limitations
  - coefficients are there, but there aren't t-stats, p-values or CIs
- common approach
  - make note of which predictors were dropped
  - run a new MLR using only relevant predictors
  - produce p-values, confidence intervals, etc
- conducting tests after feature selection
  - interpretation isn't conservative enough, CIs are too narrow, p-values are too small
  - it's multiple testing issues (family-wise error rate: the chance of making a type one error goes up)
  
Solutions
- treat feature selection and hypothesis testing as a train/validation procedure (lowers sample size, harder to get significant coefficients)
  - preform feature selection on the "train" data set
  - use the "validation set" to fit your model decided by feature selection; report the p-values and conclusions from this model

- newer method exists
  - R package selection inference
  - good statistical inference properties using LASSO
  

- You still have to check model assumptions
  