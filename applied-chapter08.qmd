---
title: "2 x 2 Contingency Tables"
---

## Overview

This chapter introduces contingency tables as a bridge between group comparisons and logistic regression models.

- Understand the role of 2×2 tables in modeling binary outcomes  
- Compare group proportions using appropriate statistical metrics  
- Learn when and how to apply Fisher’s exact test  
- Use chi-squared tests for large-sample inference

## Classification Tools

- **Linear and Quadratic Discriminant Analysis (LDA/QDA)**  
- **k-Nearest Neighbors (KNN)**  
- **Logistic Regression**
  - A parametric method with strong interpretability.
  - Analogous to multiple linear regression, but for binary categorical outcomes.


## Building Up to Multiple Linear Regression (MLR)

| Explanatory Variable           | Method                        |
|--------------------------------|-------------------------------|
| One categorical variable       | t-tests (2 groups)            |
|                                | ANOVA (3+ groups)             |
| One numeric                    | Simple linear regression      |
| Mix of categorical and numeric | Multiple linear regression    |


## Building Up to Logistic Regression

| Explanatory Variable           | Method                             |
|--------------------------------|------------------------------------|
| One categorical variable       | 2x2 contingency tables (2 groups)  |
|                                | Logistic regression (3+ groups)    |
| One numeric                    | Simple logistic regression         |
| Mix of categorical and numeric | Multiple logistic regression       |


## Understanding 2x2 Contingency Tables

The main purpose is to compare the probability of a response outcome between two groups.  

- Are smokers more likely to get cancer than non-smokers?
  - Explanatory variable: smoker/non-smoker  
  - Response variable: cancer/no cancer  

There are three typical data collection designs:

1. **Prospective**  
2. **Retrospective**  
3. **Completely observational**  

### Prospective Studies
- Populations for each level of the explanatory variable are determined in advance.
  - This may happen naturally (e.g., subjects are obese or not obese) or through random assignment (e.g., placebo vs. treatment).
- Simple random samples are taken within each group.
- **Row totals are fixed** in advance---we know what the sample size will be for each group.
- The explanatory variable is fixed, and we sample the response after some follow-up period.
- Example: Vitamin C and Colds study
- Randomized experiments are a special type of prospective study:
  - Subjects are randomly assigned to predictor groups.
  - Helps mitigate confounding.
  - Allows for causal conclusions.
  - Equal row totals often suggest a prospective study and randomized design.  
  
### Retrospective Studies
- Reverse of prospective: the **response variable is fixed** in advance.
- Samples are selected based on response status; **column totals** are fixed.
- We determine the explanatory variable after selecting individuals.
- Example: Cancer and smoking status study  
- This approach is often used when:
  - Ethical concerns prevent random assignment (e.g., assigning people to smoke).
  - Long follow-up periods are impractical.

### Observational Studies
- Only the grand total may be fixed---or no totals are fixed at all.
- Researcher has little to no control over the sampling.
- Potential for confounding.
- There may be no clearly defined response or predictor, making the study more about **association** than group comparison.

---

## Parameters in 2x2 Tables

Goal: Compare two groups using either mean differences or proportion differences.

### Mean difference (t-tests)
- Parameters:  
  - $\mu_1$: mean of the response for group 1  
  - $\mu_2$: mean of the response for group 2  
- Hypotheses:  
  - $H_0: \mu_1 = \mu_2$
  - $H_a: \mu_1 \neq \mu_2$
  - 95% CI: $\mu_1 - \mu_2$
    - If 0 is not in the interval, it supports $H_a$  

### Proportion difference
- Parameters:  
  - $\pi_1$: probability of event in group 1  
  - $\pi_2$: probability of event in group 2  
- Hypotheses:  
  - $H_0: \pi_1 = \pi_2$  
  - $H_a: \pi_1 \neq \pi_2$
  - 95% CI: $\pi_1 - \pi_2$ 
    - If 0 is not in the interval, it supports $H_a$ 

  
## Problems with Absolute Proportion Differences

- Even small absolute differences can be practically important:
$$\pi_1 - \pi_2 = 0.05 - 0.01 = 0.04$$
- The confidence interval may suggest a small estimated difference.
- This is an absolute difference.
- But it ignores relative scale:
  - A 5% event rate is 5 times higher than a 1% event rate!   

    
## Two Additional Metrics

1. **Odds Ratio**
   - Great for retrospective studies
   - Captures relative difference
   - Can be harder to understand intuitively and interpret
2. **Relative Risk**
   - Also a relative difference metric   
   - More intuitive to interpret
   - Often confused with odds ratios

### Odds Example
- Suppose the odds of getting cancer are 1 to 2 → $\omega = \frac{1}{2} = 0.5$
  - Odds are not a probability because the denominator is not the total. 
- Probability of cancer = $\frac{1}{3} \approx 0.33$
- Another example with the Compliment Rule:  
  - Probability of getting cancer = 3/8 = 0.375  
  - Probability of not getting cancer = 5/8 = 1 - 3/8 = 0.625  
  
### Odds/Probability Relationship
Let:

- $\omega_c$ = odds of cancer
- $\pi_c$ = probability of cancer
- $1 - \pi_c$: probability of not getting cancer

Then:

$$\omega_c = \frac{\pi_c}{1 - \pi_c}$$

#### Odds Interpretation
- $0 < \omega < 1$: odds are not in the event’s favor
- $\omega = 1$: 50/50 chance 
- $\omega > 1$: odds favor the event 
- Odds cannot be negative

| Metric                   | Parameter (population) | Statistic (sample) |
|--------------------------|------------------------|--------------------|
| Proportion / Probability | $\pi$                  | $\hat{\pi}$        |
| Odds                     | $\omega$               | $\hat{\omega}$     |
    

## Equivalent Hypotheses

If two populations have the same probability of an event, then they have the same odds:

- The following hypotheses are all equivalent:  
  - $H_0$: $\pi_1 = \pi_2$  
  - $H_0$: $\pi_1 - \pi_2 = 0$  
  - $H_0$: $\omega_1 = \omega_2$  
  - $H_0$: $\omega_1 - \omega_2 = 0$  

### Relative Hypotheses 
- The following hypothesis are all equivalent:  
  - $H_0$: $\pi_1 = \pi_2$  
  - $H_0$: $\frac{\pi_1}{\pi_2} = 1$  
  - $H_0$: $\omega_1 = \omega_2$ 
  - $H_0$: $\frac{\omega_1}{\omega_2} = 1$ 
- Where $\frac{\omega_1}{\omega_2}$ is the odds ratio.  
  - The odds ratio is always greater than 0.  
  - If $> 1$, the odds are higher in group 1   
  - If $< 1$, the odds are lower in group 1  
      
> Note: Odds are not probabilities! Avoid words like "chance" or phrases like "x times more likely" when interpreting an odds ratio. Instead, say "the odds are x times higher."


## Relative Risk

- $H_0$: $\frac{\pi_1}{\pi_2} = 1$ 
- Relative risk is easier to interpret
- Since it is a ratio of two probabilities, you can use words like "chance" or "more likely" when interpreting it.  
- Place the larger proportion in the numerator.  
  - This keeps the relative risk greater than 1, making interpretation more intuitive.
- When using software:
  - Be aware that the definition of the “event” is often handled automatically, and may not match the event you care about.
  - Double-check the output with a quick mental or hand calculation.
  - Many functions offer options to change the reference group or event — use them to make the output match your interpretation.


## Why So Many Metrics?

All aim to detect group difference in proportions between groups: 

1. Difference in proportions (absolute)  
2. Odds ratio (relative)  
3. Relative risk (relative)  

> Use difference in proportions when events are common and you need a simple story.  
> Use relative risk for rare events.  
> Use odds ratio when working with retrospective studies.


## Study Design Matters

**Retrospective studies**:

- Only the **odds ratio** is valid.
- Proportion and relative risk estimates are biased and are not valid metrics.


## Statistical Inference for 2x2 Tables

- There are numerous approaches to hypothesis testing and confidence intervals depending on a few general situations.  
  - Sample size  
  - Study design  
  - Degree of agreement between your test and interval 

### Analysis (Hypothesis Testing) Workflow   

1. **Identify design**: prospective, retrospective\*, observational  
2. **Choose metric**:
   - Difference in proportions: event isn't rare, easy to interpret  
   - Relative risk: rare events, clear interpretation  
   - Odds ratio\*: **always valid**, required for retrospective
3. **State hypotheses**:
   - $H_0$: $\pi_1 = \pi_2$ (difference metric)  
   - $H_0$: $\frac{\pi_1}{\pi_2} = 1$ (relative risk)  
   - $H_0$: $\frac{\omega_1}{\omega_2} = 1$ (odds ratio)  
   - For observational, state without specifying parameters: $H_0$: no association
4. **Run test and obtain CI**:
   - For large sample sizes:
     - Use a chi-squared test for hypothesis testing.
     - Use a Wald interval for confidence intervals.
       - A continuity correction may improve approximation.
   - For small to moderate sample sizes:
     - Use Fisher’s exact test for hypothesis testing.
     - Use a Fisher’s, bootstrap, or small-sample corrected Wald interval (e.g. Agresti-Coull) for the confidence interval.
5. **Interpret results**:
   - Report test and p-value 
     - State the test used
   - Report interval and what it means
     - State the interval method
  
### Workflow Example

**Polio Vaccine Study (1954)**  
- Randomized: vaccine vs. placebo  
- 400,000 children  
- Concern: paralysis side effect          

|                     | Paralysis | No paralysis |
|---------------------|-----------|--------------|
| Placebo             | 142       | 199,858      |
| Salk Vaccine        | 56        | 199,944      |
    
#### Decisions:
1. **Design**: prospective (randomized)  
2. **Metric**: difference in proportions for easy interpretation (any are valid)  
3. **Hypothesis**: $H_0: \pi_P = \pi_V$  
4. **Large n** → chi-square + Wald interval  
5. **Conclusion**:
> Using a chi-squared test, there is significant evidence that the chances of a child obtaining paralysis differ between placebo and vaccine groups (p-value = ...).  
> Using a Wald interval, we are 95% confident that the chance of paralysis is 0.029% to 0.0567% higher in the placebo group.

```{r exampleCode, eval = F}

prop.test(c(#eventsRow1,#eventsRow2),c(row1Total, row2Total), correct = FALSE) # events, total sample size, without continuity correction
prop.test(c(142, 56), c(200000, 200000), correct = FALSE)

# Relative risk example
epitab(polio, method = "riskratio", riskratio = "wald", rev = "b", pvalue = "chi2", verbose = TRUE)

```

#### Alternative Framing  
- Since events are rare, relative risk may communicate results better (very small changes in probability between the two groups).  
  - P-value is the same, CI is different.  
  - Testing conclusion is the same.  
  - We are 95% confident that children in the placebo group are 1.86 to 3.45 times more likely to experience paralysis than those in the vaccine group. 
  
  - Be mindful of practical differences regarding the metrics used.  
  
#### Assumptions  
- Design dependant assumptions:  
    - Counts in 2x2 tables should be independent.  
    - No repeated measures or time-based collections 
      