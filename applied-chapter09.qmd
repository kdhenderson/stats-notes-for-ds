---
title: "Logistic Regression"
---

## Overview

- Binary response recap  
- Simple logistic model  
- Interpreting regression coefficients (numeric and categorical predictor cases)  
- Revisiting 2x2 tables 


## Simple Logistic Regression Model

### Classification
- The main distinction from linear regression: the **response variable is categorical**
  - Binary classification examples:
    - Cancer or not  
    - Clicked advertisement or not  
    - Defaulted on payment or not  

### Predictive Models
- Continuous response: $Y = f(x)$  
- Categorical response: $P(Y = \text{'Default'} \mid X) = f(X)$  
- With categorical responses, we predict the **probability** of the response, not the value itself  
- We often write $f(X)$ as $p(X)$ to emphasize that it's a probability given predictors $X$.
- Though still a function, it describes how the probabilities are changing rather than the response directly.

### The Linearity Problem
- A linear equation for $p(X)$ doesn't make sense:
  - Linear functions range over $(-\infty, \infty)$
  - Probabilities must stay between 0 and 1   

### Visualizing $p(X)$
- Bin a continuous predictor into categories  
- Calculate the proportion of the response for each bin  

### The Path to Linearity
- Scales and transformations:
  - $p(X)$: range $(0,1)$  
  - Odds: $\dfrac{p(X)}{1 - p(X)}$: range $(0, \infty)$  
  - **Log odds (logit)**: $\ln\left(\dfrac{p(X)}{1 - p(X)}\right)$: range $(-\infty, \infty)$  
- The **logit function** maps probabilities to the full real number line  
- This transformation enables linear modeling, because probabilities on the logit scale tend to be more linear with the predictor.  
  
###  Model Statement
- Logistic regression models the log odds as a linear function:  
  $$
  \ln\left(\frac{p(X)}{1 - p(X)}\right) = \beta_0 + \beta_1X
  $$ 
  
### Parameter Estimation
- **Multiple Linear Regression (MLR):**
  - Estimates minimize the residual sum of squares  
  - Closed-form solution exists (matrix algebra)
- **Logistic Regression:**
  - Estimates minimize the **log loss** error function
  - No closed-form solution, i.e. cannot use matrix multiplication to find the answer — optimization (minimizing the log loss) is done **numerically**  

### Hypothesis Testing
- $H_0: \beta_1 = 0$ vs. $H_A: \beta_1 \ne 0$
  - To understand the relationship between a predictor and the response, we should investigate the slope coefficient on the predictor.
- Test statistic (z-score / z-statistic): $Z = \frac{\hat{\beta}_1 - 0}{SE(\hat{\beta}_1)}$
- 95% Confidence interval: $\hat{\beta}_1 \pm z_{\alpha/2} \cdot SE(\hat{\beta}_1)$
- Null distribution: $\text{standard normal distribution} = Z \sim N(0, 1)$ is used to calculate the p-value
  - Some software may report a chi-square test (equivalent p-value from $Z^2$, squares the z-statisic and using a chi-square rather than a z distribution to calculate the p-value).  
    
### Predicted Probabilities
- Model transformations:
  $$\ln\left(\frac{p(X)}{1 - p(X)}\right) = \beta_0 + \beta_1X$$
  $$\frac{p(X)}{1 - p(X)} = e^{\beta_0 + \beta_1X}$$
  $$p(X) = \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1X}}$$


## Interpreting Coefficients: Continuous Predictors

### Odds and Odds Ratios

- Logistic regression coefficients have good interpretational value.  
- Whether the predictor is numeric or categorical determines interpretation.
- Logistic regression coefficients are interpreted via odds ratios
- Odds: $\text{Odds}(X) = \frac{p(X)}{1 - p(X)} = e^{\beta_0 + \beta_1X}$
- Example: $\text{Odds}(X = 500) = e^{\beta_0 + \beta_1 \cdot 500}$, $\text{Odds}(X = 501) = e^{\beta_0 + \beta_1 \cdot 501}$
- Odds ratio (OR), i.e.the odds at X = 501 relative to the odds at X = 500: $\frac{e^{\beta_0 + \beta_1 \cdot 501}}{e^{\beta_0 + \beta_1 \cdot 500}} = e^{\beta_1}$

- Interpretation: For a 1-unit increase in $X$, the odds of the event occurring change by a factor of $e^{\beta_1}$

### Generalized Interpretation
- Interpretation does not depend on the actual value of $X$  
- For a $\Delta$-unit increase: $\text{Odds change by } e^{\Delta \cdot \beta_1}$
- Example:  
  - A \$500 increase in the balance increases the odds of default by a factor of 15.64.  
  - The odds of defaulting with a \$1,000 balance are 15.64 times higher than with \$500.  

### Tips
- Odds ratios are relative, not absolute  
- Use predicted probabilities to assess absolute likelihood.   
- Using meaningful values of $X$ and stating the odds ratio using those values is way to provide clear interpretations. 


## Interpreting Coefficients: Categorical Predictors

### Dummy Variables

- Categorical variables are dummytized
- One coefficient is created for the intercept and for all levels of the categorical predictor minus one.
- One level is omitted → reference category, which is associated with the intercept
- Coefficients are interpreted as odds ratios relative to the reference

### Example: Student Status
- Variable: `student`  
  - Yes (1), No (0)  
- Odds if student: $e^{\beta_0 + \beta_1 \cdot 1}$
- Odds if not a student: $e^{\beta_0 + \beta_1 \cdot 0} = e^{\beta_0}$
- Odds ratio:$OR = \frac{e^{\beta_0 + \beta_1}}{e^{\beta_0}} = e^{\beta_1}$
  - The odds of defaulting for a student are $e^{\beta_1}$ times higher/lower than for a nonstudent.  

### Confidence Intervals for ORs
- 95% CI for $\beta_1$: $\hat{\beta}_1 \pm z_{\alpha/2} \cdot SE(\hat{\beta}_1)$
- Exponentiate endpoints to get CI for the odds ratio: $\left( e^{\hat{\beta}_1 - z_{\alpha/2}SE(\hat{\beta}_1)}, e^{\hat{\beta}_1 + z_{\alpha/2}SE(\hat{\beta}_1)} \right)$
- Interpretation is provided for the point estimate (coefficient); the confidence interval is reported to reflect sampling error, but without andy lengthy worded interpretation.  

  
## General Workflow and Considerations

### Assumptions
- Observations are independent  
- The **log odds** are linearly related to continuous predictors  
- No unaccounted confounders  

### Model Fit
- Residual plots are less useful — binary outcome means variance changes are expected  
- Instead:
  - Compare predicted probabilities to observed proportions, i.e. does the interpretation of the estimates make sense with the EDA?   
  - Hosmer-Lemeshow test:
    - $H_0$: model fits  
    - $H_A$: model does not fit  

### If Fit Is Poor
- Strange trends in EDA → linear log odds assumption may be violated  
  - Try polynomial terms or additional predictors  
- Rejected Hosmer-Lemeshow test may not always imply a problem (especially in large samples)

### Recommended Workflow

1. EDA  
   - LOESS curve for continuous predictors  
   - Mosaic plots or comparison of proportions for categorical predictors      
2. Fit model  
   - Check fit via diagnostic tests  
3. Inference  
   - z-test for coefficient relevance, i.e. not zero  
   - Interpret coefficient as odds ratio  
   - Provide confidence interval  
   - Choose a sensible unit change based on context  
 
   
## Looking Ahead: Multiple Logistic Regression

- Like MLR, most applications require multiple predictors
- Key skills from this chapter:
  - Understanding the logit function  
  - Interpreting coefficients using odds ratios  
  - Conducting EDA before modeling  
     