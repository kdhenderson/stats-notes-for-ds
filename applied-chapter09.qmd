---
title: "Logistic Regression"
---

## Overview

- Understand how logistic regression models a binary response   
- Understand the structure and purpose of the simple logistic regression model 
- Learn how to interpret regression coefficients for both numeric and categorical predictors  
- Revisit the connection between logistic regression and 2×2 contingency tables 


## Simple Logistic Regression Model

### Classification
- The main distinction from linear regression: the **response variable is categorical**
  - Binary classification examples:
    - Cancer or not  
    - Clicked advertisement or not  
    - Defaulted on payment or not  

### Predictive Models
- Continuous response: $Y = f(x)$  
- Categorical response: $P(Y = \text{Default} \mid X) = f(X)$  
- With categorical responses, we predict the **probability** of the response, not the value itself.  
- We often write $f(X)$ as $p(X)$ to emphasize that it's a probability given predictors $X$.
- This is still a function---it just describes how the **probability** changes with the predictors rather than the response directly.

### The Linearity Problem
- A linear equation for $p(X)$ doesn't make sense:
  - Linear functions range over $(-\infty, \infty)$
  - But probabilities must stay between 0 and 1   

### Visualizing $p(X)$
- One way to understand how $p(X)$ behaves:
  - Bin a continuous predictor into categories  
  - For each bin, compute the proportion of the response  

### The Path to Linearity
- Scales and Transformations: we apply transformations to make the model linear on a new scale.
  - $p(X)$: range $(0,1)$  
  - Odds: $\dfrac{p(X)}{1 - p(X)}$: range $(0, \infty)$  
  - **Log odds (logit)**: $\ln\left(\dfrac{p(X)}{1 - p(X)}\right)$: range $(-\infty, \infty)$  
- The **logit function** maps probabilities to the full real number line  
- This transformation enables linear modeling, because probabilities on the logit scale tend to be more linear with the predictor.  

### Model Statement
- Logistic regression models the **log odds** as a linear function of $X$:

  $$
  \ln\left(\frac{p(X)}{1 - p(X)}\right) = \beta_0 + \beta_1 X
  $$
  
### Parameter Estimation

#### Multiple Linear Regression (MLR)
- Coefficient estimates minimize the **residual sum of squares**.  
- Closed-form solution exists via matrix algebra.

#### Logistic Regression
- Coefficient estimates minimize the **log loss** (cross-entropy) error function.
- No closed-form solution, i.e., cannot use matrix multiplication to find the answer.
- Optimization (minimizing the log loss) is done numerically. 

### Hypothesis Testing
- To understand the relationship between a predictor and the response, test the slope coefficient:
  - $H_0$: $\beta_1 = 0$  
  - $H_A$: $\beta_1 \ne 0$  
- Test statistic (z-score): $Z = \frac{\hat{\beta}_1}{SE(\hat{\beta}_1)}$
- 95% confidence interval: $\hat{\beta}_1 \pm z_{\alpha/2} \cdot SE(\hat{\beta}_1)$
- The null distribution is standard normal: $Z \sim N(0, 1)$  
- Some software reports a chi-squared test instead of a z-test  
  - It squares the z-statistic: $Z^2$
  - Then uses a $\chi^2$ distribution with 1 degree of freedom to compute the p-value  
  - This gives the same p-value as the two-sided z-test

### Predicted Probabilities

- Logistic regression estimates the log odds, but we often want $p(X)$:
  $$
  \ln\left(\frac{p(X)}{1 - p(X)}\right) = \beta_0 + \beta_1 X
  $$
  $$
  \frac{p(X)}{1 - p(X)} = e^{\beta_0 + \beta_1 X}
  $$
  $$
  p(X) = \frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}
  $$

---

## Interpreting Coefficients: Continuous Predictors

### Odds and Odds Ratios

- Logistic regression coefficients have good interpretational value.  
- Whether the predictor is numeric or categorical determines interpretation.
- Logistic regression coefficients are interpreted via odds ratios
- Odds: $\text{Odds}(X) = \frac{p(X)}{1 - p(X)} = e^{\beta_0 + \beta_1X}$
- Example: $\text{Odds}(X = 500) = e^{\beta_0 + \beta_1 \cdot 500}$, $\text{Odds}(X = 501) = e^{\beta_0 + \beta_1 \cdot 501}$
- Odds ratio (OR), i.e.the odds at X = 501 relative to the odds at X = 500: $\frac{e^{\beta_0 + \beta_1 \cdot 501}}{e^{\beta_0 + \beta_1 \cdot 500}} = e^{\beta_1}$

- Interpretation: For a 1-unit increase in $X$, the odds of the event occurring change by a factor of $e^{\beta_1}$

### Generalized Interpretation
- Interpretation does not depend on the actual value of $X$  
- For a $\Delta$-unit increase: $\text{Odds change by } e^{\Delta \cdot \beta_1}$
- Example:  
  - A \$500 increase in the balance increases the odds of default by a factor of 15.64.  
  - The odds of defaulting with a \$1,000 balance are 15.64 times higher than with \$500.  

### Tips
- Odds ratios are relative, not absolute  
- Use predicted probabilities to assess absolute likelihood.   
- Using meaningful values of $X$ and stating the odds ratio using those values is way to provide clear interpretations. 


## Interpreting Coefficients: Categorical Predictors

### Dummy Variables

- Categorical variables are dummytized
- One coefficient is created for the intercept and for all levels of the categorical predictor minus one.
- One level is omitted → reference category, which is associated with the intercept
- Coefficients are interpreted as odds ratios relative to the reference

### Example: Student Status
- Variable: `student`  
  - Yes (1), No (0)  
- Odds if student: $e^{\beta_0 + \beta_1 \cdot 1}$
- Odds if not a student: $e^{\beta_0 + \beta_1 \cdot 0} = e^{\beta_0}$
- Odds ratio:$OR = \frac{e^{\beta_0 + \beta_1}}{e^{\beta_0}} = e^{\beta_1}$
  - The odds of defaulting for a student are $e^{\beta_1}$ times higher/lower than for a nonstudent.  

### Confidence Intervals for ORs
- 95% CI for $\beta_1$: $\hat{\beta}_1 \pm z_{\alpha/2} \cdot SE(\hat{\beta}_1)$
- Exponentiate endpoints to get CI for the odds ratio: $\left( e^{\hat{\beta}_1 - z_{\alpha/2}SE(\hat{\beta}_1)}, e^{\hat{\beta}_1 + z_{\alpha/2}SE(\hat{\beta}_1)} \right)$
- Interpretation is provided for the point estimate (coefficient); the confidence interval is reported to reflect sampling error, but without andy lengthy worded interpretation.  

  
## General Workflow and Considerations

### Assumptions
- Observations are independent  
- The **log odds** are linearly related to continuous predictors  
- No unaccounted confounders  

### Model Fit
- Residual plots are less useful — binary outcome means variance changes are expected  
- Instead:
  - Compare predicted probabilities to observed proportions, i.e. does the interpretation of the estimates make sense with the EDA?   
  - Hosmer-Lemeshow test:
    - $H_0$: model fits  
    - $H_A$: model does not fit  

### If Fit Is Poor
- Strange trends in EDA → linear log odds assumption may be violated  
  - Try polynomial terms or additional predictors  
- Rejected Hosmer-Lemeshow test may not always imply a problem (especially in large samples)

### Recommended Workflow

1. EDA  
   - LOESS curve for continuous predictors  
   - Mosaic plots or comparison of proportions for categorical predictors      
2. Fit model  
   - Check fit via diagnostic tests  
3. Inference  
   - z-test for coefficient relevance, i.e. not zero  
   - Interpret coefficient as odds ratio  
   - Provide confidence interval  
   - Choose a sensible unit change based on context  
 
   
## Looking Ahead: Multiple Logistic Regression

- Like MLR, most applications require multiple predictors
- Key skills from this chapter:
  - Understanding the logit function  
  - Interpreting coefficients using odds ratios  
  - Conducting EDA before modeling  
     