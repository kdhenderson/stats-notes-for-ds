---
title: "Clustering"
---

## Overview

- Clustering objective  
  - $k$-means  
  - Hierarchical clustering  
- Considerations  
- Heatmaps and applications  


## Clustering Objective

When plotting two numerical variables:
- Are the observations part of one homogeneous group?
- Or are there subsets of observations that behave more similarly and look 'different' compared with others?

Key points:
- Clustering is an unsupervised technique.
- Each data point is assigned to a group (cluster).
- Data within the same group are more similar.
- Data between groups are quite different.

### Heterogeneous Populations
- Most populations are heterogeneous.
- Perks of identifying subpopulations:
  - Deeper EDA and sanity checks (for classification problems)
  - Unique trends need unique models
  - Optimizing downstream decisions


## $k$-Means Clustering

### Metric for Cluster Fit

Assuming observations are already assigned to clusters:
- Trade-off between:
  - Within-cluster variability
  - Between-cluster variability

Within-cluster metric:
- Let $k$ be the number of clusters.
- Let $C_k$ be the set of observations assigned to cluster $k$.
- Define within-cluster variability $W(C_k)$ as:
  $$
  W(C_k) = \frac{1}{\text{number of observations in cluster}} \times \text{sum of squared pairwise Euclidean distances within cluster}
  $$

- The goal is to minimize the overall within-cluster variability.
- Note: Within-cluster variability can be arbitrarily minimized.
 
### Challenges in Minimizing $W(C_k)$
- Large number of possible assignments.
- Increases with $k$ (number of clusters) and $n$ (sample size).
- $k$-means clustering obtains a local minimization.
- $k$ must be specified upfront to initialize the algorithm.    

### $k$-Means Algorithm

1. Randomly assign observations to cluster groups.
2. Reassignment:
   - (a) Calculate the average centroid for each group, i.e. mean of the observations from step 1.
   - (b) Reassign observations to the nearest centroid.
3. Repeat step 2 until within-cluster variability converges.
  
### Defining "Close"

- Distance metrics:
  - Euclidean distance
  - Mahalanobis distance
  - Manhattan distance


## Hierarchical Clustering

- Does not require specifying $k$ in advance.
- Begins with each observation as its own cluster.
- Fuses observations to create progressively coarser clusters.
- Outputs a dendrogram tree to visualize clustering.

### Linkage
- Distance (dissimilarity) metric is used between observations.
- Clusters can fuse:
  - Two points
  - One cluster and one point
  - Two clusters
- Dissimilarity metric between clusters is called linkage.

### Common Linkage Types
1. Complete  
2. Single  
3. Average  
4. Centroid  

#### Complete Linkage
- Maximal inter-cluster dissimilarity.
- Compute all pairwise distances between cluster A and cluster B.
- Record the **largest** pairwise distance between the clusters.
- Merge clusters with the **smallest maximum** distance (i.e. smallest dissimilarity).
 
#### Single Linkage
- Minimal inter-cluster dissimilarity.
- Compute all pairwise distances between cluster A and cluster B.
- Record the **smallest pairwise** distance between the clusters.
- Merge clusters with the **smallest minimum** distance (i.e. smallest dissimilarity). 
- Can produce trailing dendrograms with single observations (i.e. many single observations fused with clusters). 

#### Average Linkage
- Mean inter-cluster dissimilarity.
- Compute all pairwise distances between cluster A and cluster B.
- Record the **mean** distance between clusters.
- Repeat for all pairwise clusters available.
- Merge clusters with the **smallest average**. 
  
#### Centroid Linkage
- Mean intercluster dissimilarity
- Distance between cluster centroids.
- Compute centroids of each cluster.
- Merge clusters whose centroids are **closest** (i.e. have the smallest distance). 
 
  
## Considerations for Either Tool

### Key Considerations
- Which dissimilarity metric? (distance or correlation)
- Should variables be standardized?
- Which linkage type? (for hierarchical)
- How many clusters? 
  
### Correlation vs. Distance
- Observations may be far apart by distance but close in terms of **correlation**.
- Example:
  - Distance metric: heavy vs. light shoppers.
  - Correlation metric: shoppers buying similar items. 
  
### Standardization
- Without standardization: variables with large scales dominate clustering.
- With standardization: each variable contributes equally.
- Recommendation: Standardize unless there is a strong reason not to.

### Choice of Linkage
- Results vary by linkage choice.
- **Complete** and **average linkage**:
  - Commonly used.
  - Easier-to-read dendrograms.
- **Single linkage**:
  - Good for finding outliers.
  - Clusters may be loosely connected, i.e. observations within clusters may be less similar that you would like. 
      
### Choosing the Number of Clusters
- Apply clustering with different values of $k$.
- Compute cluster validity metrics.
- Plot metric vs. cluster size to guide decision.
   
    
## Cluster Validity Metrics

### Internal Cluster Validity
- No ground truth available.
- Estimate within-cluster vs. between-cluster variation.
    
### Common Metrics
- Silhouette statistic
- Dunn index
- Cubic clustering criterion (SAS)

#### Silhouette Statistic
For each observation:
- Calculate average dissimilarity **within** its cluster.
- Calculate the minimum average dissimilarity (my notes said similarity) to a **different** cluster.
- Silhouette value for observation $i$:
  $$
  \text{Silhouette}_i = \frac{\text{between} - \text{within}}{\max(\text{between}, \text{within})}
  $$
- Average silhouette values summarize overall clustering quality.

#### Dunn Index
- Ratio:
  $$
  \frac{\text{minimum distance between points in different clusters}}{\text{maximum distance between points within the same cluster}}
  $$
- Higher Dunn index indicates stronger cluster separation.  

### Additional Metrics
- Davies-Bouldin index (DB)
- Maulik-Bandyopadhyay index (MB)
- Saitta score function (SF)  
  
---
  
- The Clustering Challenge  
  - Results will vary based on your choices  
  - No one method is perfect for determining cluster size  
  - Should never interpret one result as the absolute truth  
  - Consider examining multiple results (for general theme) before making a decision  
  

**Heatmaps**  
  
- Need for Heatmaps  
  - Dendrogram and cluster assignments  
    - No information on the data used  
  - Helpful to see the data in addition  
    - Great sanity check  

- A plot of raw or standardized numeric variables  
  - Data matrix of observations  
  - Summary matrix (means correlations)  
- Plot is a grid of rectangles color coded by the numeric value  
- Clustering (hierarchical) can be applied to the rows and columns to investigate  

- Clustering in Practice  
  - Main purpose is to identify observations that are closer together (distance or correlation)  
    - "Observation" can be defined in many ways  
  - Ideas  
    - cluster correlation statistics
    - clustering variables (clustering on the columns instead of rows, by transposing dataset). 
  
  
      