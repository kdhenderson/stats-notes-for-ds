---
title: "Clustering"
---

## Overview

- Clustering objective  
  - $k$-means  
  - Hierarchical clustering  
- Considerations  
- Heatmaps and applications  


## Clustering Objective

When plotting two numerical variables:
- Are the observations part of one homogeneous group?
- Or are there subsets of observations that behave more similarly and look 'different' compared with others?

Key points:
- Clustering is an unsupervised technique.
- Each data point is assigned to a group (cluster).
- Data within the same group are more similar.
- Data between groups are quite different.

### Heterogeneous Populations
- Most populations are heterogeneous.
- Perks of identifying subpopulations:
  - Deeper EDA and sanity checks (for classification problems)
  - Unique trends need unique models
  - Optimizing downstream decisions


## $k$-Means Clustering

### Metric for Cluster Fit

Assuming observations are already assigned to clusters:
- Trade-off between:
  - Within-cluster variability
  - Between-cluster variability

Within-cluster metric:
- Let $k$ be the number of clusters.
- Let $C_k$ be the set of observations assigned to cluster $k$.
- Define within-cluster variability $W(C_k)$ as:
  $$
  W(C_k) = \frac{1}{\text{number of observations in cluster}} \times \text{sum of squared pairwise Euclidean distances within cluster}
  $$

- The goal is to minimize the overall within-cluster variability.
- Note: Within-cluster variability can be arbitrarily minimized.
> ⚠️ *Note: Within-cluster variability can always be minimized by increasing $k$, so the big question is: how many clusters are actually present?*

 
### Challenges in Minimizing $W(C_k)$
- Large number of possible assignments.
- Increases with $k$ (number of clusters) and $n$ (sample size).
- $k$-means clustering obtains a local minimization.
- $k$ must be specified upfront to initialize the algorithm.    

### $k$-Means Algorithm

1. Randomly assign observations to cluster groups.
2. Reassignment:
   - (a) Calculate the average centroid for each group, i.e. mean of the observations from step 1.
   - (b) Reassign observations to the nearest centroid.
3. Repeat step 2 until within-cluster variability converges.
  
### Defining "Close"

- Distance metrics:
  - Euclidean distance
  - Mahalanobis distance
  - Manhattan distance


## Hierarchical Clustering

- Does not require specifying $k$ in advance.
- Begins with each observation as its own cluster.
- Fuses observations to create progressively coarser clusters.
- Outputs a dendrogram tree to visualize clustering.

### Linkage
- Distance (dissimilarity) metric is used between observations.
- Clusters can fuse:
  - Two points
  - One cluster and one point
  - Two clusters
- Dissimilarity metric between clusters is called linkage.

### Common Linkage Types
1. Complete  
2. Single  
3. Average  
4. Centroid  

#### Complete Linkage
- Maximal inter-cluster dissimilarity.
- Compute all pairwise distances between cluster A and cluster B.
- Record the **largest** pairwise distance between the clusters.
- Merge clusters with the **smallest maximum** distance (i.e. smallest dissimilarity).
 
#### Single Linkage
- Minimal inter-cluster dissimilarity.
- Compute all pairwise distances between cluster A and cluster B.
- Record the **smallest pairwise** distance between the clusters.
- Merge clusters with the **smallest minimum** distance (i.e. smallest dissimilarity). 
- Can produce trailing dendrograms with single observations (i.e. many single observations fused with clusters). 

#### Average Linkage
- Mean inter-cluster dissimilarity.
- Compute all pairwise distances between cluster A and cluster B.
- Record the **mean** distance between clusters.
- Repeat for all pairwise clusters available.
- Merge clusters with the **smallest average**. 
  
#### Centroid Linkage
- Mean intercluster dissimilarity
- Distance between cluster centroids.
- Compute centroids of each cluster.
- Merge clusters whose centroids are **closest** (i.e. have the smallest distance). 
 
  
## Considerations for Either Tool

### Key Considerations
- Which dissimilarity metric? (distance or correlation)
- Should variables be standardized?
- Which linkage type? (for hierarchical)
- How many clusters? 
  
### Correlation vs. Distance
- Observations may be far apart by distance but close in terms of **correlation**.
- Example:
  - Distance metric: heavy vs. light shoppers.
  - Correlation metric: shoppers buying similar items. 
  
### Standardization
- Without standardization: variables with large scales dominate clustering.
- With standardization: each variable contributes equally.
- Recommendation: Standardize unless there is a strong reason not to.

### Choice of Linkage
- Results vary by linkage choice.
- **Complete** and **average linkage**:
  - Commonly used.
  - Easier-to-read dendrograms.
- **Single linkage**:
  - Good for finding outliers.
  - Clusters may be loosely connected, i.e. observations within clusters may be less similar that you would like. 
      
### Choosing the Number of Clusters
- Apply clustering with different values of $k$.
- Compute cluster validity metrics.
- Plot metric vs. cluster size to guide decision.
   
    
## Cluster Validity Metrics

### Internal Cluster Validity
- No ground truth available.
- Estimate within-cluster vs. between-cluster variation.
    
### Common Metrics
- Silhouette statistic
- Dunn index
- Cubic clustering criterion (SAS)

#### Silhouette Statistic
For each observation:
- Calculate average dissimilarity **within** its cluster.
- Calculate the minimum average dissimilarity to a **different** cluster.
- Silhouette value for observation $i$:
  $$
  \text{Silhouette}_i = \frac{\text{between} - \text{within}}{\max(\text{between}, \text{within})}
  $$
- Average silhouette values summarize overall clustering quality.

#### Dunn Index
- Ratio:
  $$
  \frac{\text{minimum distance between points in different clusters}}{\text{maximum distance between points within the same cluster}}
  $$
- Higher Dunn index indicates stronger cluster separation.  

### Additional Metrics
- Davies-Bouldin index (DB)
- Maulik-Bandyopadhyay index (MB)
- Saitta score function (SF)  
 
  
## The Clustering Challenge

- Results depend on choices made (distance metric, standardization, linkage type).
- No single method is perfect for determining the number of clusters.
- Avoid interpreting any one result as absolute truth.
- Examine multiple clustering solutions to identify general patterns.
- Simulation studies show clustering performance depends on cluster separation, noise, and structure.
- Issues like sub-clusters or overlapping groups can lead to incorrect $k$ estimates or misleading assignments.
  

## Heatmaps

### Motivation
- Dendrograms and cluster assignments do not show the underlying data.
- Heatmaps offer a visual check and help interpret clusters.

### Heatmap Details
- Plot of raw or standardized numeric variables.
- Can represent:
  - Data matrices of observations.
  - Summary matrices (e.g. means, correlations).
- Grid of rectangles colored by numeric values.


### Heatmaps with Clustering
- Hierarchical clustering can be applied to rows and/or columns.
- Helps investigate patterns among:
  - Observations (rows)
  - Variables (columns) 


## Clustering in Practice

- Goal: identify groups that are closer together (distance or correlation).
- Definition of 'observation' varies depending on context.

Possible uses:
- Clustering correlation statistics.
- Clustering variables (i.e., treating variables as the observations by transposing the data matrix).
  - Variables often cluster based on strong positive correlations.

      