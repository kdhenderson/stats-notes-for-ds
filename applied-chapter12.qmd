---
title: "Clustering"
---

## Overview

- Clustering objective  
  - $k$-means  
  - Hierarchical clustering  
- Considerations  
- Heatmaps and applications  


## Clustering Objective

When plotting two numerical variables:
- Are the observations part of one homogeneous group?
- Or are there subsets of observations that behave more similarly and look 'different' compared with others?

Key points:
- Clustering is an unsupervised technique.
- Each data point is assigned to a group (cluster).
- Data within the same group are more similar.
- Data between groups are quite different.

### Heterogeneous Populations
- Most populations are heterogeneous.
- Perks of identifying subpopulations:
  - Deeper EDA and sanity checks (for classification problems)
  - Unique trends need unique models
  - Optimizing downstream decisions


## $k$-Means Clustering

### Metric for Cluster Fit

Assuming observations are already assigned to clusters:
- Trade-off between:
  - Within-cluster variability
  - Between-cluster variability

Within-cluster metric:
- Let $k$ be the number of clusters.
- Let $C_k$ be the set of observations assigned to cluster $k$.
- Define within-cluster variability $W(C_k)$ as:
  $$
  W(C_k) = \frac{1}{\text{number of observations in cluster}} \times \text{sum of squared pairwise Euclidean distances within cluster}
  $$

- The goal is to minimize the overall within-cluster variability.
- Note: Within-cluster variability can be arbitrarily minimized.
 
### Challenges in Minimizing $W(C_k)$
- Large number of possible assignments.
- Increases with $k$ (number of clusters) and $n$ (sample size).
- $k$-means clustering obtains a local minimization.
- $k$ must be specified upfront to initialize the algorithm.    

### $k$-Means Algorithm

1. Randomly assign observations to cluster groups.
2. Reassignment:
   - (a) Calculate the average centroid for each group, i.e. mean of the observations from step 1.
   - (b) Reassign observations to the nearest centroid.
3. Repeat step 2 until within-cluster variability converges.
  
### Defining "Close"

- Distance metrics:
  - Euclidean distance
  - Mahalanobis distance
  - Manhattan distance

---

**Hierarchical Clustering**  

- Does not require specifying $k$ in advance  
- Starts by assuming each observation is a cluster  
- Fuses observations together to create more "coarse" clusters  
- Provides dendrogram tree to visualize. 

- Linkage  
  - Distance is used as dissimilarity metric for observations  
  - Dendrogram fuses  
    - Two points  
    - One cluster / one point  
    - Two clusters  
  - Dissimilarity metric needed for clusters called "linkage"  
  
- Four Common Linkage Types  
   1. Complete  
   2. Single  
   3. Average  
   4. Centroid  

- Complete Linkage  
  - Maximal intercluster dissimilarity  
  - Compute all pairwise distances between cluster A and cluster B  
  - Record the largest pairwise distance between the clusters  
  - Repeat for all pairwise clusters available  
  - Merge clusters with smallest dissimilarity  

- Single Linkage  
  - Minimal intercluster dissimilarity  
  - Compute all pairwise distances between cluster A and cluster B  
  - Record the smallest pairwise distance between the clusters  
  - Repeat for all pairwise clusters available  
  - Merge clusters with smallest dissimilarity  
  - Can produce many single observation fused with clusters (trailing dendrogram)  
  
- Average Linkage  
  - Mean intercluster dissimilarity  
  - Compute all pairwise distances between cluster A and cluster B  
  - Record the mean distance between clusters  
  - Repeat for all pairwise clusters available  
  - Merge clusters with the smallest average  
  
- Centroid Linkage  
  - Mean intercluster dissimilarity  
  - Compute centroids for each cluster  
  - Calculates distances between centroids   
  - Merge clusters with the smallest distance  
  
  
**Considerations for Either Tool**   

- Key Considerations  
  - Distance or something else?  
  - Standardize?  
  - Linkage type? (hierarchical)  
  - Number of clusters?  
  
- Correlation Dissimilarity (versus distance metric)  
  - Observations 1 and 2 are far apart in distance  
  - Observations 1 and 2 are "close" in terms of correlation 
    - Pattern is similar  
  - Shopper example: distance -> heavy vs light shoppers, correlation -> similar items  
  
- Standarization  
  - If left unstandardized:  
    - Large scale variables influence clustering more  
  - If standardized:  
    - Variables have more equal influence  
    - Recommend unless strong reason not to  
    
- Choice of Linkage  
  - Results will vary  
  - Complete and average  
    - Commonly used  
    - Easier to read dendrograms  
  - Single linkage  
    - Consider when searching for outliers  
    - Observations within clusters may be less similar that you would like  
    
- How many clusters?  
  - General strategy  
    - Apply clustering tool multiple times with different $k$  
    - Compute a cluster validity metric  
    - Plot validity metric vs. cluster size  
    
- Cluster Validity Metrics  
  - Internal cluster validity  
    - No ground truth is available  
    - Estimate within- vs. between-cluster variation  
  - Common metrics  
    - Silhouette statistic  
    - Dunn index  
    - Cubic clustering criterion (SAS)  

- Silhouette Statistic  
  - for each obs, calculate avg dissimilarity within cluster  
  - for each obs, calculate min avg similarity between a cluster  
  - statistic is calculated on each of the $i$ observations  
    - subtracts within from between divided by the max
  - average silhouette values for a general score for the clustering assignment  
  
- Dunn Index  
  - creates a ratio: shortest distance b/t 2 points not in same cluster / longest distance b/t 2 points in the same cluster  
  - like silhouette, the larger the value, the stronger the evidence of well defined clusters  
  
- Additional Metrics  
  - Davies-Bouldin index (DB)  
  - Maulik-Bandyopadhyay index (MB)  
  - Saitta score function (SF)  
  
- The Clustering Challenge  
  - Results will vary based on your choices  
  - No one method is perfect for determining cluster size  
  - Should never interpret one result as the absolute truth  
  - Consider examining multiple results (for general theme) before making a decision  
  

**Heatmaps**  
  
- Need for Heatmaps  
  - Dendrogram and cluster assignments  
    - No information on the data used  
  - Helpful to see the data in addition  
    - Great sanity check  

- A plot of raw or standardized numeric variables  
  - Data matrix of observations  
  - Summary matrix (means correlations)  
- Plot is a grid of rectangles color coded by the numeric value  
- Clustering (hierarchical) can be applied to the rows and columns to investigate  

- Clustering in Practice  
  - Main purpose is to identify observations that are closer together (distance or correlation)  
    - "Observation" can be defined in many ways  
  - Ideas  
    - cluster correlation statistics
    - clustering variables (clustering on the columns instead of rows, by transposing dataset). 
  
  
      