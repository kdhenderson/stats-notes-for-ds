---
title: "Multiple Linear Regression (MLR) Revisited"
---

## Objectives

- Understand regression problems and distinguish them from classification problems.
- Identify when to use explanatory versus predictive modeling.
- Compare parametric and nonparametric regression models and their trade-offs.
- Review multiple linear regression (MLR) and its applications.
- Evaluate key assumptions of MLR and correctly interpret model coefficients.
- Explore techniques to increase model complexity when needed.
- Detect and address multicollinearity in regression models.


## Key Terms

- Response Variable vs. Explanatory Variables  
  - The **response variable** (dependent variable) is what we aim to explain or predict.  
  - **Explanatory variables** (independent variables) are the factors that may influence the response.

- Regression vs. Classification 
  - **Regression**: The response is a continuous numeric variable (e.g. predicting car mileage).  
  - **Classification**: The response variable is categorical and can have two (binary) or more than two levels (e.g. predicting where a car was made: North America, Asia, or Europe).


### Example: Regression Problem - MPG Dataset  
A common regression problem is predicting a car's miles per gallon (MPG) using various features:

- Response Variable: `mpg` (miles per gallon)
- Explanatory Variables: `cylinders`, `horsepower`, `origin` (3 levels), `year`, etc.

### Regression Modeling Workflow
When analyzing a regression problem, we typically follow these steps:

1. Exploratory Data Analysis (EDA) 
   - Visualize relationships between the response and each explanatory variable.
   - Identify potential trends, outliers, and transformations.

2. Key Questions to Consider  
   - Which variables explain `mpg`, and how strong are these relationships?  
   - Are there specific hypotheses to test (e.g. does a car's origin significantly impact `mpg`)?  
   - Can we use this model to predict the `mpg` of a new car?


## Regression Problems

A regression problem occurs when the response variable is continuous. The goal can either be **explanation** (understanding relationships) or **prediction** (making accurate future estimates).

### To Explain or To Predict?

1. Explaining Relationships  
   - Hypothesis testing and confidence intervals  
   - Identifying relationships between response and predictors  
   - Adjusting for confounding (lurking) variables

2. Predicting Future Outcomes  
   - Focused on accuracy, not interpretation  
   - More complex models can be used

To determine the appropriate approach, ask:
What is the primary goal—explanation, prediction, or both?


## Parametric vs. Nonparametric Regression Tools

### General Regression Model Structure

The regression model follows:

$$
Y = f(X) + \varepsilon
$$

where:  
- $X = (x_1, x_2, ..., x_p)$ represents the explanatory variables.  
- $f(X)$ is the deterministic component.  
- $\varepsilon$ is the random error.  

### Key Terminology
- Population function: $f$ (true but unknown function in the population).  
- Sample function: $\hat{f}$ (estimate of $f$ using observed data).  

### Estimating $f$: Parametric vs. Nonparametric Approaches  

There are two primary approaches for estimating $f$:  
1. **Parametric** 
2. **Nonparametric**

The choice depends on how the response and predictor variables relate mathematically.  
For example:  
- A numeric predictor like horsepower may follow a quadratic function.  
- A categorical predictor like cylinders may follow a piecewise function.  

### Parametric Approach
- Requires specifying the functional form of $f(X)$.  
- Assumes additional conditions about the error term.  
- Allows hypothesis testing.  
- Works well when the model is correctly specified.  
- Simpler models can be more interpretable.  

### Nonparametric Approach
- No need to specify $f(X)$.  
- Fully data-driven and flexible.  
- Can capture complex relationships.  
- More difficult to interpret.  
- May require larger datasets for reliable estimation.  
- Does not support traditional hypothesis testing frameworks.  

### Tradeoffs: Interpretability vs. Flexibility

| Approach       | Pros | Cons |
|---------------|------|------|
| Parametric    | Interpretable  <br> Allows hypothesis testing  <br> Works well for small datasets  <br> Easier for high $p$, low $n$ settings | Cannot model complex relationships well  <br> Sensitive to misspecification |
| Nonparametric | Adapts to complex patterns  <br> Less restrictive assumptions  <br> Often performs better for large datasets | Requires large datasets  <br> No hypothesis testing  <br> Harder to interpret |

### Challenges with Parametric Models
1. Specifying highly complex forms of $f(X)$ is difficult.  
2. For large datasets, nonparametric models often perform as well or better.  
3. Additional diagnostic checks are required, such as:  
   - Checking model assumptions (linearity, normality, independence).  
   - Addressing multicollinearity and influential points.  


## A Review of Multiple Linear Regression (MLR)

### Model Structure

$$
Y = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p + \varepsilon
$$

where:  
- $Y$ is the response variable.  
- $X_1, X_2, ..., X_p$ are the explanatory variables.  
- $\varepsilon$ is the random error term.

### Assumptions of MLR
1. **Linearity** – The relationship between predictors and response is linear.  
2. **Independence** – Errors are independent.  
3. **Homoscedasticity** – Errors have constant variance.  
4. **Normality** – Errors follow a normal distribution. 
  
### Residual Diagnostic Checks  
These checks help assess whether assumptions hold:  

- **Residual vs. Fitted Plot** – Examines constant variance (homoscedasticity).  
- **Q-Q Plot and/or Histogram** – Assesses whether residuals follow a normal distribution.  
- **Variance Inflation Factor (VIF)** – Identifies multicollinearity among predictors.  
- **Data Collection Process Review** – Ensures independence.  
  - Violations often occur in repeated measures on the same subject or in time series data.  

---
### Basic Interpretations

- When each predictor is included in the model only once, interpretations follow these general rules:  
  - **Continuous predictors**: A one-unit increase in $X$ results in a change of $\beta$ in $Y$, holding all other variables constant. 
  - **Categorical variables**: Represented using dummy variables. 
**Example:** Suppose $X_1$ is categorical with three levels (A, B, C), while $X_2$ and $X_3$ are continuous. The model:  

$$
Y = \beta_0 + \beta_1\text{LevelB} + \beta_2\text{LevelC} + \beta_3X_2 + \beta_4X_3
$$

- $\beta_1$: Difference in mean response between group B and reference group A, holding other variables constant.  
- $\beta_2$: Difference in mean response between group C and reference group A, holding other variables constant.  
- The reference group A is not specifically listed in the model.

**Why "holding all other variables constant" is important**  
A key advantage of MLR is its ability to **control for confounding variables**.  

Example: **Sex discrimination in pay**  
- If we suspect a gender-based wage gap, we should control for factors like **position level, education, and job role**.  
- A more precise interpretation:  
  *"A male with the same IQ and education level (instead of generically 'holding all other variables constant') is estimated to earn $28,463 more than a female counterpart."* 


## Adding Model Complexity

### Transformations
When to transform variables:
- **Non-constant variance** → Transform $Y$.
- **Nonlinear trends in residual plots** → Transform $X$ or add polynomial or interaction terms.

### Pros and Cons of Transformations  
- Helps satisfy model assumptions.  
- Allows reliable statistical inference on regression coefficients.  
- Log transformations retain interpretability.  
- More complex transformations may hinder interpretation (less of a concern for predictive models).  

**Example:** Polynomial regression
```{r eval=FALSE}
lm(y ~ poly(x1, 3), data=mydata) # poly(predictor, degree)
```

### Ways to Increase Model Complexity
- **Modeling nonlinear trends**: Introducing higher-order terms (with multiple coefficients) for a single predictor (e.g. $X^2$ or $X^3$).
- **Multiple linear trends by category**: Using interaction terms to model relationships (between multiple predictors) that vary by group.
- **Adding predictors one at a time**: Results in an additive model where the intercept changes, but the slope remains the same.
- **Adding interactions**: Results in a non-additive model where both the intercept and slope change depending on another variable.

---

**Hypothesis testing involving interactions:** 

How do you generate a confidence interval around the non-reference category
if X1 is numerical and X2 has 2 categorical levels, with an interaction term
ref (X2 = 0): Y = b0 + b1X1
non-ref (X2 = 1): Y = (b0 + b2) + (b1 + b3)X1

Do this with contrasts.
H0: c0b0 + c1b1 + c2b2 + c3b3 = 0


### Multicollinearity

Multiple explanatory variables in your model are correlated to each other.

Becomes an issue especially when trying to explain.
1. Makes it harder to "hold the other variables fixed".
2. Creates additional uncertainty in the estimated regression coefficients (wider intervals, larger p-values).
3. Model conclusions can change drastically depending on what variables you add or delete from the model.

Variance Inflation Factors - high correlation between predictors makes high VIF, which make the SE(beta) large too. (The SE(beta) is a function of the relationship between the variability of the response and predictors and the VIF.)

VIF Properties/Rules
- If variable has a VIF approx. 1, then there is not collinearity issue.
- If 5 < VIF < 10, then that variable has mild collinearity and should be investigated.
- If VIF > 10, then that variable has severe collinearity and should be investigated.

High VIFs are not always concerning.
  - Adding complexity such as polynomial terms and interaction terms
  - Expected with categorical variables with more than two levels
  - If you only care about interpreting one regression coefficient and it has low VIF (i.e. if you are just accounting for the other variables)
  
Addressing multicollinearity, 3 settings:

1. When very specific questions are defined up front
  - already have a plan for hypothesis test
  - fit model, assumptions, address issues with assumptions
  - check for multicollinearity (VIFs, graphics): revisit EDA if multicollinearity -> does adjustment make sense; do the results change if you remove the variables with high multicollinearity
  - report findings of the original planned model but make note of multicollinearity concern, potentially include secondary analysis
  - alternative strategies: aggregate the correlated variables into a single variable; data reduction (PCA): create new variables from your original ones 
  
2. No specific question, in exploratory modeling phase, but want an interpretable model (which predictors are important and which aren't)
  - identify a subset of predictor variables that help explain the response
  - typically there are many explanatory variables to consider
  - use manual and automated model-building processes to fit the data set as well as possible and ensure it is free from multicollinearity issues (this process tends to dampen the multicollinearity issue anyway)

3. Objective is to predict only
  - there is no issue with multicollinearity when only making predictions. (the issue is with the SEs and with the hypothesis testing components of the regression tools)
  - focus becomes creating a good estimate of f, more exotic transformations and complexity, just need to verify that the model works on similar future data
