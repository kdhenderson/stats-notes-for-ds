---
title: "Multiple Linear Regression (MLR) Revisited"
---

## Objectives

- Understand regression problems and distinguish them from classification problems.
- Identify when to use explanatory versus predictive modeling.
- Compare parametric and nonparametric regression models and their trade-offs.
- Review multiple linear regression (MLR) and its applications.
- Evaluate key assumptions of MLR and correctly interpret model coefficients.
- Explore techniques to increase model complexity when needed.
- Detect and address multicollinearity in regression models.


## Key Terms

- Response Variable vs. Explanatory Variables  
  - The **response variable** (dependent variable) is what we aim to explain or predict.  
  - **Explanatory variables** (independent variables) are the factors that may influence the response.

- Regression vs. Classification 
  - **Regression**: The response is a continuous numeric variable (e.g. predicting car mileage).  
  - **Classification**: The response variable is categorical and can have two (binary) or more than two levels (e.g. predicting where a car was made: North America, Asia, or Europe).


### Example: Regression Problem - MPG Dataset  
A common regression problem is predicting a car's miles per gallon (MPG) using various features:

- Response Variable: `mpg` (miles per gallon)
- Explanatory Variables: `cylinders`, `horsepower`, `origin` (3 levels), `year`, etc.

### Regression Modeling Workflow
When analyzing a regression problem, we typically follow these steps:

1. Exploratory Data Analysis (EDA) 
   - Visualize relationships between the response and each explanatory variable.
   - Identify potential trends, outliers, and transformations.

2. Key Questions to Consider  
   - Which variables explain `mpg`, and how strong are these relationships?  
   - Are there specific hypotheses to test (e.g. does a car's origin significantly impact `mpg`)?  
   - Can we use this model to predict the `mpg` of a new car?


## Regression Problems

A regression problem occurs when the response variable is continuous. The goal can either be **explanation** (understanding relationships) or **prediction** (making accurate future estimates).

### To Explain or To Predict?

1. Explaining Relationships  
   - Hypothesis testing and confidence intervals  
   - Identifying relationships between response and predictors  
   - Adjusting for confounding (lurking) variables

2. Predicting Future Outcomes  
   - Focused on accuracy, not interpretation  
   - More complex models can be used

To determine the appropriate approach, ask:
What is the primary goal—explanation, prediction, or both?


## Parametric vs. Nonparametric Regression Tools

### General Regression Model Structure

The regression model follows:

$$
Y = f(X) + \varepsilon
$$

where:  
- $X = (x_1, x_2, ..., x_p)$ represents the explanatory variables.  
- $f(X)$ is the deterministic component.  
- $\varepsilon$ is the random error.  

### Key Terminology
- Population function: $f$ (true but unknown function in the population).  
- Sample function: $\hat{f}$ (estimate of $f$ using observed data).  

### Estimating $f$: Parametric vs. Nonparametric Approaches  

There are two primary approaches for estimating $f$:  
1. **Parametric** 
2. **Nonparametric**

The choice depends on how the response and predictor variables relate mathematically.  
For example:  
- A numeric predictor like horsepower may follow a quadratic function.  
- A categorical predictor like cylinders may follow a piecewise function.  

### Parametric Approach
- Requires specifying the functional form of $f(X)$.  
- Assumes additional conditions about the error term.  
- Allows hypothesis testing.  
- Works well when the model is correctly specified.  
- Simpler models can be more interpretable.  

### Nonparametric Approach
- No need to specify $f(X)$.  
- Fully data-driven and flexible.  
- Can capture complex relationships.  
- More difficult to interpret.  
- May require larger datasets for reliable estimation.  
- Does not support traditional hypothesis testing frameworks.  

### Tradeoffs: Interpretability vs. Flexibility

| Approach       | Pros | Cons |
|---------------|------|------|
| Parametric    | Interpretable  <br> Allows hypothesis testing  <br> Works well for small datasets  <br> Easier for high $p$, low $n$ settings | Cannot model complex relationships well  <br> Sensitive to misspecification |
| Nonparametric | Adapts to complex patterns  <br> Less restrictive assumptions  <br> Often performs better for large datasets | Requires large datasets  <br> No hypothesis testing  <br> Harder to interpret |

### Challenges with Parametric Models
1. Specifying highly complex forms of $f(X)$ is difficult.  
2. For large datasets, nonparametric models often perform as well or better.  
3. Additional diagnostic checks are required, such as:  
   - Checking model assumptions (linearity, normality, independence).  
   - Addressing multicollinearity and influential points.  


## A Review of Multiple Linear Regression (MLR)

### Model Structure

$$
Y = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p + \varepsilon
$$

where:  
- $Y$ is the response variable.  
- $X_1, X_2, ..., X_p$ are the explanatory variables.  
- $\varepsilon$ is the random error term.

### Assumptions of MLR
1. **Linearity** – The relationship between predictors and response is linear.  
2. **Independence** – Errors are independent.  
3. **Homoscedasticity** – Errors have constant variance.  
4. **Normality** – Errors follow a normal distribution. 
  
### Residual Diagnostic Checks  
These checks help assess whether assumptions hold:  

- **Residual vs. Fitted Plot** – Examines constant variance (homoscedasticity).  
- **Q-Q Plot and/or Histogram** – Evaluates whether residuals are normally distributed.  
- **Variance Inflation Factor (VIF)** – Detects multicollinearity among predictors.  
- **Data Collection Process Review** – Ensures independence.  
  - Violations often occur in repeated measures on the same subject or in time series data.  

---

**Basic Interpretations**

When each predictor is included in the model only once

Continuous predictors:
  - For every 1 unit increase in X, there is an increase (decrease) of beta in the response variable Y holding the other variables fixed.
  
Categorical variables are "dummy-tized":
  - Ex: X1 categorical with groups A, B, and C while X2, X3 are continuous
  Y = b0 + b1LevelB + b2LevelC + b3X2 + b4X3
  b1: the difference in the mean response (B vs. A (ref/intercept/not in model specifically)) holding the other variables fixed
  b2: the difference in the mean response (C vs. A) holding the other variables fixed
  
"holding all other variables fixed": most advantageous property MLR has
-> put in real world context:
  Sex discrimination in pay:
  What confounding variables could help explain a pay gay? (level of position, education, job role)
  Interpretation: It is estimated that a male with the same IQ level and education level (instead of saying the generic holding all other variables constant) has an average salary that is $28,463 higher than females. 


**Benefits from Adjusting**


**Transformations**
General rules of thumb:
* non-constant variance -> transform Y
* trend in residual plots -> transform or add terms with X

Pros and Cons:
- produces models where assumptions are met
- reliable statistical inference on the regression coefficients
- interpretation of the coefficients is still available for log transformation
- interpretation is difficult with exotic transformation (not an issue if predicting only)

To fit a polynomial model:
`lm(y ~ poly(x1,3), data=mydata) # predictor, degree

Ways models can get more complex:
- modeling a nonlinear trend (one predictor, multiple coefficients from higher order terms)
- multiple linear trends by category (multiple predictors with interaction between predictors)

Add predictor one at a time -> additive model, changing intercept but not slope

Add interaction -> non-additive, different intercept and slope


**Hypothesis testing involving interactions:** 

How do you generate a confidence interval around the non-reference category
if X1 is numerical and X2 has 2 categorical levels, with an interaction term
ref (X2 = 0): Y = b0 + b1X1
non-ref (X2 = 1): Y = (b0 + b2) + (b1 + b3)X1

Do this with contrasts.
H0: c0b0 + c1b1 + c2b2 + c3b3 = 0


### Multicollinearity

Multiple explanatory variables in your model are correlated to each other.

Becomes an issue especially when trying to explain.
1. Makes it harder to "hold the other variables fixed".
2. Creates additional uncertainty in the estimated regression coefficients (wider intervals, larger p-values).
3. Model conclusions can change drastically depending on what variables you add or delete from the model.

Variance Inflation Factors - high correlation between predictors makes high VIF, which make the SE(beta) large too. (The SE(beta) is a function of the relationship between the variability of the response and predictors and the VIF.)

VIF Properties/Rules
- If variable has a VIF approx. 1, then there is not collinearity issue.
- If 5 < VIF < 10, then that variable has mild collinearity and should be investigated.
- If VIF > 10, then that variable has severe collinearity and should be investigated.

High VIFs are not always concerning.
  - Adding complexity such as polynomial terms and interaction terms
  - Expected with categorical variables with more than two levels
  - If you only care about interpreting one regression coefficient and it has low VIF (i.e. if you are just accounting for the other variables)
  
Addressing multicollinearity, 3 settings:

1. When very specific questions are defined up front
  - already have a plan for hypothesis test
  - fit model, assumptions, address issues with assumptions
  - check for multicollinearity (VIFs, graphics): revisit EDA if multicollinearity -> does adjustment make sense; do the results change if you remove the variables with high multicollinearity
  - report findings of the original planned model but make note of multicollinearity concern, potentially include secondary analysis
  - alternative strategies: aggregate the correlated variables into a single variable; data reduction (PCA): create new variables from your original ones 
  
2. No specific question, in exploratory modeling phase, but want an interpretable model (which predictors are important and which aren't)
  - identify a subset of predictor variables that help explain the response
  - typically there are many explanatory variables to consider
  - use manual and automated model-building processes to fit the data set as well as possible and ensure it is free from multicollinearity issues (this process tends to dampen the multicollinearity issue anyway)

3. Objective is to predict only
  - there is no issue with multicollinearity when only making predictions. (the issue is with the SEs and with the hypothesis testing components of the regression tools)
  - focus becomes creating a good estimate of f, more exotic transformations and complexity, just need to verify that the model works on similar future data
