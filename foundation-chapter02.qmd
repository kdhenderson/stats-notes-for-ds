---
title: "Inference Using T-Distributions"
---

## Objectives

This chapter introduces confidence intervals and hypothesis tests based on *t*-distributions, which are used when population standard deviations are unknown. You will learn to:

- Apply the Central Limit Theorem (CLT) to inference problems.  
- Construct and interpret confidence intervals using the *t*-distribution.  
- Perform hypothesis tests and compute *t*-statistics and *p*-values.  
- Link confidence intervals to hypothesis test results.  
- Distinguish statistical from practical significance.  
- Use R to calculate critical values and *p*-values.


## Central Limit Theorem (CLT)

If we draw a large enough sample from a normal (or approximately normal) distribution, the sampling distribution of the sample mean will also be normal. The mean of this sampling distribution of the sample means equals the population mean from which those sample means were taken. This allows us to use the sampling mean, $\bar{X}$, to make inferences about the population mean, $\mu$.


## Interpretation of a Confidence Interval

If we took many random samples from the same population and constructed a confidence interval for the mean from each one, then approximately 95% of those intervals would contain the true mean. For a single interval, we interpret this as: we are 95% confident that the interval contains the true mean.


## Confidence Intervals

- Measure how large or small a parameter value could plausibly be.  
- Random sampling leads to slightly different sample means (estimates of the population mean).  
- A confidence interval is constructed around $\bar{X}$ with a given level of confidence that it contains $\mu$. In other words, it contains the plausible values of $\mu$.
- From normal distribution theory:  
  - Approximately 68% of values lie within $1\sigma$ of $\mu$ 
  - Approximately 95% within $2\sigma$  
  - Approximately 99.7% within $3\sigma$  
- - In practice, we usually estimate both the population mean ($\mu$) and standard deviation ($\sigma$) using sample values. We estimate $\sigma$ using the sample standard deviation, $s$.  
- The *t*-distribution has fatter tails than the normal distribution to account for additional uncertainty from estimating $\sigma$ with the sample standard deviation $s$.

The formula for a confidence interval is:
$$
CI = \bar{X} \pm t_{\text{crit}} \left( \frac{s}{\sqrt{n}} \right)
$$
where $t_{\text{crit}}$ is based on the desired confidence level and degrees of freedom.

**Example confidence interval statement**: "With $\_\_$% confidence, the true mean $\mu$ is between $\_\_$ and $\_\_$, based on a sample $n = \_\_$ from this population."

---

## Hypothesis Testing

### Steps for Testing Significance:
1. Choose a significance level $\alpha$:
   - $\alpha$ determines the probability of a Type I error (rejecting the null hypothesis when it is true).
   - A lower $\alpha$ reduces the chance of a Type I error.
2. Choose a one-sided or two-sided test and confirm assumptions are met (e.g. near-normal distribution, random sampling).
3. State the hypotheses: the assumption about characteristics of one or more parameter in the population.
   - $H_0$: Null hypothesis
   - $H_a$: Alternative hypothesis
4. Follow these six steps:
   1. Identify $H_0$ and $H_a$.
   2. Find the critical value(s) and draw and shade the rejection region.
   3. Calculate the test statistic (i.e. the evidence).
   4. Compute the p-value.
   5. Decide whether to reject or fail to reject $H_0$.
   6. Write a conclusion in non-technical terms, including the p-value, confidence interval, and scope.

### Test Statistic:
- A measure of a sample (the estimate) divided by its variability:
$$
t = \frac{\bar{X} - \mu_0}{\frac{s}{\sqrt{n}}}
$$
- For a one-sided test, the sign of the numerator matters. It is always the sample mean minus the population mean.

### p-Value:
- Calculated assuming the null hypothesis is true.
- A small p-value suggests that random variation alone is unlikely to explain the observed differences.
- Computed as the area under the tails (one or two) beyond the t-statistic, corresponding to $H_a$.
- A p-value is a probability not a hard cutoff, you can just state the value and let the reader decide.


## Linking Hypothesis Tests and Confidence Intervals
- A $(1-\alpha)\%$ confidence interval is equivalent to a two-sided hypothesis test with a significance level of $\alpha$.
- Hypothesis tests can be evaluated through confidence intervals.


## Statistical Significance vs. Practical Significance
- Statistical significance implies results are unlikely due to random sampling error but does not guarantee practical significance.
- p-values are influenced by sample size.
- Effect size, such as Cohenâ€™s $d$, measures practical value.


## In R:
- **Critical Value:** `qt(alpha, df)`
  - Use $\alpha/2$ or $1-\alpha/2$ for a two-sided test; $\alpha$ or $1-\alpha$ for a one-sided test.
- **p-Value:** `pt(t-statistic, df, lower.tail = TRUE/FALSE)`
  - Multiply by 2 for a two-tailed p-value.


## Complete Analysis Format:
1. Statement of the problem.
2. Address the assumptions.
3. Perform the appropriate test (five steps).
4. Provide a conclusion in non-technical terms, including the p-value, confidence interval, and scope of inference.
