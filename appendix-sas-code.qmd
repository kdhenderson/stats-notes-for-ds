---
title: "Appendix B: SAS Code Examples for Statistical Foundations"
appendix: true
format: html
execute:
  eval: false
---

Quick Navigation:
- [1. Data Import & Summary Statistics](#1-data-import--summary-statistics)
- [2. Visualization & Assumption Checks](#2-visualization--assumption-checks)
- [3. Permutation Test](#3-permutation-test)
- [4. t-Tests & Power Analysis](#4-t-tests--power-analysis)
- [5. ANOVA & Extra Sum of Squares](#5-anova--extra-sum-of-squares)
- [6. Multiple Comparisons](#6-multiple-comparisons)
- [7. Contrasts](#7-contrasts)
- [8. Non-Parametric Tests](#8-non-parametric-tests)
- [9. Correlation & Simple Regression](#9-correlation--simple-regression)
- [10. Residual Analysis](#10-residual-analysis)
- [11. Log-Log Models & Back-Transformation](#11-log-log-models--back-transformation)
- [12. Multiple Linear Regression (MLR)](#12-multiple-linear-regression-mlr)
- [13. Variable Selection](#13-variable-selection)

::: {.appendix}

## 1. Data Import & Summary Statistics

> Most SAS procedures require your dataset to be sorted by group for grouped analysis or plotting.
> You can either import data from a file or create a dataset manually for small examples.

### 1A. Import or Create a Dataset
```{sas}

/* OPTION 1: Import a CSV from your SAS Studio home directory */

%web_drop_table(WORK.mydata);

filename reffile '/home/your-username/path-to-your-data.csv';

proc import datafile=reffile
    dbms=csv
    out=WORK.mydata
    replace;
    getnames=yes;
run;

/* View structure and preview the dataset */
proc contents data=WORK.mydata;
run;

%web_open_table(WORK.mydata);
    
    
/* OPTION 2: Manually create a small dataset column-wise (each row = one observation) */

data mydata;
    input variable1 variable2;
    datalines;
    1 A
    2 B
    3 C
    4 D
    5 E
    ;
run;

proc print data = mydata;
run;


/* OPTION 3: Create a row-wise dataset (data entered horizontally using @@) */
data mydata;
    input variable @@;
    datalines;
    1 2 3 4 5 6
    ;
run;
    
proc print data = mydata;
run;
    
```

### 1B. Summary View & Sorting
```{sas}

/* Preview the data */
proc print data = dataset;
run;

/* Sort data by explanatory variable (required for some procedures) */
proc sort data = dataset;
    by explanatory;
run;

```

### 1C. Summary Statistics
```{sas}

/* Summary stats: n, mean, sd, min, max */
proc means data = dataset;
    var response;
run;

/* Medians by group */
proc means data = dataset median;
    class explanatory;
    var response;
run;

/* Manual p-value from t-statistic (two-sided) */
data mypval;
    pv = 2 * (1 - cdf("t", tstat, df));
run;
proc print data = mypval;
run;

```

### 1D. Log Transformation & Filtering
```{sas}

/* Create a log-transformed version of response */
data ldataset;
    set dataset;
    logResponse = log(response);
run;

/* Remove outlier (e.g., filter response < 1200) */
data datasetNoOutlier;
    set dataset;
    where response < 1200;
run;

/* Scatter plot without outlier */
proc sgplot data = datasetNoOutlier;
    scatter x = explanatory y = response;
run;

```

### 1E. Categorical Recoding & Viewing Subsets
```{sas}

/* Create ordinal version of categorical variable */

data educMultiGrp;
    set educMultiGrp;
    if Educ = '<12' then EO = 1;
    else if Educ = '12' then EO = 2;
    else if Educ = '13-15' then EO = 3;
    else if Educ = '16' then EO = 4;
    else if Educ = '>16' then EO = 5;
run;

/* Create a column to collapse levels */
data educMultiGrp;
    set educMultiGrp;
    if Educ = "<12" then Combine = "<12";
    else if Educ = "12" then Combine = "12";
    else if Educ = "13-15" then Combine = "13-15";
    else if Educ in ("16", ">16") then Combine = "16+";
run;

/* View first 50 rows of a dataset */
proc print data = lIncome5Grps(obs=50);
run;

```


## 2. Visualization & Assumption Checks

> Use `proc univariate`, `proc sgplot`, and `proc ttest` to visualize distributions and assess assumptions of normality and variance.  
> Many visualizations require sorting your dataset by the grouping variable.

### 2A. Univariate Plots (No Grouping)
```{sas}

/* Histogram and QQ plot (ungrouped) */
proc univariate data = dataset;
    var response;
    histogram response;
    qqplot response;
run;

/* Boxplot (ungrouped) */
proc sgplot data = dataset;
    vbox response;
run;

/* Scatterplot of response vs explanatory */
proc sgplot data = dataset;
    scatter x = explanatory y = response;
    * title "Scatterplot of ";
run;

/* Turn off the title */
title;
run;

```

### 2B. Histograms and QQ Plots by Group
```{sas}

/* Sort by grouping variable (required for BY statement) */
proc sort data = dataset;
    by explanatory;
run;
proc print data = dataset;
run;

/* Histograms grouped by explanatory */
proc univariate data = dataset;
    by explanatory;
    histogram response;
run;

/* QQ plots grouped by explanatory */
proc univariate data = dataset;
    * title "QQ Plot by ";
    by explanatory;
    qqplot response / normal(mu=est sigma=est);
run;


proc sgplot data = dataset;
	* title "Histogram by ";
    by explanatory;
    histogram response / transparency=0.5;
    density response; * with a normal distribution curve;
run;

```

### 2C. Boxplots and Density Plots by Group
```{sas}

/* Boxplot by group */
proc sgplot data=dataset;
    * title "Boxplot of";
    vbox response / category=explanatory;
run;

/* Histogram with overlaid normal curve by group */
proc sgplot data = dataset;
    by explanatory;
    histogram response / transparency = 0.5;
    density response;  /* Normal curve */
run;

```

### 2D. Visualizations from proc ttest
```{sas}

/* `proc ttest` also generates histogram, QQ plot, and boxplot (grouped) */
proc ttest data = dataset;
    class explanatory;
    var response;
run;

```

### 2E. Custom Histogram with Set Bin Width
```{sas}

/* Adjust histogram range and bin width */
proc univariate data = dataset noprint;
    by explanatory;
    histogram response / endpoints = 0 to 1000 by 50 normal; /* Adjust as needed */
    inset n mean std / pos = ne;
run;

```


## 3. Permutation Test

### 3A. Observed Difference in Means
```{sas}

/* Use proc ttest to compute observed difference between groups */
proc ttest data = dataset;
    class group;
    var response;
run;

```

### 3B. Generate Permutations with proc iml
```{sas}

/* Shuffle response values across group labels using IML */

/* Borrowed code from internet: randomizes observations and creates a matrix with one row per permutation */

/* NOTE: Replace `dataset`, `group` and `response` with your actual variable names */

proc iml;
    use dataset;
    read all var {group response} into x;  /* Make sure to specify class variable first */
    
      nPerms = 10000;                 /* Number of permutations */
    permuted = t(ranperm(x[,2], nPerms));  /* Create permuted response matrix */
    combined = x[,1] || permuted;   /* Combine original group column with each permuted column */

    create newds from combined;
    append from combined;
quit;

```

### 3C. Build Permutation Distribution
```{sas}

/* Use proc ttest to calculate mean differences for each permutation */

ods output off;
ods exclude all;   /* Suppress output */
  
/* The ods output line below is optional if you want to capture confidence intervals */
/* ods output conflimits=diff; */

proc ttest data = newds plots = none;
    class col1;
    var col2 - col10001;
run;

ods output on;
ods exclude none;

```

### 3D. Plot Permutation Histogram & Calculate p-value
```{sas}
                                                                    
/* Plot the distribution of permuted mean differences (Pooled method only) */
proc univariate data = diff;
    where method = "Pooled";
    var mean;
    histogram mean;
run;

/* Count number of permuted differences as or more extreme than observed */
/* Replace `obsDiff` with your observed test statistic from 3A */
/* Adjust for one-tailed or two-tailed test accordingly */
data numdiffs;
    set diff;
    where method = "Pooled";
    if abs(mean) >= abs(obsDiff);
run;

/* Print matching rows (just for visual check) */
proc print data = numdiffs;
    where method = "Pooled";
run;    

/* Manual Step:
   - Check the number of rows in `numdiffs` (from the log or output)
   - Divide that by the total number of permutations (e.g., 10000)
   - This gives your p-value */

```


## 4. $t$-Tests and Power Analysis

> This section includes basic $t$-test syntax, F-tests for equal variances, and power analysis using `proc power`.

### 4A. Basic $t$-Tests and Critical Values
```{sas}

/* Compute critical t-value for two-sided test at Î± = 0.05 */
data criticalvalue;
    critval = quantile("T", 0.975, df);
run;
proc print data = criticalvalue;
run;

/* One-sample t-test (H0: mean = muUnderNull) */
/* * proc ttest outputs histogram, boxplot and qqplot */
proc ttest data = dataset h0 = muUnderNull sides = 2 alpha = 0.05;
    var response;
run;

/* Paired t-test (H0: mean difference = 0, computed as Group2 - Group1) */
proc ttest data = dataset;
    paired explanatory2*explanatory1;
run;

/* Two-sample t-test (equal variance and Welch's handled together) */
/* Pooled uses pooled st.dev. and Satterthwaite uses Welch's */
proc ttest data = dataset h0 = 0 sides = 2 alpha = 0.05;
    class explanatory;
    var response;
run;

```

### 4B. Equality of Variances Tests
```{sas}

/* F-test for equality of variances is run automatically as part of proc ttest */
/* The F-test (default in proc ttest) assumes normal distributions. It's useful as secondary evidence for variance equality if visuals are inconclusive. Null: population variances are equal. */

/* Brown-Forsythe test does not require normality assumptions */
proc glm data = dataset;
    class explanatory;
    model response = explanatory;
    means explanatory / hovtest = bf;  /* Brown-Forsythe test */
run;  

```

### 4C. Power Analysis (One- and Two-Sample)
```{sas}

/* Power for one-sample t-test */
proc power;
    onesamplemeans
        sides = 1
        alpha = 0.05
        nullmean = 0
        mean = xbar
        stddev = s
        ntotal = n
        power = .;
run;

/* Power for two-sample t-test */
proc power;
    twosamplemeans
        sides = 1
        alpha = 0.05
        meandiff = effectSize
        stddev = s
        npergroup = n1
        power = .; /* specify a dot or omit this line to solve for the unknown parameter */

run;

/* Example: specify total sample size and mean for each using `nullmean and meandiff` */
proc power;
    twosamplemeans
        sides = 1
        alpha = 0.05
        nulldiff = 0
        meandiff = 3
        stddev = 4.5
        ntotal = 47;  /* e.g. 24 in group 1, 23 in group 2 */
run;

```

### 4D. Power Curve Plots
```{sas}

/* Power vs. sample size (one-sample) */
ods graphics on;

proc power;
    onesamplemeans
        sides = 1
        alpha = 0.05
        nullmean = 0
        ntotal = 60 80
        mean = 0.07
        stddev = 0.2
        power = .;
    plot x = n min = 60 max = 80;
run;

ods graphics off;

/* Power vs. effect size (two-sample) */
ods graphics on;

proc power;
    twosamplemeans
        sides = 1
        alpha = 0.05
        nulldiff = 0
        meandiff = 3 to 5 by 0.1
        ntotal = 47
        stddev = 4.5
        power = .;
    plot x = effect min = 3 max = 5;
run;

ods graphics off;
		
```


## 5. ANOVA and Extra Sum of Squares

> This section includes one-way ANOVA, multiple comparisons, linear contrasts, and extra sum of squares tests for nested or grouped comparisons.

### 5A. One-Way ANOVA with Variance Checks
```{sas}

/* One-way ANOVA: for EMM and SMM (full), tests if any group means differ (i.e. any pair has difference of means) */
/* Brown-Forsythe test is included to check equal variance assumption (does not require normality) */
proc glm data = dataset;
    class explanatory;
    model response = explanatory;
    means explanatory / hovtest = bf;  /* Brown-Forsythe test (homogeneity of variance) */
run;

/* Manually compute F critical value */
data critical_value;
    set calculations;
    critical_value = finv(1 - alpha, dfn, dfd);  /* right-tailed (1-alpha) */
run;

```


### 5B. Multiple Comparisons (Tukey + Pairwise Tests)
```{sas}

/* Example: pairwise t-tests with Tukey adjustment and confidence intervals */
proc glm data = playersHeight;
    class Sport;
    model Height = Sport;
    means Sport / hovtest = bf;
    lsmeans Sport / pdiff adjust = tukey cl;
run;

/* Optionally show differences both directions (AâB and BâA) */
proc glm data = playersHeight;
    class Sport;
    model Height = Sport;
    means Sport / hovtest = bf tukey cldiff;
run;

/* The cl option provides CIs for the means, 
 while cldiff provides CIs for the differences between means. */

```


### 5C. Linear Contrasts
```{sas}
/* LINEAR CONTRASTS (on sum and avg) */
/* QOI 3: Create a contrast to test to see if there is evidence that
the mean height of group1 is significantly different than
the average of the mean heights of the other 4 groups
(as an alternative to the extra sum of squares test). */

/* Test linear contrasts using CONTRAST and ESTIMATE */
/* These compare one group mean to a linear combination of others */

/* didn't state order = data so groups are in alphabetical order;
order = data; keeps the data in the order it came in: proc glm data = Handicap order = data;
contrast gives you the sum of squares
estimate gives you the estimate of the gamma */  
  
proc glm data = dataset;
    class explanatory;
    model response = explanatory;
    means explanatory;

    /* Grp1 vs sum or average of others (assuming 5 groups) */
    contrast 'Grp1 vs Sum of Other 4 Groups' explanatory 4 -1 -1 -1 -1;
    estimate 'Grp1 vs Sum of Other 4 Groups' explanatory 4 -1 -1 -1 -1;
    estimate 'Grp1 vs Avg of Other 4 Groups' explanatory 4 -1 -1 -1 -1 / divisor = 4;

    /* Grp1 vs Grp2 */
    contrast 'Grp1 vs Grp2' explanatory 1 -1 0 0 0;
    estimate 'Estimate Grp1 vs Grp2' explanatory 1 -1 0 0 0;
run;  
  

/* To compute a 95% CI manually for the difference in averages of above (point estimate +- multiplier * standard error): critical t-value (multiplier for 95% CI) */
data quantile;
    quant = quantile("t", 0.975, df=n-groups);
run;
proc print data = quantile;
run;

```

### 5D. Bonferroni Adjustment for Selected Comparisons
```{sas}

/* Simultaneous CIs for selected differences (e.g. Î¼2âÎ¼3, Î¼2âÎ¼5, Î¼3âÎ¼5) */
/* k=3 (comparing 3 pairs) */

proc glm data = dataset order = data;
    class explanatory;
    model response = explanatory;
    means explanatory;
    lsmeans explanatory / pdiff cl;

    /* ESTIMATE used to extract estimate and SEs for CI construction */
    estimate 'mu2 - mu3' explanatory 0 1 -1 0 0;
    estimate 'mu2 - mu5' explanatory 0 1 0 0 -1;
    estimate 'mu3 - mu5' explanatory 0 0 1 0 -1;
run;


/* to get 95% CI for the difference in averages of above
point estimate +- multiplier * standard error */
/* Bonferroni multiplier: 1 - Î±/(2k) for two-tailed test with k comparisons; df=n-totalgrps */
data quantile;
    quant = quantile("t", 1 - 0.05/6, df);  /* adjust denominator for # comparisons and tails */
run;
proc print data = quantile;
run;

```

---

### 5E. Summary: Choosing a Multiple Comparison Adjustment

- When deciding multiple comparison correction:
  - Were the comparisons planned before looking at the data or unplanned? If planned, no need to make multiple comparison correction. Can use LSD.
  - Are the groups being compared to a single control or reference group? If so, use Dunnettâs.
  - Are the groups normally distributed with equal standard deviation? If so, use Tukey-Kramer.
  - If we are doing some or all unplanned pairwise comparisons, without a reference/control group and without the assumption of normality or equal variance, use Bonferroni.
```{sas}

/* Commands for various multiple comparison methods */

* LSD (Least Significant Difference, use when comparisons are planned) with CI half-width;
proc glm data = dataset order = data;
    class explanatory;
    model response = explanatory;
    means explanatory / lsd;  * means returns the half-width;
    lsmeans explanatory / pdiff cl;
run;

* Dunnett (compare each group to a control/reference group);
proc glm data = dataset order = data;
    class explanatory;
    model response = explanatory;
    means explanatory / dunnett('Control');
    lsmeans explanatory / pdiff = control('Control') adjust = dunnett cl;
run;  
/* Calculate the half-widths: HalfWidth = (UpperCL - LowerCL) / 2 */

* Tukey-Kramer (assumes normality + equal variance; unplanned all-pairwise) with CI half-width;
proc glm data = dataset order = data;
    class explanatory;
    model response = explanatory;
    means explanatory / tukey;
    lsmeans explanatory / pdiff adjust = tukey cl;
run;

* Bonferroni (safest if assumptions are not met or comparisons are partially unplanned) with CI half-width;
proc glm data = dataset order = data;
    class explanatory;
    model response = explanatory;
    means explanatory / bon;
    lsmeans explanatory / pdiff adjust = bon cl;
run;

* View all options together (Jarenâs code);
proc glm data = dataset;
    class explanatory;
    model response = explanatory;
    means explanatory / lsd tukey dunnett bon scheffe;
run;

```

----

### 5F. Extra Sum of Squares F-Test
```{sas}

* conduct an anova (explanatory=group);
proc glm data = data;
class explanatory;
model response = explanatory;
run;

* get p-value from f-statistic (extra sum-of-squares);
data f_test;
p_value = 1 - probf(fstat, dfn, dfd); *1- for right tail;
run;
proc print data=f_test;
run;

* another way to get p-value;
data quantile;
myquant = 1-CDF('F', fstat, dfn, dfd)
run;
proc print data = quantile;
run;


/* EXTRA SUM OF SQUARES */
/* Do the BYOA calculations. */
data calculations;
    alpha = 0.05;  
    dftotal = 2580;
    dfd = 2579;
    dfn = dftotal - dfd;
    ssred = 2234.10;
    ssfull = 2232.12;
    ssmodel = ssred - ssfull;
    mse = ssfull / dfd;
    msmodel = ssmodel / dfn;
    fstat = msmodel / mse;
run;
* Find the critical value for the above hypothesis test;
data critical_value;
    set calculations;
    critical_value = finv(1-alpha, dfn, dfd); *1-alpha for right-tail;
run;
* Find the p-value from F, dfn, dfd;
data f_test;
	set critical_value;
	p_value = 1 - probf(fstat, dfn, dfd); *1-probf for right-tail;
run;

/* CHECK THE RESULTS WITH A CONTRAST */
proc glm data = dataset;
class explanatory;
model response = explanatory;
means explanatory;
Contrast 'Contrast Grp1 vs. Grp2' explanatory 0 0 0 1 -1 0;
Estimate 'Estimate Grp1 vs. Grp2' explanatory 0 0 0 1 -1 0;
run;


/* WELCH'S ANOVA (for normal distributions, but unequal variance) */
proc glm data = dataset;
    class explanatory;
    model response = explanatory;
    means explanatory / welch;
run;

data critical_value;
    critical_value = finv(1-0.05, 4, 706.2); *1-alpha for right-tail;
run;
proc print data = critical_value;
run;


/* KRUSKAL-WALLIS TEST */
/* This is appropriate especially in the case of unequal variances
when there is significant deviations from normality
especially in the case of very small sample size (no CLT)
and is appropriate when you want inference on the median (skewed data)
it is basically doing an ANOVA on the ranked data */
/* If following up with a comparison of specific groups, must use rank-sum. */

proc npar1way data = dataset Wilcoxon;
	class explanatory;
	var response;
run;


## 6. /* Non-Parametric Tests (note that the permutation test is also a non-parametric test */

/* RANK-SUM TEST (for inference on the median of two-samples) */

/* Rank-Sum Test: EXACT Test
note: the bigger the sample size the exact test might run too many permutations */
proc npar1way data = dataset wilcoxon;
class explanatory;
var response;
exact HL wilcoxon; *will give us exact p-values and HL for CI;
run;

/* To get the CI to match a one-sided alpha 0.5 test,
specify the exact HL option alpha 0.1 for the CI. 
HL = Hodges-Lehmann Estimation for confidence intervals */
proc npar1way data = dataset wilcoxon alpha = 0.1;
class explanatory;
var response;
exact HL wilcoxon;
run;

/* Rank-Sum Test: NORMAL Approximation
the larger the sample size, you can use the z approximation
the smaller, the more conservative, choose the t approximation */
proc npar1way data = dataset wilcoxon;
class explanatory;
var response;
run;

/* To get the CI to match a one-sided alpha 0.5 test, specify alpha = 0.1
specify the HL option (asymptotic is for the normal approximation) */
proc npar1way data = dataset wilcoxon HL alpha = 0.1;
class explanatory;
var response;
run;

/* To get the one sided critical value for a z-distribution (normal approximation) */
data critval;
cv = quantile("normal", 0.95); *alpha for left, 1-alpha for right;
run;
proc print data = critval;
run;


/* SIGNED-RANK TEST (for paired data) */

* Add a diff column;
data dataset;
    set dateset;
    diff = before - after;
run;

proc print data = dataset;
run;

* Run the Wilcoxon signed-rank test;
* this seems to give an incorrect output for S, but close for 2-sided p-value;
proc univariate data = dataset;
var diff;
run;

* get the critical value (one-sided z-distribution);
data critval;
cv = quantile("normal", 0.95); *alpha for left, 1-alpha for right;
run;

proc print data = critval;
run;


* This is extra code for the by hand calculations;
* Calculate the absolute differences and rank them;
data ranked;
    set dataset;
    abs_diff = abs(diff);
run;

proc rank data=ranked out=ranked ties=mean;
    var abs_diff;
    ranks rank_abs_diff;
run;

* Sum the ranks of the positive differences;
data stats;
    set ranked;
    if diff > 0 then S = rank_abs_diff;
    else S = 0;
run;

proc means data=stats sum noprint;
    var S;
    output out=sums sum(S)=sum_S n=n_obs;
run;

* Calculate expected mean, standard deviation, and Z statistic;
data final;
    set sums;
    mean_S = n_obs * (n_obs + 1) / 4;
    sd_S = sqrt(n_obs * (n_obs + 1) * (2 * n_obs + 1) / 24);
    Z_statistic = (sum_S - 0.5 - mean_S) / sd_S;
run;

* Print the results;
proc print data=final;
    var sum_S mean_S sd_S Z_statistic;
run;

*To find a one-sided p-value associated with a z-statistic;
data pvalue;
pv = 1-probnorm(zStat);
run;

proc print data = pvalue;
run;


/* KRUSKAL-WALLIS TEST (for inference on the median of three or more samples) */
* ððð ð¡ ðð¡ðð¡ðð ð¡ðð: ð2 (chi-square);
proc npar1way data = dataset;
	class explanatory;
	var response;
run;


## 7. /* SAS code summary */


/* PEARSON'S R CORRELATION COEFFICIENT */

* visualize a scatterplot of the data;
proc sgscatter data = dataset;
plot response*explanatory / markerattrs=(symbol=circlefilled);
title "Analysis";
run;

* get the t-critical value;
data criticalvalue;
critval = quantile("t", 0.975, n-2);
run;
proc print data = criticalvalue;
run;

* find the correlation coefficient and p-value;
proc corr data = dataset;
run;


/* SIMPLE LINEAR REGRESSION */

* get the parameter estimate table with slope and intercept;
proc reg data = dataset;
model response = explanatory;
run;

* if you want to change alpha and get the parameter estimate table including confidence intervals;
proc reg data = dataset alpha = 0.01;
model response = explanatory / clb;
run;


* another way to get the parameter estimate table;
proc glm data = dataset;
* solution -> regression: parameter estimate table;
model response = explanatory / solution;
run;

* if you want to change alpha and get a confidence interval for the parameters;
proc glm data = dataset alpha = 0.01;
model respons = explanatory / solution clparm;
run;


/* CONFIDENCE AND PREDICTION INTERVALS FOR NEW DATA POINTS */

* add new data;
data dataset;
input explanatory response;
datalines;
62 65
90 64
50 48
35 57
200 601
100 146
90 47
95 .
200 .
;

proc reg data = dataset;
model response = explanatory / clm;
* clm = CI for mean;
run;

proc reg data = dataset;
model response = explanatory / cli;
* cli = PI for individual;
run;

/* OR */

proc glm data = dataset;
model response = explanatory / solution clm;
* clm = CI for mean;
run;

proc glm data = dataset;
model response = explanatory / solution cli;
* cli = PI for individual;
run;

## 8. /* USEFUL FUNCTIONS */

* to get pvalues (2-sided);
data mypval;
pv = 2*(1-cdf(âtâ, tstat, df));
run;
proc print data = mypval;
run;

from permuation section
* plot histograms;
proc univariate data = data;
class group;
histogram variable;
run;

:::